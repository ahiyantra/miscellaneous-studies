{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":90676,"databundleVersionId":10554064,"sourceType":"competition"}],"dockerImageVersionId":30822,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# alternative code 01","metadata":{}},{"cell_type":"code","source":"!pip install -q autogluon.tabular ray==2.10.0\n\nfrom autogluon.tabular import TabularPredictor\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import accuracy_score\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport pandas as pd\nimport numpy as np\nimport warnings\nimport pickle\nimport shutil\nimport glob\n\nwarnings.filterwarnings('ignore')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-21T11:53:19.350971Z","iopub.execute_input":"2024-12-21T11:53:19.351366Z","iopub.status.idle":"2024-12-21T11:53:47.812141Z","shell.execute_reply.started":"2024-12-21T11:53:19.351297Z","shell.execute_reply":"2024-12-21T11:53:47.810726Z"}},"outputs":[{"name":"stdout","text":"\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.1/65.1 MB\u001b[0m \u001b[31m21.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m352.2/352.2 kB\u001b[0m \u001b[31m17.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m266.2/266.2 kB\u001b[0m \u001b[31m11.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.1/64.1 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m68.2/68.2 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.3/13.3 MB\u001b[0m \u001b[31m76.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25h","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"class CFG:\n    train_path = '/kaggle/input/mental-health-prediction-hackathon/train.csv'\n    test_path = '/kaggle/input/mental-health-prediction-hackathon/test.csv'\n    sample_sub_path = '/kaggle/input/mental-health-prediction-hackathon/sample_submission.csv'\n    \n    oof_path = '/kaggle/input/depression-prediction-oof-files'\n    \n    target = 'Depression'\n    n_folds = 5\n    seed = 42\n    \n    time_limit = 3600 * 9","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-21T11:53:47.813220Z","iopub.execute_input":"2024-12-21T11:53:47.813702Z","iopub.status.idle":"2024-12-21T11:53:47.820170Z","shell.execute_reply.started":"2024-12-21T11:53:47.813674Z","shell.execute_reply":"2024-12-21T11:53:47.818512Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"train = pd.read_csv(CFG.train_path, index_col='id')\n\nX = train.drop(CFG.target, axis=1)\ny = train[CFG.target]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-21T11:53:47.821623Z","iopub.execute_input":"2024-12-21T11:53:47.821992Z","iopub.status.idle":"2024-12-21T11:53:48.514637Z","shell.execute_reply.started":"2024-12-21T11:53:47.821960Z","shell.execute_reply":"2024-12-21T11:53:48.513477Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"def get_data(path):\n    oof_pred_probs_files = glob.glob(f'{path}/*_oof_pred_probs_*.pkl')\n    test_pred_probs_files = glob.glob(f'{path}/*_test_pred_probs_*.pkl')\n\n    oof_pred_probs = pickle.load(open(oof_pred_probs_files[0], 'rb'))\n    test_pred_probs = pickle.load(open(test_pred_probs_files[0], 'rb'))\n    \n    scores = []\n    skf = StratifiedKFold(n_splits=CFG.n_folds, random_state=CFG.seed, shuffle=True)\n    for _, val_idx in skf.split(X, y):\n        y_val = y[val_idx]\n        y_pred_probs = oof_pred_probs[val_idx]   \n        score = accuracy_score(y_val, y_pred_probs.round())\n        scores.append(score)\n        \n    return oof_pred_probs, test_pred_probs, scores","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-21T11:53:48.515916Z","iopub.execute_input":"2024-12-21T11:53:48.516348Z","iopub.status.idle":"2024-12-21T11:53:48.523731Z","shell.execute_reply.started":"2024-12-21T11:53:48.516284Z","shell.execute_reply":"2024-12-21T11:53:48.522554Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"scores = {}\noof_pred_probs = {}\ntest_pred_probs = {}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-21T11:53:48.527525Z","iopub.execute_input":"2024-12-21T11:53:48.527865Z","iopub.status.idle":"2024-12-21T11:53:48.546835Z","shell.execute_reply.started":"2024-12-21T11:53:48.527840Z","shell.execute_reply":"2024-12-21T11:53:48.545289Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"model_paths = glob.glob(f'{CFG.oof_path}/*')\nfor model_path in model_paths:\n    model_name = model_path.split('/')[-1]\n    oof_pred_probs[model_name], test_pred_probs[model_name], scores[model_name] = get_data(model_path)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-21T11:53:48.552106Z","iopub.execute_input":"2024-12-21T11:53:48.552479Z","iopub.status.idle":"2024-12-21T11:53:48.567662Z","shell.execute_reply.started":"2024-12-21T11:53:48.552452Z","shell.execute_reply":"2024-12-21T11:53:48.566203Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"train = pd.DataFrame(oof_pred_probs)\ntrain[CFG.target] = y\n\ntest = pd.DataFrame(test_pred_probs)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-21T11:53:48.569118Z","iopub.execute_input":"2024-12-21T11:53:48.569530Z","iopub.status.idle":"2024-12-21T11:53:48.596791Z","shell.execute_reply.started":"2024-12-21T11:53:48.569478Z","shell.execute_reply":"2024-12-21T11:53:48.595563Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"skf = StratifiedKFold(n_splits=CFG.n_folds, random_state=CFG.seed, shuffle=True)\nsplit = skf.split(train, train[CFG.target])\nfor i, (_, val_index) in enumerate(split):\n    train.loc[val_index, 'fold'] = i","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-21T11:53:48.597877Z","iopub.execute_input":"2024-12-21T11:53:48.598283Z","iopub.status.idle":"2024-12-21T11:53:48.657260Z","shell.execute_reply.started":"2024-12-21T11:53:48.598247Z","shell.execute_reply":"2024-12-21T11:53:48.656009Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"predictor = TabularPredictor(\n    problem_type='binary',\n    eval_metric='accuracy',\n    label=CFG.target,\n    groups='fold',\n    verbosity=2\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-21T11:53:48.658798Z","iopub.execute_input":"2024-12-21T11:53:48.659199Z","iopub.status.idle":"2024-12-21T11:53:48.667365Z","shell.execute_reply.started":"2024-12-21T11:53:48.659163Z","shell.execute_reply":"2024-12-21T11:53:48.666052Z"}},"outputs":[{"name":"stderr","text":"No path specified. Models will be saved in: \"AutogluonModels/ag-20241221_115348\"\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"predictor.fit(\n    train_data=train,\n    time_limit=CFG.time_limit,\n    presets='best_quality',\n    ag_args_fit={\n        'num_gpus': 1, \n        'num_cpus': 4\n    }\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-21T11:53:48.668443Z","iopub.execute_input":"2024-12-21T11:53:48.668782Z","iopub.status.idle":"2024-12-21T11:54:28.634136Z","shell.execute_reply.started":"2024-12-21T11:53:48.668754Z","shell.execute_reply":"2024-12-21T11:54:28.631073Z"}},"outputs":[{"name":"stderr","text":"Verbosity: 2 (Standard Logging)\n=================== System Info ===================\nAutoGluon Version:  1.2\nPython Version:     3.10.12\nOperating System:   Linux\nPlatform Machine:   x86_64\nPlatform Version:   #1 SMP PREEMPT_DYNAMIC Sun Nov 10 10:07:59 UTC 2024\nCPU Count:          4\nMemory Avail:       30.05 GB / 31.35 GB (95.9%)\nDisk Space Avail:   19.50 GB / 19.52 GB (99.9%)\n===================================================\nPresets specified: ['best_quality']\nSetting dynamic_stacking from 'auto' to True. Reason: Enable dynamic_stacking when use_bag_holdout is disabled. (use_bag_holdout=False)\nStack configuration (auto_stack=True): num_stack_levels=1, num_bag_folds=8, num_bag_sets=1\nDyStack is enabled (dynamic_stacking=True). AutoGluon will try to determine whether the input data is affected by stacked overfitting and enable or disable stacking as a consequence.\n\tThis is used to identify the optimal `num_stack_levels` value. Copies of AutoGluon will be fit on subsets of the data. Then holdout validation data is used to detect stacked overfitting.\n\tRunning DyStack for up to 8100s of the 32400s of remaining time (25%).\n\tRunning DyStack sub-fit in a ray process to avoid memory leakage. Enabling ray logging (enable_ray_logging=True). Specify `ds_args={'enable_ray_logging': False}` if you experience logging issues.\n2024-12-21 11:53:53,131\tINFO worker.py:1752 -- Started a local Ray instance.\n\t\tContext path: \"/kaggle/working/AutogluonModels/ag-20241221_115348/ds_sub_fit/sub_fit_ho\"\n\u001b[36m(_dystack pid=210)\u001b[0m Running DyStack sub-fit ...\n\u001b[36m(_dystack pid=210)\u001b[0m Values in column 'fold' used as split folds instead of being automatically set. Bagged models will have 5 splits.\n\u001b[36m(_dystack pid=210)\u001b[0m Beginning AutoGluon training ... Time limit = 8094s\n\u001b[36m(_dystack pid=210)\u001b[0m AutoGluon will save models to \"/kaggle/working/AutogluonModels/ag-20241221_115348/ds_sub_fit/sub_fit_ho\"\n\u001b[36m(_dystack pid=210)\u001b[0m Train Data Rows:    125066\n\u001b[36m(_dystack pid=210)\u001b[0m Train Data Columns: 1\n\u001b[36m(_dystack pid=210)\u001b[0m Label Column:       Depression\n\u001b[36m(_dystack pid=210)\u001b[0m Problem Type:       binary\n\u001b[36m(_dystack pid=210)\u001b[0m Preprocessing data ...\n\u001b[36m(_dystack pid=210)\u001b[0m Selected class <--> label mapping:  class 1 = 1, class 0 = 0\n\u001b[36m(_dystack pid=210)\u001b[0m Using Feature Generators to preprocess the data ...\n\u001b[36m(_dystack pid=210)\u001b[0m Fitting AutoMLPipelineFeatureGenerator...\n\u001b[36m(_dystack pid=210)\u001b[0m \tAvailable Memory:                    30344.73 MB\n\u001b[36m(_dystack pid=210)\u001b[0m \tTrain Data (Original)  Memory Usage: 0.00 MB (0.0% of available memory)\n\u001b[36m(_dystack pid=210)\u001b[0m \tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n\u001b[36m(_dystack pid=210)\u001b[0m \tStage 1 Generators:\n\u001b[36m(_dystack pid=210)\u001b[0m \tStage 2 Generators:\n\u001b[36m(_dystack pid=210)\u001b[0m \tStage 3 Generators:\n\u001b[36m(_dystack pid=210)\u001b[0m \tStage 4 Generators:\n\u001b[36m(_dystack pid=210)\u001b[0m \tStage 5 Generators:\n\u001b[36m(_dystack pid=210)\u001b[0m \tWARNING: No useful features were detected in the data! AutoGluon will train using 0 features, and will always predict the same value. Ensure that you are passing the correct data to AutoGluon!\n\u001b[36m(_dystack pid=210)\u001b[0m Fitting DummyFeatureGenerator...\n\u001b[36m(_dystack pid=210)\u001b[0m \tTypes of features in original data (raw dtype, special dtypes):\n\u001b[36m(_dystack pid=210)\u001b[0m \tTypes of features in processed data (raw dtype, special dtypes):\n\u001b[36m(_dystack pid=210)\u001b[0m \t\t('int', []) : 1 | ['__dummy__']\n\u001b[36m(_dystack pid=210)\u001b[0m \t0.0s = Fit runtime\n\u001b[36m(_dystack pid=210)\u001b[0m \t0 features in original data used to generate 1 features in processed data.\n\u001b[36m(_dystack pid=210)\u001b[0m \tTrain Data (Processed) Memory Usage: 0.95 MB (0.0% of available memory)\n\u001b[36m(_dystack pid=210)\u001b[0m Data preprocessing and feature engineering runtime = 0.05s ...\n\u001b[36m(_dystack pid=210)\u001b[0m AutoGluon will gauge predictive performance using evaluation metric: 'accuracy'\n\u001b[36m(_dystack pid=210)\u001b[0m \tTo change this, specify the eval_metric parameter of Predictor()\n\u001b[36m(_dystack pid=210)\u001b[0m Large model count detected (112 configs) ... Only displaying the first 3 models of each family. To see all, set `verbosity=3`.\n\u001b[36m(_dystack pid=210)\u001b[0m User-specified model hyperparameters to be fit:\n\u001b[36m(_dystack pid=210)\u001b[0m {\n\u001b[36m(_dystack pid=210)\u001b[0m \t'NN_TORCH': [{}, {'activation': 'elu', 'dropout_prob': 0.10077639529843717, 'hidden_size': 108, 'learning_rate': 0.002735937344002146, 'num_layers': 4, 'use_batchnorm': True, 'weight_decay': 1.356433327634438e-12, 'ag_args': {'name_suffix': '_r79', 'priority': -2}}, {'activation': 'elu', 'dropout_prob': 0.11897478034205347, 'hidden_size': 213, 'learning_rate': 0.0010474382260641949, 'num_layers': 4, 'use_batchnorm': False, 'weight_decay': 5.594471067786272e-10, 'ag_args': {'name_suffix': '_r22', 'priority': -7}}],\n\u001b[36m(_dystack pid=210)\u001b[0m \t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, {'learning_rate': 0.03, 'num_leaves': 128, 'feature_fraction': 0.9, 'min_data_in_leaf': 3, 'ag_args': {'name_suffix': 'Large', 'priority': 0, 'hyperparameter_tune_kwargs': None}}],\n\u001b[36m(_dystack pid=210)\u001b[0m \t'CAT': [{}, {'depth': 6, 'grow_policy': 'SymmetricTree', 'l2_leaf_reg': 2.1542798306067823, 'learning_rate': 0.06864209415792857, 'max_ctr_complexity': 4, 'one_hot_max_size': 10, 'ag_args': {'name_suffix': '_r177', 'priority': -1}}, {'depth': 8, 'grow_policy': 'Depthwise', 'l2_leaf_reg': 2.7997999596449104, 'learning_rate': 0.031375015734637225, 'max_ctr_complexity': 2, 'one_hot_max_size': 3, 'ag_args': {'name_suffix': '_r9', 'priority': -5}}],\n\u001b[36m(_dystack pid=210)\u001b[0m \t'XGB': [{}, {'colsample_bytree': 0.6917311125174739, 'enable_categorical': False, 'learning_rate': 0.018063876087523967, 'max_depth': 10, 'min_child_weight': 0.6028633586934382, 'ag_args': {'name_suffix': '_r33', 'priority': -8}}, {'colsample_bytree': 0.6628423832084077, 'enable_categorical': False, 'learning_rate': 0.08775715546881824, 'max_depth': 5, 'min_child_weight': 0.6294123374222513, 'ag_args': {'name_suffix': '_r89', 'priority': -16}}],\n\u001b[36m(_dystack pid=210)\u001b[0m \t'FASTAI': [{}, {'bs': 256, 'emb_drop': 0.5411770367537934, 'epochs': 43, 'layers': [800, 400], 'lr': 0.01519848858318159, 'ps': 0.23782946566604385, 'ag_args': {'name_suffix': '_r191', 'priority': -4}}, {'bs': 2048, 'emb_drop': 0.05070411322605811, 'epochs': 29, 'layers': [200, 100], 'lr': 0.08974235041576624, 'ps': 0.10393466140748028, 'ag_args': {'name_suffix': '_r102', 'priority': -11}}],\n\u001b[36m(_dystack pid=210)\u001b[0m \t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n\u001b[36m(_dystack pid=210)\u001b[0m \t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n\u001b[36m(_dystack pid=210)\u001b[0m \t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n\u001b[36m(_dystack pid=210)\u001b[0m }\n\u001b[36m(_dystack pid=210)\u001b[0m AutoGluon will fit 2 stack levels (L1 to L2) ...\n\u001b[36m(_dystack pid=210)\u001b[0m Fitting 110 L1 models, fit_strategy=\"sequential\" ...\n\u001b[36m(_dystack pid=210)\u001b[0m Fitting model: KNeighborsUnif_BAG_L1 ... Training model for up to 5394.53s of the 8093.80s of remaining time.\n\u001b[36m(_dystack pid=210)\u001b[0m \tNo valid features to train KNeighborsUnif_BAG_L1... Skipping this model.\n\u001b[36m(_dystack pid=210)\u001b[0m Fitting model: KNeighborsDist_BAG_L1 ... Training model for up to 5394.44s of the 8093.71s of remaining time.\n\u001b[36m(_dystack pid=210)\u001b[0m \tNo valid features to train KNeighborsDist_BAG_L1... Skipping this model.\n\u001b[36m(_dystack pid=210)\u001b[0m Fitting model: LightGBMXT_BAG_L1 ... Training model for up to 5394.35s of the 8093.63s of remaining time.\n\u001b[36m(_dystack pid=210)\u001b[0m \tNo valid features to train LightGBMXT_BAG_L1... Skipping this model.\n\u001b[36m(_dystack pid=210)\u001b[0m Fitting model: LightGBM_BAG_L1 ... Training model for up to 5394.27s of the 8093.54s of remaining time.\n\u001b[36m(_dystack pid=210)\u001b[0m \tNo valid features to train LightGBM_BAG_L1... Skipping this model.\n\u001b[36m(_dystack pid=210)\u001b[0m Fitting model: RandomForestGini_BAG_L1 ... Training model for up to 5394.18s of the 8093.45s of remaining time.\n\u001b[36m(_dystack pid=210)\u001b[0m \tNo valid features to train RandomForestGini_BAG_L1... Skipping this model.\n\u001b[36m(_dystack pid=210)\u001b[0m Fitting model: RandomForestEntr_BAG_L1 ... Training model for up to 5394.07s of the 8093.34s of remaining time.\n\u001b[36m(_dystack pid=210)\u001b[0m \tNo valid features to train RandomForestEntr_BAG_L1... Skipping this model.\n\u001b[36m(_dystack pid=210)\u001b[0m Fitting model: CatBoost_BAG_L1 ... Training model for up to 5393.94s of the 8093.21s of remaining time.\n\u001b[36m(_dystack pid=210)\u001b[0m \tNo valid features to train CatBoost_BAG_L1... Skipping this model.\n\u001b[36m(_dystack pid=210)\u001b[0m Fitting model: ExtraTreesGini_BAG_L1 ... Training model for up to 5393.80s of the 8093.08s of remaining time.\n\u001b[36m(_dystack pid=210)\u001b[0m \tNo valid features to train ExtraTreesGini_BAG_L1... Skipping this model.\n\u001b[36m(_dystack pid=210)\u001b[0m Fitting model: ExtraTreesEntr_BAG_L1 ... Training model for up to 5393.67s of the 8092.94s of remaining time.\n\u001b[36m(_dystack pid=210)\u001b[0m \tNo valid features to train ExtraTreesEntr_BAG_L1... Skipping this model.\n\u001b[36m(_dystack pid=210)\u001b[0m Fitting model: NeuralNetFastAI_BAG_L1 ... Training model for up to 5393.54s of the 8092.81s of remaining time.\n\u001b[36m(_dystack pid=210)\u001b[0m \tNo valid features to train NeuralNetFastAI_BAG_L1... Skipping this model.\n\u001b[36m(_dystack pid=210)\u001b[0m Fitting model: XGBoost_BAG_L1 ... Training model for up to 5393.40s of the 8092.67s of remaining time.\n\u001b[36m(_dystack pid=210)\u001b[0m \tNo valid features to train XGBoost_BAG_L1... Skipping this model.\n\u001b[36m(_dystack pid=210)\u001b[0m Fitting model: NeuralNetTorch_BAG_L1 ... Training model for up to 5393.27s of the 8092.54s of remaining time.\n\u001b[36m(_dystack pid=210)\u001b[0m \tNo valid features to train NeuralNetTorch_BAG_L1... Skipping this model.\n\u001b[36m(_dystack pid=210)\u001b[0m Fitting model: LightGBMLarge_BAG_L1 ... Training model for up to 5393.14s of the 8092.41s of remaining time.\n\u001b[36m(_dystack pid=210)\u001b[0m \tNo valid features to train LightGBMLarge_BAG_L1... Skipping this model.\n\u001b[36m(_dystack pid=210)\u001b[0m Fitting model: CatBoost_r177_BAG_L1 ... Training model for up to 5393.01s of the 8092.28s of remaining time.\n\u001b[36m(_dystack pid=210)\u001b[0m \tNo valid features to train CatBoost_r177_BAG_L1... Skipping this model.\n\u001b[36m(_dystack pid=210)\u001b[0m Fitting model: NeuralNetTorch_r79_BAG_L1 ... Training model for up to 5392.88s of the 8092.15s of remaining time.\n\u001b[36m(_dystack pid=210)\u001b[0m \tNo valid features to train NeuralNetTorch_r79_BAG_L1... Skipping this model.\n\u001b[36m(_dystack pid=210)\u001b[0m Fitting model: LightGBM_r131_BAG_L1 ... Training model for up to 5392.74s of the 8092.02s of remaining time.\n\u001b[36m(_dystack pid=210)\u001b[0m \tNo valid features to train LightGBM_r131_BAG_L1... Skipping this model.\n\u001b[36m(_dystack pid=210)\u001b[0m Fitting model: NeuralNetFastAI_r191_BAG_L1 ... Training model for up to 5392.61s of the 8091.88s of remaining time.\n\u001b[36m(_dystack pid=210)\u001b[0m \tNo valid features to train NeuralNetFastAI_r191_BAG_L1... Skipping this model.\n\u001b[36m(_dystack pid=210)\u001b[0m Fitting model: CatBoost_r9_BAG_L1 ... Training model for up to 5392.48s of the 8091.75s of remaining time.\n\u001b[36m(_dystack pid=210)\u001b[0m \tNo valid features to train CatBoost_r9_BAG_L1... Skipping this model.\n\u001b[36m(_dystack pid=210)\u001b[0m Fitting model: LightGBM_r96_BAG_L1 ... Training model for up to 5392.35s of the 8091.62s of remaining time.\n\u001b[36m(_dystack pid=210)\u001b[0m \tNo valid features to train LightGBM_r96_BAG_L1... Skipping this model.\n\u001b[36m(_dystack pid=210)\u001b[0m Fitting model: NeuralNetTorch_r22_BAG_L1 ... Training model for up to 5392.21s of the 8091.48s of remaining time.\n\u001b[36m(_dystack pid=210)\u001b[0m \tNo valid features to train NeuralNetTorch_r22_BAG_L1... Skipping this model.\n\u001b[36m(_dystack pid=210)\u001b[0m Fitting model: XGBoost_r33_BAG_L1 ... Training model for up to 5392.08s of the 8091.35s of remaining time.\n\u001b[36m(_dystack pid=210)\u001b[0m \tNo valid features to train XGBoost_r33_BAG_L1... Skipping this model.\n\u001b[36m(_dystack pid=210)\u001b[0m Fitting model: ExtraTrees_r42_BAG_L1 ... Training model for up to 5391.95s of the 8091.22s of remaining time.\n\u001b[36m(_dystack pid=210)\u001b[0m \tNo valid features to train ExtraTrees_r42_BAG_L1... Skipping this model.\n\u001b[36m(_dystack pid=210)\u001b[0m Fitting model: CatBoost_r137_BAG_L1 ... Training model for up to 5391.82s of the 8091.09s of remaining time.\n\u001b[36m(_dystack pid=210)\u001b[0m \tNo valid features to train CatBoost_r137_BAG_L1... Skipping this model.\n\u001b[36m(_dystack pid=210)\u001b[0m Fitting model: NeuralNetFastAI_r102_BAG_L1 ... Training model for up to 5391.68s of the 8090.96s of remaining time.\n\u001b[36m(_dystack pid=210)\u001b[0m \tNo valid features to train NeuralNetFastAI_r102_BAG_L1... Skipping this model.\n\u001b[36m(_dystack pid=210)\u001b[0m Fitting model: CatBoost_r13_BAG_L1 ... Training model for up to 5391.55s of the 8090.83s of remaining time.\n\u001b[36m(_dystack pid=210)\u001b[0m \tNo valid features to train CatBoost_r13_BAG_L1... Skipping this model.\n\u001b[36m(_dystack pid=210)\u001b[0m Fitting model: RandomForest_r195_BAG_L1 ... Training model for up to 5391.42s of the 8090.70s of remaining time.\n\u001b[36m(_dystack pid=210)\u001b[0m \tNo valid features to train RandomForest_r195_BAG_L1... Skipping this model.\n\u001b[36m(_dystack pid=210)\u001b[0m Fitting model: LightGBM_r188_BAG_L1 ... Training model for up to 5391.29s of the 8090.57s of remaining time.\n\u001b[36m(_dystack pid=210)\u001b[0m \tNo valid features to train LightGBM_r188_BAG_L1... Skipping this model.\n\u001b[36m(_dystack pid=210)\u001b[0m Fitting model: NeuralNetFastAI_r145_BAG_L1 ... Training model for up to 5391.16s of the 8090.43s of remaining time.\n\u001b[36m(_dystack pid=210)\u001b[0m \tNo valid features to train NeuralNetFastAI_r145_BAG_L1... Skipping this model.\n\u001b[36m(_dystack pid=210)\u001b[0m Fitting model: XGBoost_r89_BAG_L1 ... Training model for up to 5391.03s of the 8090.30s of remaining time.\n\u001b[36m(_dystack pid=210)\u001b[0m \tNo valid features to train XGBoost_r89_BAG_L1... Skipping this model.\n\u001b[36m(_dystack pid=210)\u001b[0m Fitting model: NeuralNetTorch_r30_BAG_L1 ... Training model for up to 5390.90s of the 8090.17s of remaining time.\n\u001b[36m(_dystack pid=210)\u001b[0m \tNo valid features to train NeuralNetTorch_r30_BAG_L1... Skipping this model.\n\u001b[36m(_dystack pid=210)\u001b[0m Fitting model: LightGBM_r130_BAG_L1 ... Training model for up to 5390.77s of the 8090.04s of remaining time.\n\u001b[36m(_dystack pid=210)\u001b[0m \tNo valid features to train LightGBM_r130_BAG_L1... Skipping this model.\n\u001b[36m(_dystack pid=210)\u001b[0m Fitting model: NeuralNetTorch_r86_BAG_L1 ... Training model for up to 5390.63s of the 8089.90s of remaining time.\n\u001b[36m(_dystack pid=210)\u001b[0m \tNo valid features to train NeuralNetTorch_r86_BAG_L1... Skipping this model.\n\u001b[36m(_dystack pid=210)\u001b[0m Fitting model: CatBoost_r50_BAG_L1 ... Training model for up to 5390.50s of the 8089.77s of remaining time.\n\u001b[36m(_dystack pid=210)\u001b[0m \tNo valid features to train CatBoost_r50_BAG_L1... Skipping this model.\n\u001b[36m(_dystack pid=210)\u001b[0m Fitting model: NeuralNetFastAI_r11_BAG_L1 ... Training model for up to 5390.36s of the 8089.63s of remaining time.\n\u001b[36m(_dystack pid=210)\u001b[0m \tNo valid features to train NeuralNetFastAI_r11_BAG_L1... Skipping this model.\n\u001b[36m(_dystack pid=210)\u001b[0m Fitting model: XGBoost_r194_BAG_L1 ... Training model for up to 5390.23s of the 8089.50s of remaining time.\n\u001b[36m(_dystack pid=210)\u001b[0m \tNo valid features to train XGBoost_r194_BAG_L1... Skipping this model.\n\u001b[36m(_dystack pid=210)\u001b[0m Fitting model: ExtraTrees_r172_BAG_L1 ... Training model for up to 5390.10s of the 8089.37s of remaining time.\n\u001b[36m(_dystack pid=210)\u001b[0m \tNo valid features to train ExtraTrees_r172_BAG_L1... Skipping this model.\n\u001b[36m(_dystack pid=210)\u001b[0m Fitting model: CatBoost_r69_BAG_L1 ... Training model for up to 5389.99s of the 8089.26s of remaining time.\n\u001b[36m(_dystack pid=210)\u001b[0m \tNo valid features to train CatBoost_r69_BAG_L1... Skipping this model.\n\u001b[36m(_dystack pid=210)\u001b[0m Fitting model: NeuralNetFastAI_r103_BAG_L1 ... Training model for up to 5389.88s of the 8089.16s of remaining time.\n\u001b[36m(_dystack pid=210)\u001b[0m \tNo valid features to train NeuralNetFastAI_r103_BAG_L1... Skipping this model.\n\u001b[36m(_dystack pid=210)\u001b[0m Fitting model: NeuralNetTorch_r14_BAG_L1 ... Training model for up to 5389.77s of the 8089.05s of remaining time.\n\u001b[36m(_dystack pid=210)\u001b[0m \tNo valid features to train NeuralNetTorch_r14_BAG_L1... Skipping this model.\n\u001b[36m(_dystack pid=210)\u001b[0m Fitting model: LightGBM_r161_BAG_L1 ... Training model for up to 5389.67s of the 8088.94s of remaining time.\n\u001b[36m(_dystack pid=210)\u001b[0m \tNo valid features to train LightGBM_r161_BAG_L1... Skipping this model.\n\u001b[36m(_dystack pid=210)\u001b[0m Fitting model: NeuralNetFastAI_r143_BAG_L1 ... Training model for up to 5389.57s of the 8088.84s of remaining time.\n\u001b[36m(_dystack pid=210)\u001b[0m \tNo valid features to train NeuralNetFastAI_r143_BAG_L1... Skipping this model.\n\u001b[36m(_dystack pid=210)\u001b[0m Fitting model: CatBoost_r70_BAG_L1 ... Training model for up to 5389.48s of the 8088.76s of remaining time.\n\u001b[36m(_dystack pid=210)\u001b[0m \tNo valid features to train CatBoost_r70_BAG_L1... Skipping this model.\n\u001b[36m(_dystack pid=210)\u001b[0m Fitting model: NeuralNetFastAI_r156_BAG_L1 ... Training model for up to 5389.39s of the 8088.67s of remaining time.\n\u001b[36m(_dystack pid=210)\u001b[0m \tNo valid features to train NeuralNetFastAI_r156_BAG_L1... Skipping this model.\n\u001b[36m(_dystack pid=210)\u001b[0m Fitting model: LightGBM_r196_BAG_L1 ... Training model for up to 5389.31s of the 8088.58s of remaining time.\n\u001b[36m(_dystack pid=210)\u001b[0m \tNo valid features to train LightGBM_r196_BAG_L1... Skipping this model.\n\u001b[36m(_dystack pid=210)\u001b[0m Fitting model: RandomForest_r39_BAG_L1 ... Training model for up to 5389.22s of the 8088.49s of remaining time.\n\u001b[36m(_dystack pid=210)\u001b[0m \tNo valid features to train RandomForest_r39_BAG_L1... Skipping this model.\n\u001b[36m(_dystack pid=210)\u001b[0m Fitting model: CatBoost_r167_BAG_L1 ... Training model for up to 5389.13s of the 8088.40s of remaining time.\n\u001b[36m(_dystack pid=210)\u001b[0m \tNo valid features to train CatBoost_r167_BAG_L1... Skipping this model.\n\u001b[36m(_dystack pid=210)\u001b[0m Fitting model: NeuralNetFastAI_r95_BAG_L1 ... Training model for up to 5389.04s of the 8088.31s of remaining time.\n\u001b[36m(_dystack pid=210)\u001b[0m \tNo valid features to train NeuralNetFastAI_r95_BAG_L1... Skipping this model.\n\u001b[36m(_dystack pid=210)\u001b[0m Fitting model: NeuralNetTorch_r41_BAG_L1 ... Training model for up to 5388.94s of the 8088.22s of remaining time.\n\u001b[36m(_dystack pid=210)\u001b[0m \tNo valid features to train NeuralNetTorch_r41_BAG_L1... Skipping this model.\n\u001b[36m(_dystack pid=210)\u001b[0m Fitting model: XGBoost_r98_BAG_L1 ... Training model for up to 5388.85s of the 8088.12s of remaining time.\n\u001b[36m(_dystack pid=210)\u001b[0m \tNo valid features to train XGBoost_r98_BAG_L1... Skipping this model.\n\u001b[36m(_dystack pid=210)\u001b[0m Fitting model: LightGBM_r15_BAG_L1 ... Training model for up to 5388.76s of the 8088.03s of remaining time.\n\u001b[36m(_dystack pid=210)\u001b[0m \tNo valid features to train LightGBM_r15_BAG_L1... Skipping this model.\n\u001b[36m(_dystack pid=210)\u001b[0m Fitting model: NeuralNetTorch_r158_BAG_L1 ... Training model for up to 5388.66s of the 8087.93s of remaining time.\n\u001b[36m(_dystack pid=210)\u001b[0m \tNo valid features to train NeuralNetTorch_r158_BAG_L1... Skipping this model.\n\u001b[36m(_dystack pid=210)\u001b[0m Fitting model: CatBoost_r86_BAG_L1 ... Training model for up to 5388.52s of the 8087.79s of remaining time.\n\u001b[36m(_dystack pid=210)\u001b[0m \tNo valid features to train CatBoost_r86_BAG_L1... Skipping this model.\n\u001b[36m(_dystack pid=210)\u001b[0m Fitting model: NeuralNetFastAI_r37_BAG_L1 ... Training model for up to 5388.39s of the 8087.66s of remaining time.\n\u001b[36m(_dystack pid=210)\u001b[0m \tNo valid features to train NeuralNetFastAI_r37_BAG_L1... Skipping this model.\n\u001b[36m(_dystack pid=210)\u001b[0m Fitting model: NeuralNetTorch_r197_BAG_L1 ... Training model for up to 5388.26s of the 8087.53s of remaining time.\n\u001b[36m(_dystack pid=210)\u001b[0m \tNo valid features to train NeuralNetTorch_r197_BAG_L1... Skipping this model.\n\u001b[36m(_dystack pid=210)\u001b[0m Fitting model: CatBoost_r49_BAG_L1 ... Training model for up to 5388.12s of the 8087.40s of remaining time.\n\u001b[36m(_dystack pid=210)\u001b[0m \tNo valid features to train CatBoost_r49_BAG_L1... Skipping this model.\n\u001b[36m(_dystack pid=210)\u001b[0m Fitting model: ExtraTrees_r49_BAG_L1 ... Training model for up to 5387.99s of the 8087.26s of remaining time.\n\u001b[36m(_dystack pid=210)\u001b[0m \tNo valid features to train ExtraTrees_r49_BAG_L1... Skipping this model.\n\u001b[36m(_dystack pid=210)\u001b[0m Fitting model: LightGBM_r143_BAG_L1 ... Training model for up to 5387.89s of the 8087.16s of remaining time.\n\u001b[36m(_dystack pid=210)\u001b[0m \tNo valid features to train LightGBM_r143_BAG_L1... Skipping this model.\n\u001b[36m(_dystack pid=210)\u001b[0m Fitting model: RandomForest_r127_BAG_L1 ... Training model for up to 5387.79s of the 8087.07s of remaining time.\n\u001b[36m(_dystack pid=210)\u001b[0m \tNo valid features to train RandomForest_r127_BAG_L1... Skipping this model.\n\u001b[36m(_dystack pid=210)\u001b[0m Fitting model: NeuralNetFastAI_r134_BAG_L1 ... Training model for up to 5387.70s of the 8086.98s of remaining time.\n\u001b[36m(_dystack pid=210)\u001b[0m \tNo valid features to train NeuralNetFastAI_r134_BAG_L1... Skipping this model.\n\u001b[36m(_dystack pid=210)\u001b[0m Fitting model: RandomForest_r34_BAG_L1 ... Training model for up to 5387.61s of the 8086.89s of remaining time.\n\u001b[36m(_dystack pid=210)\u001b[0m \tNo valid features to train RandomForest_r34_BAG_L1... Skipping this model.\n\u001b[36m(_dystack pid=210)\u001b[0m Fitting model: LightGBM_r94_BAG_L1 ... Training model for up to 5387.52s of the 8086.80s of remaining time.\n\u001b[36m(_dystack pid=210)\u001b[0m \tNo valid features to train LightGBM_r94_BAG_L1... Skipping this model.\n\u001b[36m(_dystack pid=210)\u001b[0m Fitting model: NeuralNetTorch_r143_BAG_L1 ... Training model for up to 5387.43s of the 8086.71s of remaining time.\n\u001b[36m(_dystack pid=210)\u001b[0m \tNo valid features to train NeuralNetTorch_r143_BAG_L1... Skipping this model.\n\u001b[36m(_dystack pid=210)\u001b[0m Fitting model: CatBoost_r128_BAG_L1 ... Training model for up to 5387.34s of the 8086.62s of remaining time.\n\u001b[36m(_dystack pid=210)\u001b[0m \tNo valid features to train CatBoost_r128_BAG_L1... Skipping this model.\n\u001b[36m(_dystack pid=210)\u001b[0m Fitting model: NeuralNetFastAI_r111_BAG_L1 ... Training model for up to 5387.25s of the 8086.53s of remaining time.\n\u001b[36m(_dystack pid=210)\u001b[0m \tNo valid features to train NeuralNetFastAI_r111_BAG_L1... Skipping this model.\n\u001b[36m(_dystack pid=210)\u001b[0m Fitting model: NeuralNetTorch_r31_BAG_L1 ... Training model for up to 5387.16s of the 8086.44s of remaining time.\n\u001b[36m(_dystack pid=210)\u001b[0m \tNo valid features to train NeuralNetTorch_r31_BAG_L1... Skipping this model.\n\u001b[36m(_dystack pid=210)\u001b[0m Fitting model: ExtraTrees_r4_BAG_L1 ... Training model for up to 5387.07s of the 8086.35s of remaining time.\n\u001b[36m(_dystack pid=210)\u001b[0m \tNo valid features to train ExtraTrees_r4_BAG_L1... Skipping this model.\n\u001b[36m(_dystack pid=210)\u001b[0m Fitting model: NeuralNetFastAI_r65_BAG_L1 ... Training model for up to 5386.99s of the 8086.26s of remaining time.\n\u001b[36m(_dystack pid=210)\u001b[0m \tNo valid features to train NeuralNetFastAI_r65_BAG_L1... Skipping this model.\n\u001b[36m(_dystack pid=210)\u001b[0m Fitting model: NeuralNetFastAI_r88_BAG_L1 ... Training model for up to 5386.90s of the 8086.17s of remaining time.\n\u001b[36m(_dystack pid=210)\u001b[0m \tNo valid features to train NeuralNetFastAI_r88_BAG_L1... Skipping this model.\n\u001b[36m(_dystack pid=210)\u001b[0m Fitting model: LightGBM_r30_BAG_L1 ... Training model for up to 5386.81s of the 8086.08s of remaining time.\n\u001b[36m(_dystack pid=210)\u001b[0m \tNo valid features to train LightGBM_r30_BAG_L1... Skipping this model.\n\u001b[36m(_dystack pid=210)\u001b[0m Fitting model: XGBoost_r49_BAG_L1 ... Training model for up to 5386.71s of the 8085.99s of remaining time.\n\u001b[36m(_dystack pid=210)\u001b[0m \tNo valid features to train XGBoost_r49_BAG_L1... Skipping this model.\n\u001b[36m(_dystack pid=210)\u001b[0m Fitting model: CatBoost_r5_BAG_L1 ... Training model for up to 5386.63s of the 8085.90s of remaining time.\n\u001b[36m(_dystack pid=210)\u001b[0m \tNo valid features to train CatBoost_r5_BAG_L1... Skipping this model.\n\u001b[36m(_dystack pid=210)\u001b[0m Fitting model: NeuralNetTorch_r87_BAG_L1 ... Training model for up to 5386.54s of the 8085.81s of remaining time.\n\u001b[36m(_dystack pid=210)\u001b[0m \tNo valid features to train NeuralNetTorch_r87_BAG_L1... Skipping this model.\n\u001b[36m(_dystack pid=210)\u001b[0m Fitting model: NeuralNetTorch_r71_BAG_L1 ... Training model for up to 5386.45s of the 8085.72s of remaining time.\n\u001b[36m(_dystack pid=210)\u001b[0m \tNo valid features to train NeuralNetTorch_r71_BAG_L1... Skipping this model.\n\u001b[36m(_dystack pid=210)\u001b[0m Fitting model: CatBoost_r143_BAG_L1 ... Training model for up to 5386.36s of the 8085.63s of remaining time.\n\u001b[36m(_dystack pid=210)\u001b[0m \tNo valid features to train CatBoost_r143_BAG_L1... Skipping this model.\n\u001b[36m(_dystack pid=210)\u001b[0m Fitting model: ExtraTrees_r178_BAG_L1 ... Training model for up to 5386.27s of the 8085.54s of remaining time.\n\u001b[36m(_dystack pid=210)\u001b[0m \tNo valid features to train ExtraTrees_r178_BAG_L1... Skipping this model.\n\u001b[36m(_dystack pid=210)\u001b[0m Fitting model: RandomForest_r166_BAG_L1 ... Training model for up to 5386.18s of the 8085.45s of remaining time.\n\u001b[36m(_dystack pid=210)\u001b[0m \tNo valid features to train RandomForest_r166_BAG_L1... Skipping this model.\n\u001b[36m(_dystack pid=210)\u001b[0m Fitting model: XGBoost_r31_BAG_L1 ... Training model for up to 5386.09s of the 8085.36s of remaining time.\n\u001b[36m(_dystack pid=210)\u001b[0m \tNo valid features to train XGBoost_r31_BAG_L1... Skipping this model.\n\u001b[36m(_dystack pid=210)\u001b[0m Fitting model: NeuralNetTorch_r185_BAG_L1 ... Training model for up to 5386.00s of the 8085.27s of remaining time.\n\u001b[36m(_dystack pid=210)\u001b[0m \tNo valid features to train NeuralNetTorch_r185_BAG_L1... Skipping this model.\n\u001b[36m(_dystack pid=210)\u001b[0m Fitting model: NeuralNetFastAI_r160_BAG_L1 ... Training model for up to 5385.90s of the 8085.17s of remaining time.\n\u001b[36m(_dystack pid=210)\u001b[0m \tNo valid features to train NeuralNetFastAI_r160_BAG_L1... Skipping this model.\n\u001b[36m(_dystack pid=210)\u001b[0m Fitting model: CatBoost_r60_BAG_L1 ... Training model for up to 5385.77s of the 8085.04s of remaining time.\n\u001b[36m(_dystack pid=210)\u001b[0m \tNo valid features to train CatBoost_r60_BAG_L1... Skipping this model.\n\u001b[36m(_dystack pid=210)\u001b[0m Fitting model: RandomForest_r15_BAG_L1 ... Training model for up to 5385.63s of the 8084.91s of remaining time.\n\u001b[36m(_dystack pid=210)\u001b[0m \tNo valid features to train RandomForest_r15_BAG_L1... Skipping this model.\n\u001b[36m(_dystack pid=210)\u001b[0m Fitting model: LightGBM_r135_BAG_L1 ... Training model for up to 5385.50s of the 8084.77s of remaining time.\n\u001b[36m(_dystack pid=210)\u001b[0m \tNo valid features to train LightGBM_r135_BAG_L1... Skipping this model.\n\u001b[36m(_dystack pid=210)\u001b[0m Fitting model: XGBoost_r22_BAG_L1 ... Training model for up to 5385.36s of the 8084.63s of remaining time.\n\u001b[36m(_dystack pid=210)\u001b[0m \tNo valid features to train XGBoost_r22_BAG_L1... Skipping this model.\n\u001b[36m(_dystack pid=210)\u001b[0m Fitting model: NeuralNetFastAI_r69_BAG_L1 ... Training model for up to 5385.23s of the 8084.50s of remaining time.\n\u001b[36m(_dystack pid=210)\u001b[0m \tNo valid features to train NeuralNetFastAI_r69_BAG_L1... Skipping this model.\n\u001b[36m(_dystack pid=210)\u001b[0m Fitting model: CatBoost_r6_BAG_L1 ... Training model for up to 5385.09s of the 8084.37s of remaining time.\n\u001b[36m(_dystack pid=210)\u001b[0m \tNo valid features to train CatBoost_r6_BAG_L1... Skipping this model.\n\u001b[36m(_dystack pid=210)\u001b[0m Fitting model: NeuralNetFastAI_r138_BAG_L1 ... Training model for up to 5384.96s of the 8084.23s of remaining time.\n\u001b[36m(_dystack pid=210)\u001b[0m \tNo valid features to train NeuralNetFastAI_r138_BAG_L1... Skipping this model.\n\u001b[36m(_dystack pid=210)\u001b[0m Fitting model: LightGBM_r121_BAG_L1 ... Training model for up to 5384.85s of the 8084.12s of remaining time.\n\u001b[36m(_dystack pid=210)\u001b[0m \tNo valid features to train LightGBM_r121_BAG_L1... Skipping this model.\n\u001b[36m(_dystack pid=210)\u001b[0m Fitting model: NeuralNetFastAI_r172_BAG_L1 ... Training model for up to 5384.76s of the 8084.03s of remaining time.\n\u001b[36m(_dystack pid=210)\u001b[0m \tNo valid features to train NeuralNetFastAI_r172_BAG_L1... Skipping this model.\n\u001b[36m(_dystack pid=210)\u001b[0m Fitting model: CatBoost_r180_BAG_L1 ... Training model for up to 5384.65s of the 8083.92s of remaining time.\n\u001b[36m(_dystack pid=210)\u001b[0m \tNo valid features to train CatBoost_r180_BAG_L1... Skipping this model.\n\u001b[36m(_dystack pid=210)\u001b[0m Fitting model: NeuralNetTorch_r76_BAG_L1 ... Training model for up to 5384.55s of the 8083.82s of remaining time.\n\u001b[36m(_dystack pid=210)\u001b[0m \tNo valid features to train NeuralNetTorch_r76_BAG_L1... Skipping this model.\n\u001b[36m(_dystack pid=210)\u001b[0m Fitting model: ExtraTrees_r197_BAG_L1 ... Training model for up to 5384.39s of the 8083.67s of remaining time.\n\u001b[36m(_dystack pid=210)\u001b[0m \tNo valid features to train ExtraTrees_r197_BAG_L1... Skipping this model.\n\u001b[36m(_dystack pid=210)\u001b[0m Fitting model: NeuralNetTorch_r121_BAG_L1 ... Training model for up to 5384.29s of the 8083.56s of remaining time.\n\u001b[36m(_dystack pid=210)\u001b[0m \tNo valid features to train NeuralNetTorch_r121_BAG_L1... Skipping this model.\n\u001b[36m(_dystack pid=210)\u001b[0m Fitting model: NeuralNetFastAI_r127_BAG_L1 ... Training model for up to 5384.19s of the 8083.46s of remaining time.\n\u001b[36m(_dystack pid=210)\u001b[0m \tNo valid features to train NeuralNetFastAI_r127_BAG_L1... Skipping this model.\n\u001b[36m(_dystack pid=210)\u001b[0m Fitting model: RandomForest_r16_BAG_L1 ... Training model for up to 5384.10s of the 8083.37s of remaining time.\n\u001b[36m(_dystack pid=210)\u001b[0m \tNo valid features to train RandomForest_r16_BAG_L1... Skipping this model.\n\u001b[36m(_dystack pid=210)\u001b[0m Fitting model: NeuralNetFastAI_r194_BAG_L1 ... Training model for up to 5384.01s of the 8083.29s of remaining time.\n\u001b[36m(_dystack pid=210)\u001b[0m \tNo valid features to train NeuralNetFastAI_r194_BAG_L1... Skipping this model.\n\u001b[36m(_dystack pid=210)\u001b[0m Fitting model: CatBoost_r12_BAG_L1 ... Training model for up to 5383.92s of the 8083.20s of remaining time.\n\u001b[36m(_dystack pid=210)\u001b[0m \tNo valid features to train CatBoost_r12_BAG_L1... Skipping this model.\n\u001b[36m(_dystack pid=210)\u001b[0m Fitting model: NeuralNetTorch_r135_BAG_L1 ... Training model for up to 5383.83s of the 8083.11s of remaining time.\n\u001b[36m(_dystack pid=210)\u001b[0m \tNo valid features to train NeuralNetTorch_r135_BAG_L1... Skipping this model.\n\u001b[36m(_dystack pid=210)\u001b[0m Fitting model: NeuralNetFastAI_r4_BAG_L1 ... Training model for up to 5383.74s of the 8083.01s of remaining time.\n\u001b[36m(_dystack pid=210)\u001b[0m \tNo valid features to train NeuralNetFastAI_r4_BAG_L1... Skipping this model.\n\u001b[36m(_dystack pid=210)\u001b[0m Fitting model: ExtraTrees_r126_BAG_L1 ... Training model for up to 5383.65s of the 8082.92s of remaining time.\n\u001b[36m(_dystack pid=210)\u001b[0m \tNo valid features to train ExtraTrees_r126_BAG_L1... Skipping this model.\n\u001b[36m(_dystack pid=210)\u001b[0m Fitting model: NeuralNetTorch_r36_BAG_L1 ... Training model for up to 5383.56s of the 8082.84s of remaining time.\n\u001b[36m(_dystack pid=210)\u001b[0m \tNo valid features to train NeuralNetTorch_r36_BAG_L1... Skipping this model.\n\u001b[36m(_dystack pid=210)\u001b[0m Fitting model: NeuralNetFastAI_r100_BAG_L1 ... Training model for up to 5383.47s of the 8082.74s of remaining time.\n\u001b[36m(_dystack pid=210)\u001b[0m \tNo valid features to train NeuralNetFastAI_r100_BAG_L1... Skipping this model.\n\u001b[36m(_dystack pid=210)\u001b[0m Fitting model: CatBoost_r163_BAG_L1 ... Training model for up to 5383.38s of the 8082.65s of remaining time.\n\u001b[36m(_dystack pid=210)\u001b[0m \tNo valid features to train CatBoost_r163_BAG_L1... Skipping this model.\n\u001b[36m(_dystack pid=210)\u001b[0m Fitting model: CatBoost_r198_BAG_L1 ... Training model for up to 5383.29s of the 8082.57s of remaining time.\n\u001b[36m(_dystack pid=210)\u001b[0m \tNo valid features to train CatBoost_r198_BAG_L1... Skipping this model.\n\u001b[36m(_dystack pid=210)\u001b[0m Fitting model: NeuralNetFastAI_r187_BAG_L1 ... Training model for up to 5383.20s of the 8082.47s of remaining time.\n\u001b[36m(_dystack pid=210)\u001b[0m \tNo valid features to train NeuralNetFastAI_r187_BAG_L1... Skipping this model.\n\u001b[36m(_dystack pid=210)\u001b[0m Fitting model: NeuralNetTorch_r19_BAG_L1 ... Training model for up to 5383.11s of the 8082.38s of remaining time.\n\u001b[36m(_dystack pid=210)\u001b[0m \tNo valid features to train NeuralNetTorch_r19_BAG_L1... Skipping this model.\n\u001b[36m(_dystack pid=210)\u001b[0m Fitting model: XGBoost_r95_BAG_L1 ... Training model for up to 5383.02s of the 8082.29s of remaining time.\n\u001b[36m(_dystack pid=210)\u001b[0m \tNo valid features to train XGBoost_r95_BAG_L1... Skipping this model.\n\u001b[36m(_dystack pid=210)\u001b[0m Fitting model: XGBoost_r34_BAG_L1 ... Training model for up to 5382.92s of the 8082.19s of remaining time.\n\u001b[36m(_dystack pid=210)\u001b[0m \tNo valid features to train XGBoost_r34_BAG_L1... Skipping this model.\n\u001b[36m(_dystack pid=210)\u001b[0m Fitting model: LightGBM_r42_BAG_L1 ... Training model for up to 5382.82s of the 8082.10s of remaining time.\n\u001b[36m(_dystack pid=210)\u001b[0m \tNo valid features to train LightGBM_r42_BAG_L1... Skipping this model.\n\u001b[36m(_dystack pid=210)\u001b[0m Fitting model: NeuralNetTorch_r1_BAG_L1 ... Training model for up to 5382.73s of the 8082.00s of remaining time.\n\u001b[36m(_dystack pid=210)\u001b[0m \tNo valid features to train NeuralNetTorch_r1_BAG_L1... Skipping this model.\n\u001b[36m(_dystack pid=210)\u001b[0m Fitting model: NeuralNetTorch_r89_BAG_L1 ... Training model for up to 5382.63s of the 8081.90s of remaining time.\n\u001b[36m(_dystack pid=210)\u001b[0m \tNo valid features to train NeuralNetTorch_r89_BAG_L1... Skipping this model.\n\u001b[36m(_dystack pid=210)\u001b[0m No base models to train on, skipping auxiliary stack level 2...\n\u001b[36m(_dystack pid=210)\u001b[0m No base models to train on, skipping stack level 2...\n\u001b[36m(_dystack pid=210)\u001b[0m No base models to train on, skipping auxiliary stack level 3...\n\u001b[36m(_dystack pid=210)\u001b[0m Warning: AutoGluon did not successfully train any models\n\u001b[36m(_dystack pid=210)\u001b[0m AutoGluon training complete, total runtime = 12.08s ... Best model: None\nWarning: Exception encountered during DyStack sub-fit:\n\tNo models were trained successfully during fit(). Inspect the log output or increase verbosity to determine why no models were fit. Alternatively, set `raise_on_no_models_fitted` to False during the fit call.\n\t1\t = Optimal   num_stack_levels (Stacked Overfitting Occurred: False)\n\t20s\t = DyStack   runtime |\t32380s\t = Remaining runtime\nStarting main fit with num_stack_levels=1.\n\tFor future fit calls on this dataset, you can skip DyStack to save time: `predictor.fit(..., dynamic_stacking=False, num_stack_levels=1)`\nValues in column 'fold' used as split folds instead of being automatically set. Bagged models will have 5 splits.\nBeginning AutoGluon training ... Time limit = 32380s\nAutoGluon will save models to \"/kaggle/working/AutogluonModels/ag-20241221_115348\"\nTrain Data Rows:    140700\nTrain Data Columns: 1\nLabel Column:       Depression\nProblem Type:       binary\nPreprocessing data ...\nSelected class <--> label mapping:  class 1 = 1, class 0 = 0\nUsing Feature Generators to preprocess the data ...\nFitting AutoMLPipelineFeatureGenerator...\n\tAvailable Memory:                    30310.52 MB\n\tTrain Data (Original)  Memory Usage: 0.00 MB (0.0% of available memory)\n\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n\tStage 1 Generators:\n\tStage 2 Generators:\n\tStage 3 Generators:\n\tStage 4 Generators:\n\tStage 5 Generators:\n\tWARNING: No useful features were detected in the data! AutoGluon will train using 0 features, and will always predict the same value. Ensure that you are passing the correct data to AutoGluon!\nFitting DummyFeatureGenerator...\n\tTypes of features in original data (raw dtype, special dtypes):\n\tTypes of features in processed data (raw dtype, special dtypes):\n\t\t('int', []) : 1 | ['__dummy__']\n\t0.1s = Fit runtime\n\t0 features in original data used to generate 1 features in processed data.\n\tTrain Data (Processed) Memory Usage: 1.07 MB (0.0% of available memory)\nData preprocessing and feature engineering runtime = 0.12s ...\nAutoGluon will gauge predictive performance using evaluation metric: 'accuracy'\n\tTo change this, specify the eval_metric parameter of Predictor()\nLarge model count detected (112 configs) ... Only displaying the first 3 models of each family. To see all, set `verbosity=3`.\nUser-specified model hyperparameters to be fit:\n{\n\t'NN_TORCH': [{}, {'activation': 'elu', 'dropout_prob': 0.10077639529843717, 'hidden_size': 108, 'learning_rate': 0.002735937344002146, 'num_layers': 4, 'use_batchnorm': True, 'weight_decay': 1.356433327634438e-12, 'ag_args': {'name_suffix': '_r79', 'priority': -2}}, {'activation': 'elu', 'dropout_prob': 0.11897478034205347, 'hidden_size': 213, 'learning_rate': 0.0010474382260641949, 'num_layers': 4, 'use_batchnorm': False, 'weight_decay': 5.594471067786272e-10, 'ag_args': {'name_suffix': '_r22', 'priority': -7}}],\n\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, {'learning_rate': 0.03, 'num_leaves': 128, 'feature_fraction': 0.9, 'min_data_in_leaf': 3, 'ag_args': {'name_suffix': 'Large', 'priority': 0, 'hyperparameter_tune_kwargs': None}}],\n\t'CAT': [{}, {'depth': 6, 'grow_policy': 'SymmetricTree', 'l2_leaf_reg': 2.1542798306067823, 'learning_rate': 0.06864209415792857, 'max_ctr_complexity': 4, 'one_hot_max_size': 10, 'ag_args': {'name_suffix': '_r177', 'priority': -1}}, {'depth': 8, 'grow_policy': 'Depthwise', 'l2_leaf_reg': 2.7997999596449104, 'learning_rate': 0.031375015734637225, 'max_ctr_complexity': 2, 'one_hot_max_size': 3, 'ag_args': {'name_suffix': '_r9', 'priority': -5}}],\n\t'XGB': [{}, {'colsample_bytree': 0.6917311125174739, 'enable_categorical': False, 'learning_rate': 0.018063876087523967, 'max_depth': 10, 'min_child_weight': 0.6028633586934382, 'ag_args': {'name_suffix': '_r33', 'priority': -8}}, {'colsample_bytree': 0.6628423832084077, 'enable_categorical': False, 'learning_rate': 0.08775715546881824, 'max_depth': 5, 'min_child_weight': 0.6294123374222513, 'ag_args': {'name_suffix': '_r89', 'priority': -16}}],\n\t'FASTAI': [{}, {'bs': 256, 'emb_drop': 0.5411770367537934, 'epochs': 43, 'layers': [800, 400], 'lr': 0.01519848858318159, 'ps': 0.23782946566604385, 'ag_args': {'name_suffix': '_r191', 'priority': -4}}, {'bs': 2048, 'emb_drop': 0.05070411322605811, 'epochs': 29, 'layers': [200, 100], 'lr': 0.08974235041576624, 'ps': 0.10393466140748028, 'ag_args': {'name_suffix': '_r102', 'priority': -11}}],\n\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n}\nAutoGluon will fit 2 stack levels (L1 to L2) ...\nFitting 110 L1 models, fit_strategy=\"sequential\" ...\nFitting model: KNeighborsUnif_BAG_L1 ... Training model for up to 21580.89s of the 32379.42s of remaining time.\n\tNo valid features to train KNeighborsUnif_BAG_L1... Skipping this model.\nFitting model: KNeighborsDist_BAG_L1 ... Training model for up to 21580.64s of the 32379.17s of remaining time.\n\tNo valid features to train KNeighborsDist_BAG_L1... Skipping this model.\nFitting model: LightGBMXT_BAG_L1 ... Training model for up to 21580.44s of the 32378.96s of remaining time.\n\tNo valid features to train LightGBMXT_BAG_L1... Skipping this model.\nFitting model: LightGBM_BAG_L1 ... Training model for up to 21580.23s of the 32378.75s of remaining time.\n\tNo valid features to train LightGBM_BAG_L1... Skipping this model.\nFitting model: RandomForestGini_BAG_L1 ... Training model for up to 21580.03s of the 32378.55s of remaining time.\n\tNo valid features to train RandomForestGini_BAG_L1... Skipping this model.\nFitting model: RandomForestEntr_BAG_L1 ... Training model for up to 21579.86s of the 32378.39s of remaining time.\n\tNo valid features to train RandomForestEntr_BAG_L1... Skipping this model.\nFitting model: CatBoost_BAG_L1 ... Training model for up to 21579.67s of the 32378.19s of remaining time.\n\tNo valid features to train CatBoost_BAG_L1... Skipping this model.\nFitting model: ExtraTreesGini_BAG_L1 ... Training model for up to 21579.51s of the 32378.03s of remaining time.\n\tNo valid features to train ExtraTreesGini_BAG_L1... Skipping this model.\nFitting model: ExtraTreesEntr_BAG_L1 ... Training model for up to 21579.37s of the 32377.90s of remaining time.\n\tNo valid features to train ExtraTreesEntr_BAG_L1... Skipping this model.\nFitting model: NeuralNetFastAI_BAG_L1 ... Training model for up to 21579.17s of the 32377.70s of remaining time.\n\tNo valid features to train NeuralNetFastAI_BAG_L1... Skipping this model.\nFitting model: XGBoost_BAG_L1 ... Training model for up to 21579.02s of the 32377.54s of remaining time.\n\tNo valid features to train XGBoost_BAG_L1... Skipping this model.\nFitting model: NeuralNetTorch_BAG_L1 ... Training model for up to 21578.81s of the 32377.34s of remaining time.\n\tNo valid features to train NeuralNetTorch_BAG_L1... Skipping this model.\nFitting model: LightGBMLarge_BAG_L1 ... Training model for up to 21578.59s of the 32377.11s of remaining time.\n\tNo valid features to train LightGBMLarge_BAG_L1... Skipping this model.\nFitting model: CatBoost_r177_BAG_L1 ... Training model for up to 21578.42s of the 32376.95s of remaining time.\n\tNo valid features to train CatBoost_r177_BAG_L1... Skipping this model.\nFitting model: NeuralNetTorch_r79_BAG_L1 ... Training model for up to 21578.29s of the 32376.81s of remaining time.\n\tNo valid features to train NeuralNetTorch_r79_BAG_L1... Skipping this model.\nFitting model: LightGBM_r131_BAG_L1 ... Training model for up to 21578.15s of the 32376.67s of remaining time.\n\tNo valid features to train LightGBM_r131_BAG_L1... Skipping this model.\nFitting model: NeuralNetFastAI_r191_BAG_L1 ... Training model for up to 21578.01s of the 32376.54s of remaining time.\n\tNo valid features to train NeuralNetFastAI_r191_BAG_L1... Skipping this model.\nFitting model: CatBoost_r9_BAG_L1 ... Training model for up to 21577.87s of the 32376.40s of remaining time.\n\tNo valid features to train CatBoost_r9_BAG_L1... Skipping this model.\nFitting model: LightGBM_r96_BAG_L1 ... Training model for up to 21577.74s of the 32376.26s of remaining time.\n\tNo valid features to train LightGBM_r96_BAG_L1... Skipping this model.\nFitting model: NeuralNetTorch_r22_BAG_L1 ... Training model for up to 21577.54s of the 32376.06s of remaining time.\n\tNo valid features to train NeuralNetTorch_r22_BAG_L1... Skipping this model.\nFitting model: XGBoost_r33_BAG_L1 ... Training model for up to 21577.33s of the 32375.86s of remaining time.\n\tNo valid features to train XGBoost_r33_BAG_L1... Skipping this model.\nFitting model: ExtraTrees_r42_BAG_L1 ... Training model for up to 21577.13s of the 32375.66s of remaining time.\n\tNo valid features to train ExtraTrees_r42_BAG_L1... Skipping this model.\nFitting model: CatBoost_r137_BAG_L1 ... Training model for up to 21576.93s of the 32375.46s of remaining time.\n\tNo valid features to train CatBoost_r137_BAG_L1... Skipping this model.\nFitting model: NeuralNetFastAI_r102_BAG_L1 ... Training model for up to 21576.73s of the 32375.25s of remaining time.\n\tNo valid features to train NeuralNetFastAI_r102_BAG_L1... Skipping this model.\nFitting model: CatBoost_r13_BAG_L1 ... Training model for up to 21576.58s of the 32375.11s of remaining time.\n\tNo valid features to train CatBoost_r13_BAG_L1... Skipping this model.\nFitting model: RandomForest_r195_BAG_L1 ... Training model for up to 21576.39s of the 32374.91s of remaining time.\n\tNo valid features to train RandomForest_r195_BAG_L1... Skipping this model.\nFitting model: LightGBM_r188_BAG_L1 ... Training model for up to 21576.19s of the 32374.72s of remaining time.\n\tNo valid features to train LightGBM_r188_BAG_L1... Skipping this model.\nFitting model: NeuralNetFastAI_r145_BAG_L1 ... Training model for up to 21575.99s of the 32374.51s of remaining time.\n\tNo valid features to train NeuralNetFastAI_r145_BAG_L1... Skipping this model.\nFitting model: XGBoost_r89_BAG_L1 ... Training model for up to 21575.79s of the 32374.32s of remaining time.\n\tNo valid features to train XGBoost_r89_BAG_L1... Skipping this model.\nFitting model: NeuralNetTorch_r30_BAG_L1 ... Training model for up to 21575.63s of the 32374.16s of remaining time.\n\tNo valid features to train NeuralNetTorch_r30_BAG_L1... Skipping this model.\nFitting model: LightGBM_r130_BAG_L1 ... Training model for up to 21575.50s of the 32374.02s of remaining time.\n\tNo valid features to train LightGBM_r130_BAG_L1... Skipping this model.\nFitting model: NeuralNetTorch_r86_BAG_L1 ... Training model for up to 21575.36s of the 32373.88s of remaining time.\n\tNo valid features to train NeuralNetTorch_r86_BAG_L1... Skipping this model.\nFitting model: CatBoost_r50_BAG_L1 ... Training model for up to 21575.22s of the 32373.75s of remaining time.\n\tNo valid features to train CatBoost_r50_BAG_L1... Skipping this model.\nFitting model: NeuralNetFastAI_r11_BAG_L1 ... Training model for up to 21575.09s of the 32373.61s of remaining time.\n\tNo valid features to train NeuralNetFastAI_r11_BAG_L1... Skipping this model.\nFitting model: XGBoost_r194_BAG_L1 ... Training model for up to 21574.96s of the 32373.48s of remaining time.\n\tNo valid features to train XGBoost_r194_BAG_L1... Skipping this model.\nFitting model: ExtraTrees_r172_BAG_L1 ... Training model for up to 21574.82s of the 32373.34s of remaining time.\n\tNo valid features to train ExtraTrees_r172_BAG_L1... Skipping this model.\nFitting model: CatBoost_r69_BAG_L1 ... Training model for up to 21574.68s of the 32373.20s of remaining time.\n\tNo valid features to train CatBoost_r69_BAG_L1... Skipping this model.\nFitting model: NeuralNetFastAI_r103_BAG_L1 ... Training model for up to 21574.54s of the 32373.07s of remaining time.\n\tNo valid features to train NeuralNetFastAI_r103_BAG_L1... Skipping this model.\nFitting model: NeuralNetTorch_r14_BAG_L1 ... Training model for up to 21574.40s of the 32372.93s of remaining time.\n\tNo valid features to train NeuralNetTorch_r14_BAG_L1... Skipping this model.\nFitting model: LightGBM_r161_BAG_L1 ... Training model for up to 21574.27s of the 32372.79s of remaining time.\n\tNo valid features to train LightGBM_r161_BAG_L1... Skipping this model.\nFitting model: NeuralNetFastAI_r143_BAG_L1 ... Training model for up to 21574.13s of the 32372.65s of remaining time.\n\tNo valid features to train NeuralNetFastAI_r143_BAG_L1... Skipping this model.\nFitting model: CatBoost_r70_BAG_L1 ... Training model for up to 21573.99s of the 32372.52s of remaining time.\n\tNo valid features to train CatBoost_r70_BAG_L1... Skipping this model.\nFitting model: NeuralNetFastAI_r156_BAG_L1 ... Training model for up to 21573.86s of the 32372.39s of remaining time.\n\tNo valid features to train NeuralNetFastAI_r156_BAG_L1... Skipping this model.\nFitting model: LightGBM_r196_BAG_L1 ... Training model for up to 21573.72s of the 32372.25s of remaining time.\n\tNo valid features to train LightGBM_r196_BAG_L1... Skipping this model.\nFitting model: RandomForest_r39_BAG_L1 ... Training model for up to 21573.58s of the 32372.10s of remaining time.\n\tNo valid features to train RandomForest_r39_BAG_L1... Skipping this model.\nFitting model: CatBoost_r167_BAG_L1 ... Training model for up to 21573.44s of the 32371.96s of remaining time.\n\tNo valid features to train CatBoost_r167_BAG_L1... Skipping this model.\nFitting model: NeuralNetFastAI_r95_BAG_L1 ... Training model for up to 21573.29s of the 32371.82s of remaining time.\n\tNo valid features to train NeuralNetFastAI_r95_BAG_L1... Skipping this model.\nFitting model: NeuralNetTorch_r41_BAG_L1 ... Training model for up to 21573.16s of the 32371.68s of remaining time.\n\tNo valid features to train NeuralNetTorch_r41_BAG_L1... Skipping this model.\nFitting model: XGBoost_r98_BAG_L1 ... Training model for up to 21573.02s of the 32371.55s of remaining time.\n\tNo valid features to train XGBoost_r98_BAG_L1... Skipping this model.\nFitting model: LightGBM_r15_BAG_L1 ... Training model for up to 21572.88s of the 32371.41s of remaining time.\n\tNo valid features to train LightGBM_r15_BAG_L1... Skipping this model.\nFitting model: NeuralNetTorch_r158_BAG_L1 ... Training model for up to 21572.75s of the 32371.27s of remaining time.\n\tNo valid features to train NeuralNetTorch_r158_BAG_L1... Skipping this model.\nFitting model: CatBoost_r86_BAG_L1 ... Training model for up to 21572.55s of the 32371.07s of remaining time.\n\tNo valid features to train CatBoost_r86_BAG_L1... Skipping this model.\nFitting model: NeuralNetFastAI_r37_BAG_L1 ... Training model for up to 21572.39s of the 32370.91s of remaining time.\n\tNo valid features to train NeuralNetFastAI_r37_BAG_L1... Skipping this model.\nFitting model: NeuralNetTorch_r197_BAG_L1 ... Training model for up to 21572.19s of the 32370.71s of remaining time.\n\tNo valid features to train NeuralNetTorch_r197_BAG_L1... Skipping this model.\nFitting model: CatBoost_r49_BAG_L1 ... Training model for up to 21571.99s of the 32370.51s of remaining time.\n\tNo valid features to train CatBoost_r49_BAG_L1... Skipping this model.\nFitting model: ExtraTrees_r49_BAG_L1 ... Training model for up to 21571.78s of the 32370.31s of remaining time.\n\tNo valid features to train ExtraTrees_r49_BAG_L1... Skipping this model.\nFitting model: LightGBM_r143_BAG_L1 ... Training model for up to 21571.59s of the 32370.12s of remaining time.\n\tNo valid features to train LightGBM_r143_BAG_L1... Skipping this model.\nFitting model: RandomForest_r127_BAG_L1 ... Training model for up to 21571.45s of the 32369.97s of remaining time.\n\tNo valid features to train RandomForest_r127_BAG_L1... Skipping this model.\nFitting model: NeuralNetFastAI_r134_BAG_L1 ... Training model for up to 21571.31s of the 32369.84s of remaining time.\n\tNo valid features to train NeuralNetFastAI_r134_BAG_L1... Skipping this model.\nFitting model: RandomForest_r34_BAG_L1 ... Training model for up to 21571.11s of the 32369.64s of remaining time.\n\tNo valid features to train RandomForest_r34_BAG_L1... Skipping this model.\nFitting model: LightGBM_r94_BAG_L1 ... Training model for up to 21570.91s of the 32369.43s of remaining time.\n\tNo valid features to train LightGBM_r94_BAG_L1... Skipping this model.\nFitting model: NeuralNetTorch_r143_BAG_L1 ... Training model for up to 21570.70s of the 32369.23s of remaining time.\n\tNo valid features to train NeuralNetTorch_r143_BAG_L1... Skipping this model.\nFitting model: CatBoost_r128_BAG_L1 ... Training model for up to 21570.53s of the 32369.06s of remaining time.\n\tNo valid features to train CatBoost_r128_BAG_L1... Skipping this model.\nFitting model: NeuralNetFastAI_r111_BAG_L1 ... Training model for up to 21570.40s of the 32368.92s of remaining time.\n\tNo valid features to train NeuralNetFastAI_r111_BAG_L1... Skipping this model.\nFitting model: NeuralNetTorch_r31_BAG_L1 ... Training model for up to 21570.27s of the 32368.79s of remaining time.\n\tNo valid features to train NeuralNetTorch_r31_BAG_L1... Skipping this model.\nFitting model: ExtraTrees_r4_BAG_L1 ... Training model for up to 21570.13s of the 32368.65s of remaining time.\n\tNo valid features to train ExtraTrees_r4_BAG_L1... Skipping this model.\nFitting model: NeuralNetFastAI_r65_BAG_L1 ... Training model for up to 21569.99s of the 32368.52s of remaining time.\n\tNo valid features to train NeuralNetFastAI_r65_BAG_L1... Skipping this model.\nFitting model: NeuralNetFastAI_r88_BAG_L1 ... Training model for up to 21569.86s of the 32368.38s of remaining time.\n\tNo valid features to train NeuralNetFastAI_r88_BAG_L1... Skipping this model.\nFitting model: LightGBM_r30_BAG_L1 ... Training model for up to 21569.72s of the 32368.25s of remaining time.\n\tNo valid features to train LightGBM_r30_BAG_L1... Skipping this model.\nFitting model: XGBoost_r49_BAG_L1 ... Training model for up to 21569.59s of the 32368.12s of remaining time.\n\tNo valid features to train XGBoost_r49_BAG_L1... Skipping this model.\nFitting model: CatBoost_r5_BAG_L1 ... Training model for up to 21569.46s of the 32367.98s of remaining time.\n\tNo valid features to train CatBoost_r5_BAG_L1... Skipping this model.\nFitting model: NeuralNetTorch_r87_BAG_L1 ... Training model for up to 21569.26s of the 32367.78s of remaining time.\n\tNo valid features to train NeuralNetTorch_r87_BAG_L1... Skipping this model.\nFitting model: NeuralNetTorch_r71_BAG_L1 ... Training model for up to 21569.05s of the 32367.58s of remaining time.\n\tNo valid features to train NeuralNetTorch_r71_BAG_L1... Skipping this model.\nFitting model: CatBoost_r143_BAG_L1 ... Training model for up to 21568.85s of the 32367.37s of remaining time.\n\tNo valid features to train CatBoost_r143_BAG_L1... Skipping this model.\nFitting model: ExtraTrees_r178_BAG_L1 ... Training model for up to 21568.63s of the 32367.15s of remaining time.\n\tNo valid features to train ExtraTrees_r178_BAG_L1... Skipping this model.\nFitting model: RandomForest_r166_BAG_L1 ... Training model for up to 21568.46s of the 32366.98s of remaining time.\n\tNo valid features to train RandomForest_r166_BAG_L1... Skipping this model.\nFitting model: XGBoost_r31_BAG_L1 ... Training model for up to 21568.31s of the 32366.84s of remaining time.\n\tNo valid features to train XGBoost_r31_BAG_L1... Skipping this model.\nFitting model: NeuralNetTorch_r185_BAG_L1 ... Training model for up to 21568.18s of the 32366.70s of remaining time.\n\tNo valid features to train NeuralNetTorch_r185_BAG_L1... Skipping this model.\nFitting model: NeuralNetFastAI_r160_BAG_L1 ... Training model for up to 21568.04s of the 32366.57s of remaining time.\n\tNo valid features to train NeuralNetFastAI_r160_BAG_L1... Skipping this model.\nFitting model: CatBoost_r60_BAG_L1 ... Training model for up to 21567.90s of the 32366.43s of remaining time.\n\tNo valid features to train CatBoost_r60_BAG_L1... Skipping this model.\nFitting model: RandomForest_r15_BAG_L1 ... Training model for up to 21567.77s of the 32366.29s of remaining time.\n\tNo valid features to train RandomForest_r15_BAG_L1... Skipping this model.\nFitting model: LightGBM_r135_BAG_L1 ... Training model for up to 21567.63s of the 32366.15s of remaining time.\n\tNo valid features to train LightGBM_r135_BAG_L1... Skipping this model.\nFitting model: XGBoost_r22_BAG_L1 ... Training model for up to 21567.49s of the 32366.01s of remaining time.\n\tNo valid features to train XGBoost_r22_BAG_L1... Skipping this model.\nFitting model: NeuralNetFastAI_r69_BAG_L1 ... Training model for up to 21567.36s of the 32365.88s of remaining time.\n\tNo valid features to train NeuralNetFastAI_r69_BAG_L1... Skipping this model.\nFitting model: CatBoost_r6_BAG_L1 ... Training model for up to 21567.22s of the 32365.74s of remaining time.\n\tNo valid features to train CatBoost_r6_BAG_L1... Skipping this model.\nFitting model: NeuralNetFastAI_r138_BAG_L1 ... Training model for up to 21567.08s of the 32365.60s of remaining time.\n\tNo valid features to train NeuralNetFastAI_r138_BAG_L1... Skipping this model.\nFitting model: LightGBM_r121_BAG_L1 ... Training model for up to 21566.94s of the 32365.47s of remaining time.\n\tNo valid features to train LightGBM_r121_BAG_L1... Skipping this model.\nFitting model: NeuralNetFastAI_r172_BAG_L1 ... Training model for up to 21566.80s of the 32365.33s of remaining time.\n\tNo valid features to train NeuralNetFastAI_r172_BAG_L1... Skipping this model.\nFitting model: CatBoost_r180_BAG_L1 ... Training model for up to 21566.66s of the 32365.19s of remaining time.\n\tNo valid features to train CatBoost_r180_BAG_L1... Skipping this model.\nFitting model: NeuralNetTorch_r76_BAG_L1 ... Training model for up to 21566.52s of the 32365.05s of remaining time.\n\tNo valid features to train NeuralNetTorch_r76_BAG_L1... Skipping this model.\nFitting model: ExtraTrees_r197_BAG_L1 ... Training model for up to 21566.32s of the 32364.84s of remaining time.\n\tNo valid features to train ExtraTrees_r197_BAG_L1... Skipping this model.\nFitting model: NeuralNetTorch_r121_BAG_L1 ... Training model for up to 21566.12s of the 32364.64s of remaining time.\n\tNo valid features to train NeuralNetTorch_r121_BAG_L1... Skipping this model.\nFitting model: NeuralNetFastAI_r127_BAG_L1 ... Training model for up to 21565.91s of the 32364.44s of remaining time.\n\tNo valid features to train NeuralNetFastAI_r127_BAG_L1... Skipping this model.\nFitting model: RandomForest_r16_BAG_L1 ... Training model for up to 21565.71s of the 32364.23s of remaining time.\n\tNo valid features to train RandomForest_r16_BAG_L1... Skipping this model.\nFitting model: NeuralNetFastAI_r194_BAG_L1 ... Training model for up to 21565.49s of the 32364.02s of remaining time.\n\tNo valid features to train NeuralNetFastAI_r194_BAG_L1... Skipping this model.\nFitting model: CatBoost_r12_BAG_L1 ... Training model for up to 21565.29s of the 32363.81s of remaining time.\n\tNo valid features to train CatBoost_r12_BAG_L1... Skipping this model.\nFitting model: NeuralNetTorch_r135_BAG_L1 ... Training model for up to 21565.08s of the 32363.60s of remaining time.\n\tNo valid features to train NeuralNetTorch_r135_BAG_L1... Skipping this model.\nFitting model: NeuralNetFastAI_r4_BAG_L1 ... Training model for up to 21564.88s of the 32363.40s of remaining time.\n\tNo valid features to train NeuralNetFastAI_r4_BAG_L1... Skipping this model.\nFitting model: ExtraTrees_r126_BAG_L1 ... Training model for up to 21564.67s of the 32363.19s of remaining time.\n\tNo valid features to train ExtraTrees_r126_BAG_L1... Skipping this model.\nFitting model: NeuralNetTorch_r36_BAG_L1 ... Training model for up to 21564.46s of the 32362.98s of remaining time.\n\tNo valid features to train NeuralNetTorch_r36_BAG_L1... Skipping this model.\nFitting model: NeuralNetFastAI_r100_BAG_L1 ... Training model for up to 21564.25s of the 32362.77s of remaining time.\n\tNo valid features to train NeuralNetFastAI_r100_BAG_L1... Skipping this model.\nFitting model: CatBoost_r163_BAG_L1 ... Training model for up to 21564.05s of the 32362.57s of remaining time.\n\tNo valid features to train CatBoost_r163_BAG_L1... Skipping this model.\nFitting model: CatBoost_r198_BAG_L1 ... Training model for up to 21563.84s of the 32362.36s of remaining time.\n\tNo valid features to train CatBoost_r198_BAG_L1... Skipping this model.\nFitting model: NeuralNetFastAI_r187_BAG_L1 ... Training model for up to 21563.63s of the 32362.16s of remaining time.\n\tNo valid features to train NeuralNetFastAI_r187_BAG_L1... Skipping this model.\nFitting model: NeuralNetTorch_r19_BAG_L1 ... Training model for up to 21563.42s of the 32361.94s of remaining time.\n\tNo valid features to train NeuralNetTorch_r19_BAG_L1... Skipping this model.\nFitting model: XGBoost_r95_BAG_L1 ... Training model for up to 21563.21s of the 32361.73s of remaining time.\n\tNo valid features to train XGBoost_r95_BAG_L1... Skipping this model.\nFitting model: XGBoost_r34_BAG_L1 ... Training model for up to 21563.00s of the 32361.52s of remaining time.\n\tNo valid features to train XGBoost_r34_BAG_L1... Skipping this model.\nFitting model: LightGBM_r42_BAG_L1 ... Training model for up to 21562.79s of the 32361.31s of remaining time.\n\tNo valid features to train LightGBM_r42_BAG_L1... Skipping this model.\nFitting model: NeuralNetTorch_r1_BAG_L1 ... Training model for up to 21562.58s of the 32361.10s of remaining time.\n\tNo valid features to train NeuralNetTorch_r1_BAG_L1... Skipping this model.\nFitting model: NeuralNetTorch_r89_BAG_L1 ... Training model for up to 21562.38s of the 32360.90s of remaining time.\n\tNo valid features to train NeuralNetTorch_r89_BAG_L1... Skipping this model.\nNo base models to train on, skipping auxiliary stack level 2...\nNo base models to train on, skipping stack level 2...\nNo base models to train on, skipping auxiliary stack level 3...\nWarning: AutoGluon did not successfully train any models\nAutoGluon training complete, total runtime = 18.84s ... Best model: None\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m<ipython-input-11-4cdeae632557>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m predictor.fit(\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mtrain_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mtime_limit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mCFG\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime_limit\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mpresets\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'best_quality'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     ag_args_fit={\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/autogluon/core/utils/decorators.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m             \u001b[0mgargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mother_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mgargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mgkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/autogluon/tabular/predictor/predictor.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, train_data, tuning_data, time_limit, presets, hyperparameters, feature_metadata, infer_limit, infer_limit_batch_size, fit_weighted_ensemble, fit_full_last_level_weighted_ensemble, full_weighted_ensemble_additionally, dynamic_stacking, calibrate_decision_threshold, num_cpus, num_gpus, fit_strategy, memory_limit, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m   1297\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit_strategy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfit_strategy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1298\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1299\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mag_fit_kwargs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mag_fit_kwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mag_post_fit_kwargs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mag_post_fit_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1300\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1301\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/autogluon/tabular/predictor/predictor.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, ag_fit_kwargs, ag_post_fit_kwargs)\u001b[0m\n\u001b[1;32m   1305\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_learner\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mag_fit_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1306\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_post_fit_vars\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1307\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_post_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mag_post_fit_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1308\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1309\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/autogluon/tabular/predictor/predictor.py\u001b[0m in \u001b[0;36m_post_fit\u001b[0;34m(self, keep_only_best, refit_full, set_best_to_refit_full, save_space, calibrate, calibrate_decision_threshold, infer_limit, num_cpus, num_gpus, refit_full_kwargs, fit_strategy, raise_on_no_models_fitted)\u001b[0m\n\u001b[1;32m   1628\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_names\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1629\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mraise_on_no_models_fitted\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1630\u001b[0;31m                 raise RuntimeError(\n\u001b[0m\u001b[1;32m   1631\u001b[0m                     \u001b[0;34m\"No models were trained successfully during fit().\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1632\u001b[0m                     \u001b[0;34m\" Inspect the log output or increase verbosity to determine why no models were fit.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mRuntimeError\u001b[0m: No models were trained successfully during fit(). Inspect the log output or increase verbosity to determine why no models were fit. Alternatively, set `raise_on_no_models_fitted` to False during the fit call."],"ename":"RuntimeError","evalue":"No models were trained successfully during fit(). Inspect the log output or increase verbosity to determine why no models were fit. Alternatively, set `raise_on_no_models_fitted` to False during the fit call.","output_type":"error"}],"execution_count":11},{"cell_type":"code","source":"predictor.leaderboard(silent=True).style.background_gradient(subset=['score_val'], cmap='RdYlGn')\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-21T11:54:28.635055Z","iopub.status.idle":"2024-12-21T11:54:28.635478Z","shell.execute_reply":"2024-12-21T11:54:28.635274Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def get_ensemble_weights(predictor):\n    info = predictor.info()\n    ensemble_weights = {}\n    for model_name, values in info[\"model_info\"].items():\n        if \"Ensemble\" in model_name:\n            children_info = values[\"children_info\"]\n            ensemble_weights[model_name] = values[\"children_info\"][list(children_info.keys())[0]][\"model_weights\"]\n    return ensemble_weights","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-21T11:54:28.636722Z","iopub.status.idle":"2024-12-21T11:54:28.637235Z","shell.execute_reply":"2024-12-21T11:54:28.637039Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"ensemble_weights = get_ensemble_weights(predictor)\n\nfor key, value in ensemble_weights.items():\n    plt.figure(figsize=(8, 8))\n    plt.pie(value.values(), labels=value.keys(), autopct='%1.1f%%', colors=sns.color_palette('Set2', len(value)))\n    plt.title(key)\n    plt.tight_layout()\n    plt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-21T11:54:28.638573Z","iopub.status.idle":"2024-12-21T11:54:28.639061Z","shell.execute_reply":"2024-12-21T11:54:28.638852Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"split = StratifiedKFold(n_splits=CFG.n_folds, random_state=CFG.seed, shuffle=True).split(train, train[CFG.target])\nfor fold_idx, (train_index, val_index) in enumerate(split):\n    for model in predictor.model_names():\n        model_oof_pred_probs = predictor.predict_proba_oof(model).values[:, 1]\n        fold_score = accuracy_score(train.loc[val_index, CFG.target], model_oof_pred_probs[val_index].round())\n        if model not in scores:\n            scores[model] = []\n        scores[model].append(fold_score)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-21T11:54:28.640487Z","iopub.status.idle":"2024-12-21T11:54:28.641011Z","shell.execute_reply":"2024-12-21T11:54:28.640798Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"scores = pd.DataFrame(scores)\nmean_scores = scores.mean().sort_values(ascending=False)\norder = scores.mean().sort_values(ascending=False).index.tolist()\n\nmin_score = mean_scores.min()\nmax_score = mean_scores.max()\npadding = (max_score - min_score) * 0.5\nlower_limit = min_score - padding\nupper_limit = max_score + padding\n\nfig, axs = plt.subplots(1, 2, figsize=(15, scores.shape[1] * 0.4))\n\nsns.boxplot(data=scores, order=order, ax=axs[0], orient='h', palette='RdYlGn_r')\naxs[0].set_title('Fold AUC')\naxs[0].set_xlabel('')\naxs[0].set_ylabel('')\n\nbarplot = sns.barplot(x=mean_scores.values, y=mean_scores.index, ax=axs[1], palette='RdYlGn_r')\naxs[1].set_title('Average AUC')\naxs[1].set_xlabel('')\naxs[1].set_xlim(left=lower_limit, right=upper_limit)\naxs[1].set_ylabel('')\n\nfor i, score in enumerate(mean_scores.values):\n    barplot.text(score, i, round(score, 6), va='center')\n\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-21T11:54:28.641842Z","iopub.status.idle":"2024-12-21T11:54:28.642345Z","shell.execute_reply":"2024-12-21T11:54:28.642117Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"sub = pd.read_csv(CFG.sample_sub_path)\nsub[CFG.target] = predictor.predict(test).values\nsub.to_csv(f'sub_autogluon_{np.mean(scores[predictor.model_best]):.6f}.csv', index=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-21T11:54:28.643211Z","iopub.status.idle":"2024-12-21T11:54:28.643711Z","shell.execute_reply":"2024-12-21T11:54:28.643505Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"sub\nshutil.rmtree(\"AutogluonModels\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-21T11:54:28.644717Z","iopub.status.idle":"2024-12-21T11:54:28.645188Z","shell.execute_reply":"2024-12-21T11:54:28.644992Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-21T11:54:28.646202Z","iopub.status.idle":"2024-12-21T11:54:28.646698Z","shell.execute_reply":"2024-12-21T11:54:28.646496Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# alternative code 02","metadata":{}},{"cell_type":"code","source":"!pip install -q autogluon.tabular ray==2.10.0","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# block 1\n\nfrom autogluon.tabular import TabularPredictor\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import accuracy_score\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport pandas as pd\nimport numpy as np\nimport warnings\nimport pickle\nimport shutil\nimport glob\n\nwarnings.filterwarnings('ignore')\n\n# block 2\n\nclass CFG:\n    train_path = '/kaggle/input/mental-health-prediction-hackathon/train.csv'\n    test_path = '/kaggle/input/mental-health-prediction-hackathon/test.csv'\n    sample_sub_path = '/kaggle/input/mental-health-prediction-hackathon/sample_submission.csv'\n    \n    oof_path = 'path/to/oof/files'  # Adjust this path to where your OOF files are located\n    \n    target = 'Depression'\n    n_folds = 5\n    seed = 42\n    \n    time_limit = 3600 * 9\n\n# block 3\n\ntrain = pd.read_csv(CFG.train_path, index_col='id')\n\nX = train.drop(CFG.target, axis=1)\ny = train[CFG.target]\n\n# block 4\n\ndef get_data(path):\n    oof_pred_probs_files = glob.glob(f'{path}/*_oof_pred_probs_*.pkl')\n    test_pred_probs_files = glob.glob(f'{path}/*_test_pred_probs_*.pkl')\n\n    oof_pred_probs = pickle.load(open(oof_pred_probs_files[0], 'rb'))\n    test_pred_probs = pickle.load(open(test_pred_probs_files[0], 'rb'))\n    \n    scores = []\n    skf = StratifiedKFold(n_splits=CFG.n_folds, random_state=CFG.seed, shuffle=True)\n    for _, val_idx in skf.split(X, y):\n        y_val = y[val_idx]\n        y_pred_probs = oof_pred_probs[val_idx]   \n        score = accuracy_score(y_val, y_pred_probs.round())\n        scores.append(score)\n        \n    return oof_pred_probs, test_pred_probs, scores\n\n# block 5\n\nscores = {}\noof_pred_probs = {}\ntest_pred_probs = {}\n\n# block 6\n\nmodel_paths = glob.glob(f'{CFG.oof_path}/*')\nfor model_path in model_paths:\n    model_name = model_path.split('/')[-1]\n    oof_pred_probs[model_name], test_pred_probs[model_name], scores[model_name] = get_data(model_path)\n\n# block 7\n\ntrain = pd.DataFrame(oof_pred_probs)\ntrain[CFG.target] = y\n\ntest = pd.DataFrame(test_pred_probs)\n\n# block 8\n\nskf = StratifiedKFold(n_splits=CFG.n_folds, random_state=CFG.seed, shuffle=True)\nsplit = skf.split(train, train[CFG.target])\nfor i, (_, val_index) in enumerate(split):\n    train.loc[val_index, 'fold'] = i\n\n# block 9\n\npredictor = TabularPredictor(\n    problem_type='binary',\n    eval_metric='accuracy',\n    label=CFG.target,\n    groups='fold',\n    verbosity=2\n)\n\n# block 10\n\npredictor.fit(\n    train_data=train,\n    time_limit=CFG.time_limit,\n    presets='best_quality',\n    ag_args_fit={\n        'num_gpus': 1, \n        'num_cpus': 4\n    }\n)\n\n# block 11\n\nleaderboard = predictor.leaderboard(silent=True).style.background_gradient(subset=['score_val'], cmap='RdYlGn')\nprint(\"Leaderboard after training:\")\nprint(leaderboard)\n\n# block 12\n\ndef get_ensemble_weights(predictor):\n    info = predictor.info()\n    ensemble_weights = {}\n    for model_name, values in info[\"model_info\"].items():\n        if \"Ensemble\" in model_name:\n            children_info = values[\"children_info\"]\n            ensemble_weights[model_name] = values[\"children_info\"][list(children_info.keys())[0]][\"model_weights\"]\n    return ensemble_weights\n\n# block 13\n\nensemble_weights = get_ensemble_weights(predictor)\n\nfor key, value in ensemble_weights.items():\n    plt.figure(figsize=(8, 8))\n    plt.pie(value.values(), labels=value.keys(), autopct='%1.1f%%', colors=sns.color_palette('Set2', len(value)))\n    plt.title(key)\n    plt.tight_layout()\n    plt.show()\n\n# block 14\n\nsplit = StratifiedKFold(n_splits=CFG.n_folds, random_state=CFG.seed, shuffle=True).split(train, train[CFG.target])\nfor fold_idx, (train_index, val_index) in enumerate(split):\n    for model in predictor.model_names():\n        model_oof_pred_probs = predictor.predict_proba_oof(model).values[:, 1]\n        fold_score = accuracy_score(train.loc[val_index, CFG.target], model_oof_pred_probs[val_index].round())\n        if model not in scores:\n            scores[model] = []\n        scores[model].append(fold_score)\n\n# block 15\n\nscores = pd.DataFrame(scores)\nmean_scores = scores.mean().sort_values(ascending=False)\norder = scores.mean().sort_values(ascending=False).index.tolist()\n\nmin_score = mean_scores.min()\nmax_score = mean_scores.max()\npadding = (max_score - min_score) * 0.5\nlower_limit = min_score - padding\nupper_limit = max_score + padding\n\nfig, axs = plt.subplots(1, 2, figsize=(15, scores.shape[1] * 0.4))\n\nsns.boxplot(data=scores, order=order, ax=axs[0], orient='h', palette='RdYlGn_r')\naxs[0].set_title('Fold AUC')\naxs[0].set_xlabel('')\naxs[0].set_ylabel('')\n\nbarplot = sns.barplot(x=mean_scores.values, y=mean_scores.index, ax=axs[1], palette='RdYlGn_r')\naxs[1].set_title('Average AUC')\naxs[1].set_xlabel('')\naxs[1].set_xlim(left=lower_limit, right=upper_limit)\naxs[1].set_ylabel('')\n\nfor i, score in enumerate(mean_scores.values):\n    barplot.text(score, i, round(score, 6), va='center')\n\nplt.tight_layout()\nplt.show()\n\n# block 16\n\nsub = pd.read_csv(CFG.sample_sub_path)\nsub[CFG.target] = predictor.predict(test).values\n\ntest_accuracy = accuracy_score(test[CFG.target], sub[CFG.target])\nprint(\"Test Accuracy: \", test_accuracy)\n\nsub.to_csv(f'sub_autogluon_{np.mean(scores.mean()):.6f}.csv', index=False)\n\n# block 17\n\nsub\nshutil.rmtree(\"AutogluonModels\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-21T15:02:05.831568Z","iopub.execute_input":"2024-12-21T15:02:05.831834Z","iopub.status.idle":"2024-12-21T15:02:48.004903Z","shell.execute_reply.started":"2024-12-21T15:02:05.831804Z","shell.execute_reply":"2024-12-21T15:02:48.003672Z"}},"outputs":[{"name":"stdout","text":"\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.1/65.1 MB\u001b[0m \u001b[31m27.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m352.2/352.2 kB\u001b[0m \u001b[31m26.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m266.2/266.2 kB\u001b[0m \u001b[31m20.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.1/64.1 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m68.2/68.2 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.3/13.3 MB\u001b[0m \u001b[31m89.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0m\n\u001b[?25h","output_type":"stream"},{"name":"stderr","text":"No path specified. Models will be saved in: \"AutogluonModels/ag-20241221_150227\"\nVerbosity: 2 (Standard Logging)\n=================== System Info ===================\nAutoGluon Version:  1.2\nPython Version:     3.10.12\nOperating System:   Linux\nPlatform Machine:   x86_64\nPlatform Version:   #1 SMP PREEMPT_DYNAMIC Sun Nov 10 10:07:59 UTC 2024\nCPU Count:          4\nMemory Avail:       29.95 GB / 31.35 GB (95.5%)\nDisk Space Avail:   19.50 GB / 19.52 GB (99.9%)\n===================================================\nPresets specified: ['best_quality']\nSetting dynamic_stacking from 'auto' to True. Reason: Enable dynamic_stacking when use_bag_holdout is disabled. (use_bag_holdout=False)\nStack configuration (auto_stack=True): num_stack_levels=1, num_bag_folds=8, num_bag_sets=1\nDyStack is enabled (dynamic_stacking=True). AutoGluon will try to determine whether the input data is affected by stacked overfitting and enable or disable stacking as a consequence.\n\tThis is used to identify the optimal `num_stack_levels` value. Copies of AutoGluon will be fit on subsets of the data. Then holdout validation data is used to detect stacked overfitting.\n\tRunning DyStack for up to 8100s of the 32400s of remaining time (25%).\n\tRunning DyStack sub-fit in a ray process to avoid memory leakage. Enabling ray logging (enable_ray_logging=True). Specify `ds_args={'enable_ray_logging': False}` if you experience logging issues.\n2024-12-21 15:02:29,659\tINFO worker.py:1752 -- Started a local Ray instance.\n\t\tContext path: \"/kaggle/working/AutogluonModels/ag-20241221_150227/ds_sub_fit/sub_fit_ho\"\n\u001b[36m(_dystack pid=207)\u001b[0m Running DyStack sub-fit ...\n\u001b[36m(_dystack pid=207)\u001b[0m Values in column 'fold' used as split folds instead of being automatically set. Bagged models will have 5 splits.\n\u001b[36m(_dystack pid=207)\u001b[0m Beginning AutoGluon training ... Time limit = 8096s\n\u001b[36m(_dystack pid=207)\u001b[0m AutoGluon will save models to \"/kaggle/working/AutogluonModels/ag-20241221_150227/ds_sub_fit/sub_fit_ho\"\n\u001b[36m(_dystack pid=207)\u001b[0m Train Data Rows:    125066\n\u001b[36m(_dystack pid=207)\u001b[0m Train Data Columns: 1\n\u001b[36m(_dystack pid=207)\u001b[0m Label Column:       Depression\n\u001b[36m(_dystack pid=207)\u001b[0m Problem Type:       binary\n\u001b[36m(_dystack pid=207)\u001b[0m Preprocessing data ...\n\u001b[36m(_dystack pid=207)\u001b[0m Selected class <--> label mapping:  class 1 = 1, class 0 = 0\n\u001b[36m(_dystack pid=207)\u001b[0m Using Feature Generators to preprocess the data ...\n\u001b[36m(_dystack pid=207)\u001b[0m Fitting AutoMLPipelineFeatureGenerator...\n\u001b[36m(_dystack pid=207)\u001b[0m \tAvailable Memory:                    30183.60 MB\n\u001b[36m(_dystack pid=207)\u001b[0m \tTrain Data (Original)  Memory Usage: 0.00 MB (0.0% of available memory)\n\u001b[36m(_dystack pid=207)\u001b[0m \tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n\u001b[36m(_dystack pid=207)\u001b[0m \tStage 1 Generators:\n\u001b[36m(_dystack pid=207)\u001b[0m \tStage 2 Generators:\n\u001b[36m(_dystack pid=207)\u001b[0m \tStage 3 Generators:\n\u001b[36m(_dystack pid=207)\u001b[0m \tStage 4 Generators:\n\u001b[36m(_dystack pid=207)\u001b[0m \tStage 5 Generators:\n\u001b[36m(_dystack pid=207)\u001b[0m \tWARNING: No useful features were detected in the data! AutoGluon will train using 0 features, and will always predict the same value. Ensure that you are passing the correct data to AutoGluon!\n\u001b[36m(_dystack pid=207)\u001b[0m Fitting DummyFeatureGenerator...\n\u001b[36m(_dystack pid=207)\u001b[0m \tTypes of features in original data (raw dtype, special dtypes):\n\u001b[36m(_dystack pid=207)\u001b[0m \tTypes of features in processed data (raw dtype, special dtypes):\n\u001b[36m(_dystack pid=207)\u001b[0m \t\t('int', []) : 1 | ['__dummy__']\n\u001b[36m(_dystack pid=207)\u001b[0m \t0.0s = Fit runtime\n\u001b[36m(_dystack pid=207)\u001b[0m \t0 features in original data used to generate 1 features in processed data.\n\u001b[36m(_dystack pid=207)\u001b[0m \tTrain Data (Processed) Memory Usage: 0.95 MB (0.0% of available memory)\n\u001b[36m(_dystack pid=207)\u001b[0m Data preprocessing and feature engineering runtime = 0.04s ...\n\u001b[36m(_dystack pid=207)\u001b[0m AutoGluon will gauge predictive performance using evaluation metric: 'accuracy'\n\u001b[36m(_dystack pid=207)\u001b[0m \tTo change this, specify the eval_metric parameter of Predictor()\n\u001b[36m(_dystack pid=207)\u001b[0m Large model count detected (112 configs) ... Only displaying the first 3 models of each family. To see all, set `verbosity=3`.\n\u001b[36m(_dystack pid=207)\u001b[0m User-specified model hyperparameters to be fit:\n\u001b[36m(_dystack pid=207)\u001b[0m {\n\u001b[36m(_dystack pid=207)\u001b[0m \t'NN_TORCH': [{}, {'activation': 'elu', 'dropout_prob': 0.10077639529843717, 'hidden_size': 108, 'learning_rate': 0.002735937344002146, 'num_layers': 4, 'use_batchnorm': True, 'weight_decay': 1.356433327634438e-12, 'ag_args': {'name_suffix': '_r79', 'priority': -2}}, {'activation': 'elu', 'dropout_prob': 0.11897478034205347, 'hidden_size': 213, 'learning_rate': 0.0010474382260641949, 'num_layers': 4, 'use_batchnorm': False, 'weight_decay': 5.594471067786272e-10, 'ag_args': {'name_suffix': '_r22', 'priority': -7}}],\n\u001b[36m(_dystack pid=207)\u001b[0m \t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, {'learning_rate': 0.03, 'num_leaves': 128, 'feature_fraction': 0.9, 'min_data_in_leaf': 3, 'ag_args': {'name_suffix': 'Large', 'priority': 0, 'hyperparameter_tune_kwargs': None}}],\n\u001b[36m(_dystack pid=207)\u001b[0m \t'CAT': [{}, {'depth': 6, 'grow_policy': 'SymmetricTree', 'l2_leaf_reg': 2.1542798306067823, 'learning_rate': 0.06864209415792857, 'max_ctr_complexity': 4, 'one_hot_max_size': 10, 'ag_args': {'name_suffix': '_r177', 'priority': -1}}, {'depth': 8, 'grow_policy': 'Depthwise', 'l2_leaf_reg': 2.7997999596449104, 'learning_rate': 0.031375015734637225, 'max_ctr_complexity': 2, 'one_hot_max_size': 3, 'ag_args': {'name_suffix': '_r9', 'priority': -5}}],\n\u001b[36m(_dystack pid=207)\u001b[0m \t'XGB': [{}, {'colsample_bytree': 0.6917311125174739, 'enable_categorical': False, 'learning_rate': 0.018063876087523967, 'max_depth': 10, 'min_child_weight': 0.6028633586934382, 'ag_args': {'name_suffix': '_r33', 'priority': -8}}, {'colsample_bytree': 0.6628423832084077, 'enable_categorical': False, 'learning_rate': 0.08775715546881824, 'max_depth': 5, 'min_child_weight': 0.6294123374222513, 'ag_args': {'name_suffix': '_r89', 'priority': -16}}],\n\u001b[36m(_dystack pid=207)\u001b[0m \t'FASTAI': [{}, {'bs': 256, 'emb_drop': 0.5411770367537934, 'epochs': 43, 'layers': [800, 400], 'lr': 0.01519848858318159, 'ps': 0.23782946566604385, 'ag_args': {'name_suffix': '_r191', 'priority': -4}}, {'bs': 2048, 'emb_drop': 0.05070411322605811, 'epochs': 29, 'layers': [200, 100], 'lr': 0.08974235041576624, 'ps': 0.10393466140748028, 'ag_args': {'name_suffix': '_r102', 'priority': -11}}],\n\u001b[36m(_dystack pid=207)\u001b[0m \t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n\u001b[36m(_dystack pid=207)\u001b[0m \t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n\u001b[36m(_dystack pid=207)\u001b[0m \t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n\u001b[36m(_dystack pid=207)\u001b[0m }\n\u001b[36m(_dystack pid=207)\u001b[0m AutoGluon will fit 2 stack levels (L1 to L2) ...\n\u001b[36m(_dystack pid=207)\u001b[0m Fitting 110 L1 models, fit_strategy=\"sequential\" ...\n\u001b[36m(_dystack pid=207)\u001b[0m Fitting model: KNeighborsUnif_BAG_L1 ... Training model for up to 5395.98s of the 8095.98s of remaining time.\n\u001b[36m(_dystack pid=207)\u001b[0m \tNo valid features to train KNeighborsUnif_BAG_L1... Skipping this model.\n\u001b[36m(_dystack pid=207)\u001b[0m Fitting model: KNeighborsDist_BAG_L1 ... Training model for up to 5395.92s of the 8095.92s of remaining time.\n\u001b[36m(_dystack pid=207)\u001b[0m \tNo valid features to train KNeighborsDist_BAG_L1... Skipping this model.\n\u001b[36m(_dystack pid=207)\u001b[0m Fitting model: LightGBMXT_BAG_L1 ... Training model for up to 5395.87s of the 8095.87s of remaining time.\n\u001b[36m(_dystack pid=207)\u001b[0m \tNo valid features to train LightGBMXT_BAG_L1... Skipping this model.\n\u001b[36m(_dystack pid=207)\u001b[0m Fitting model: LightGBM_BAG_L1 ... Training model for up to 5395.82s of the 8095.83s of remaining time.\n\u001b[36m(_dystack pid=207)\u001b[0m \tNo valid features to train LightGBM_BAG_L1... Skipping this model.\n\u001b[36m(_dystack pid=207)\u001b[0m Fitting model: RandomForestGini_BAG_L1 ... Training model for up to 5395.78s of the 8095.78s of remaining time.\n\u001b[36m(_dystack pid=207)\u001b[0m \tNo valid features to train RandomForestGini_BAG_L1... Skipping this model.\n\u001b[36m(_dystack pid=207)\u001b[0m Fitting model: RandomForestEntr_BAG_L1 ... Training model for up to 5395.74s of the 8095.74s of remaining time.\n\u001b[36m(_dystack pid=207)\u001b[0m \tNo valid features to train RandomForestEntr_BAG_L1... Skipping this model.\n\u001b[36m(_dystack pid=207)\u001b[0m Fitting model: CatBoost_BAG_L1 ... Training model for up to 5395.69s of the 8095.70s of remaining time.\n\u001b[36m(_dystack pid=207)\u001b[0m \tNo valid features to train CatBoost_BAG_L1... Skipping this model.\n\u001b[36m(_dystack pid=207)\u001b[0m Fitting model: ExtraTreesGini_BAG_L1 ... Training model for up to 5395.65s of the 8095.65s of remaining time.\n\u001b[36m(_dystack pid=207)\u001b[0m \tNo valid features to train ExtraTreesGini_BAG_L1... Skipping this model.\n\u001b[36m(_dystack pid=207)\u001b[0m Fitting model: ExtraTreesEntr_BAG_L1 ... Training model for up to 5395.60s of the 8095.60s of remaining time.\n\u001b[36m(_dystack pid=207)\u001b[0m \tNo valid features to train ExtraTreesEntr_BAG_L1... Skipping this model.\n\u001b[36m(_dystack pid=207)\u001b[0m Fitting model: NeuralNetFastAI_BAG_L1 ... Training model for up to 5395.55s of the 8095.56s of remaining time.\n\u001b[36m(_dystack pid=207)\u001b[0m \tNo valid features to train NeuralNetFastAI_BAG_L1... Skipping this model.\n\u001b[36m(_dystack pid=207)\u001b[0m Fitting model: XGBoost_BAG_L1 ... Training model for up to 5395.51s of the 8095.51s of remaining time.\n\u001b[36m(_dystack pid=207)\u001b[0m \tNo valid features to train XGBoost_BAG_L1... Skipping this model.\n\u001b[36m(_dystack pid=207)\u001b[0m Fitting model: NeuralNetTorch_BAG_L1 ... Training model for up to 5395.47s of the 8095.47s of remaining time.\n\u001b[36m(_dystack pid=207)\u001b[0m \tNo valid features to train NeuralNetTorch_BAG_L1... Skipping this model.\n\u001b[36m(_dystack pid=207)\u001b[0m Fitting model: LightGBMLarge_BAG_L1 ... Training model for up to 5395.42s of the 8095.42s of remaining time.\n\u001b[36m(_dystack pid=207)\u001b[0m \tNo valid features to train LightGBMLarge_BAG_L1... Skipping this model.\n\u001b[36m(_dystack pid=207)\u001b[0m Fitting model: CatBoost_r177_BAG_L1 ... Training model for up to 5395.38s of the 8095.38s of remaining time.\n\u001b[36m(_dystack pid=207)\u001b[0m \tNo valid features to train CatBoost_r177_BAG_L1... Skipping this model.\n\u001b[36m(_dystack pid=207)\u001b[0m Fitting model: NeuralNetTorch_r79_BAG_L1 ... Training model for up to 5395.33s of the 8095.33s of remaining time.\n\u001b[36m(_dystack pid=207)\u001b[0m \tNo valid features to train NeuralNetTorch_r79_BAG_L1... Skipping this model.\n\u001b[36m(_dystack pid=207)\u001b[0m Fitting model: LightGBM_r131_BAG_L1 ... Training model for up to 5395.28s of the 8095.29s of remaining time.\n\u001b[36m(_dystack pid=207)\u001b[0m \tNo valid features to train LightGBM_r131_BAG_L1... Skipping this model.\n\u001b[36m(_dystack pid=207)\u001b[0m Fitting model: NeuralNetFastAI_r191_BAG_L1 ... Training model for up to 5395.24s of the 8095.24s of remaining time.\n\u001b[36m(_dystack pid=207)\u001b[0m \tNo valid features to train NeuralNetFastAI_r191_BAG_L1... Skipping this model.\n\u001b[36m(_dystack pid=207)\u001b[0m Fitting model: CatBoost_r9_BAG_L1 ... Training model for up to 5395.19s of the 8095.19s of remaining time.\n\u001b[36m(_dystack pid=207)\u001b[0m \tNo valid features to train CatBoost_r9_BAG_L1... Skipping this model.\n\u001b[36m(_dystack pid=207)\u001b[0m Fitting model: LightGBM_r96_BAG_L1 ... Training model for up to 5395.15s of the 8095.15s of remaining time.\n\u001b[36m(_dystack pid=207)\u001b[0m \tNo valid features to train LightGBM_r96_BAG_L1... Skipping this model.\n\u001b[36m(_dystack pid=207)\u001b[0m Fitting model: NeuralNetTorch_r22_BAG_L1 ... Training model for up to 5395.10s of the 8095.10s of remaining time.\n\u001b[36m(_dystack pid=207)\u001b[0m \tNo valid features to train NeuralNetTorch_r22_BAG_L1... Skipping this model.\n\u001b[36m(_dystack pid=207)\u001b[0m Fitting model: XGBoost_r33_BAG_L1 ... Training model for up to 5395.05s of the 8095.06s of remaining time.\n\u001b[36m(_dystack pid=207)\u001b[0m \tNo valid features to train XGBoost_r33_BAG_L1... Skipping this model.\n\u001b[36m(_dystack pid=207)\u001b[0m Fitting model: ExtraTrees_r42_BAG_L1 ... Training model for up to 5395.01s of the 8095.01s of remaining time.\n\u001b[36m(_dystack pid=207)\u001b[0m \tNo valid features to train ExtraTrees_r42_BAG_L1... Skipping this model.\n\u001b[36m(_dystack pid=207)\u001b[0m Fitting model: CatBoost_r137_BAG_L1 ... Training model for up to 5394.96s of the 8094.96s of remaining time.\n\u001b[36m(_dystack pid=207)\u001b[0m \tNo valid features to train CatBoost_r137_BAG_L1... Skipping this model.\n\u001b[36m(_dystack pid=207)\u001b[0m Fitting model: NeuralNetFastAI_r102_BAG_L1 ... Training model for up to 5394.92s of the 8094.92s of remaining time.\n\u001b[36m(_dystack pid=207)\u001b[0m \tNo valid features to train NeuralNetFastAI_r102_BAG_L1... Skipping this model.\n\u001b[36m(_dystack pid=207)\u001b[0m Fitting model: CatBoost_r13_BAG_L1 ... Training model for up to 5394.87s of the 8094.87s of remaining time.\n\u001b[36m(_dystack pid=207)\u001b[0m \tNo valid features to train CatBoost_r13_BAG_L1... Skipping this model.\n\u001b[36m(_dystack pid=207)\u001b[0m Fitting model: RandomForest_r195_BAG_L1 ... Training model for up to 5394.83s of the 8094.83s of remaining time.\n\u001b[36m(_dystack pid=207)\u001b[0m \tNo valid features to train RandomForest_r195_BAG_L1... Skipping this model.\n\u001b[36m(_dystack pid=207)\u001b[0m Fitting model: LightGBM_r188_BAG_L1 ... Training model for up to 5394.78s of the 8094.78s of remaining time.\n\u001b[36m(_dystack pid=207)\u001b[0m \tNo valid features to train LightGBM_r188_BAG_L1... Skipping this model.\n\u001b[36m(_dystack pid=207)\u001b[0m Fitting model: NeuralNetFastAI_r145_BAG_L1 ... Training model for up to 5394.73s of the 8094.73s of remaining time.\n\u001b[36m(_dystack pid=207)\u001b[0m \tNo valid features to train NeuralNetFastAI_r145_BAG_L1... Skipping this model.\n\u001b[36m(_dystack pid=207)\u001b[0m Fitting model: XGBoost_r89_BAG_L1 ... Training model for up to 5394.68s of the 8094.68s of remaining time.\n\u001b[36m(_dystack pid=207)\u001b[0m \tNo valid features to train XGBoost_r89_BAG_L1... Skipping this model.\n\u001b[36m(_dystack pid=207)\u001b[0m Fitting model: NeuralNetTorch_r30_BAG_L1 ... Training model for up to 5394.63s of the 8094.63s of remaining time.\n\u001b[36m(_dystack pid=207)\u001b[0m \tNo valid features to train NeuralNetTorch_r30_BAG_L1... Skipping this model.\n\u001b[36m(_dystack pid=207)\u001b[0m Fitting model: LightGBM_r130_BAG_L1 ... Training model for up to 5394.58s of the 8094.59s of remaining time.\n\u001b[36m(_dystack pid=207)\u001b[0m \tNo valid features to train LightGBM_r130_BAG_L1... Skipping this model.\n\u001b[36m(_dystack pid=207)\u001b[0m Fitting model: NeuralNetTorch_r86_BAG_L1 ... Training model for up to 5394.54s of the 8094.54s of remaining time.\n\u001b[36m(_dystack pid=207)\u001b[0m \tNo valid features to train NeuralNetTorch_r86_BAG_L1... Skipping this model.\n\u001b[36m(_dystack pid=207)\u001b[0m Fitting model: CatBoost_r50_BAG_L1 ... Training model for up to 5394.49s of the 8094.49s of remaining time.\n\u001b[36m(_dystack pid=207)\u001b[0m \tNo valid features to train CatBoost_r50_BAG_L1... Skipping this model.\n\u001b[36m(_dystack pid=207)\u001b[0m Fitting model: NeuralNetFastAI_r11_BAG_L1 ... Training model for up to 5394.44s of the 8094.44s of remaining time.\n\u001b[36m(_dystack pid=207)\u001b[0m \tNo valid features to train NeuralNetFastAI_r11_BAG_L1... Skipping this model.\n\u001b[36m(_dystack pid=207)\u001b[0m Fitting model: XGBoost_r194_BAG_L1 ... Training model for up to 5394.39s of the 8094.40s of remaining time.\n\u001b[36m(_dystack pid=207)\u001b[0m \tNo valid features to train XGBoost_r194_BAG_L1... Skipping this model.\n\u001b[36m(_dystack pid=207)\u001b[0m Fitting model: ExtraTrees_r172_BAG_L1 ... Training model for up to 5394.35s of the 8094.35s of remaining time.\n\u001b[36m(_dystack pid=207)\u001b[0m \tNo valid features to train ExtraTrees_r172_BAG_L1... Skipping this model.\n\u001b[36m(_dystack pid=207)\u001b[0m Fitting model: CatBoost_r69_BAG_L1 ... Training model for up to 5394.30s of the 8094.30s of remaining time.\n\u001b[36m(_dystack pid=207)\u001b[0m \tNo valid features to train CatBoost_r69_BAG_L1... Skipping this model.\n\u001b[36m(_dystack pid=207)\u001b[0m Fitting model: NeuralNetFastAI_r103_BAG_L1 ... Training model for up to 5394.25s of the 8094.26s of remaining time.\n\u001b[36m(_dystack pid=207)\u001b[0m \tNo valid features to train NeuralNetFastAI_r103_BAG_L1... Skipping this model.\n\u001b[36m(_dystack pid=207)\u001b[0m Fitting model: NeuralNetTorch_r14_BAG_L1 ... Training model for up to 5394.21s of the 8094.21s of remaining time.\n\u001b[36m(_dystack pid=207)\u001b[0m \tNo valid features to train NeuralNetTorch_r14_BAG_L1... Skipping this model.\n\u001b[36m(_dystack pid=207)\u001b[0m Fitting model: LightGBM_r161_BAG_L1 ... Training model for up to 5394.16s of the 8094.16s of remaining time.\n\u001b[36m(_dystack pid=207)\u001b[0m \tNo valid features to train LightGBM_r161_BAG_L1... Skipping this model.\n\u001b[36m(_dystack pid=207)\u001b[0m Fitting model: NeuralNetFastAI_r143_BAG_L1 ... Training model for up to 5394.11s of the 8094.12s of remaining time.\n\u001b[36m(_dystack pid=207)\u001b[0m \tNo valid features to train NeuralNetFastAI_r143_BAG_L1... Skipping this model.\n\u001b[36m(_dystack pid=207)\u001b[0m Fitting model: CatBoost_r70_BAG_L1 ... Training model for up to 5394.07s of the 8094.07s of remaining time.\n\u001b[36m(_dystack pid=207)\u001b[0m \tNo valid features to train CatBoost_r70_BAG_L1... Skipping this model.\n\u001b[36m(_dystack pid=207)\u001b[0m Fitting model: NeuralNetFastAI_r156_BAG_L1 ... Training model for up to 5394.02s of the 8094.02s of remaining time.\n\u001b[36m(_dystack pid=207)\u001b[0m \tNo valid features to train NeuralNetFastAI_r156_BAG_L1... Skipping this model.\n\u001b[36m(_dystack pid=207)\u001b[0m Fitting model: LightGBM_r196_BAG_L1 ... Training model for up to 5393.97s of the 8093.98s of remaining time.\n\u001b[36m(_dystack pid=207)\u001b[0m \tNo valid features to train LightGBM_r196_BAG_L1... Skipping this model.\n\u001b[36m(_dystack pid=207)\u001b[0m Fitting model: RandomForest_r39_BAG_L1 ... Training model for up to 5393.92s of the 8093.93s of remaining time.\n\u001b[36m(_dystack pid=207)\u001b[0m \tNo valid features to train RandomForest_r39_BAG_L1... Skipping this model.\n\u001b[36m(_dystack pid=207)\u001b[0m Fitting model: CatBoost_r167_BAG_L1 ... Training model for up to 5393.88s of the 8093.88s of remaining time.\n\u001b[36m(_dystack pid=207)\u001b[0m \tNo valid features to train CatBoost_r167_BAG_L1... Skipping this model.\n\u001b[36m(_dystack pid=207)\u001b[0m Fitting model: NeuralNetFastAI_r95_BAG_L1 ... Training model for up to 5393.83s of the 8093.83s of remaining time.\n\u001b[36m(_dystack pid=207)\u001b[0m \tNo valid features to train NeuralNetFastAI_r95_BAG_L1... Skipping this model.\n\u001b[36m(_dystack pid=207)\u001b[0m Fitting model: NeuralNetTorch_r41_BAG_L1 ... Training model for up to 5393.78s of the 8093.79s of remaining time.\n\u001b[36m(_dystack pid=207)\u001b[0m \tNo valid features to train NeuralNetTorch_r41_BAG_L1... Skipping this model.\n\u001b[36m(_dystack pid=207)\u001b[0m Fitting model: XGBoost_r98_BAG_L1 ... Training model for up to 5393.74s of the 8093.74s of remaining time.\n\u001b[36m(_dystack pid=207)\u001b[0m \tNo valid features to train XGBoost_r98_BAG_L1... Skipping this model.\n\u001b[36m(_dystack pid=207)\u001b[0m Fitting model: LightGBM_r15_BAG_L1 ... Training model for up to 5393.69s of the 8093.69s of remaining time.\n\u001b[36m(_dystack pid=207)\u001b[0m \tNo valid features to train LightGBM_r15_BAG_L1... Skipping this model.\n\u001b[36m(_dystack pid=207)\u001b[0m Fitting model: NeuralNetTorch_r158_BAG_L1 ... Training model for up to 5393.65s of the 8093.65s of remaining time.\n\u001b[36m(_dystack pid=207)\u001b[0m \tNo valid features to train NeuralNetTorch_r158_BAG_L1... Skipping this model.\n\u001b[36m(_dystack pid=207)\u001b[0m Fitting model: CatBoost_r86_BAG_L1 ... Training model for up to 5393.60s of the 8093.60s of remaining time.\n\u001b[36m(_dystack pid=207)\u001b[0m \tNo valid features to train CatBoost_r86_BAG_L1... Skipping this model.\n\u001b[36m(_dystack pid=207)\u001b[0m Fitting model: NeuralNetFastAI_r37_BAG_L1 ... Training model for up to 5393.55s of the 8093.55s of remaining time.\n\u001b[36m(_dystack pid=207)\u001b[0m \tNo valid features to train NeuralNetFastAI_r37_BAG_L1... Skipping this model.\n\u001b[36m(_dystack pid=207)\u001b[0m Fitting model: NeuralNetTorch_r197_BAG_L1 ... Training model for up to 5393.50s of the 8093.50s of remaining time.\n\u001b[36m(_dystack pid=207)\u001b[0m \tNo valid features to train NeuralNetTorch_r197_BAG_L1... Skipping this model.\n\u001b[36m(_dystack pid=207)\u001b[0m Fitting model: CatBoost_r49_BAG_L1 ... Training model for up to 5393.45s of the 8093.45s of remaining time.\n\u001b[36m(_dystack pid=207)\u001b[0m \tNo valid features to train CatBoost_r49_BAG_L1... Skipping this model.\n\u001b[36m(_dystack pid=207)\u001b[0m Fitting model: ExtraTrees_r49_BAG_L1 ... Training model for up to 5393.40s of the 8093.40s of remaining time.\n\u001b[36m(_dystack pid=207)\u001b[0m \tNo valid features to train ExtraTrees_r49_BAG_L1... Skipping this model.\n\u001b[36m(_dystack pid=207)\u001b[0m Fitting model: LightGBM_r143_BAG_L1 ... Training model for up to 5393.35s of the 8093.36s of remaining time.\n\u001b[36m(_dystack pid=207)\u001b[0m \tNo valid features to train LightGBM_r143_BAG_L1... Skipping this model.\n\u001b[36m(_dystack pid=207)\u001b[0m Fitting model: RandomForest_r127_BAG_L1 ... Training model for up to 5393.31s of the 8093.31s of remaining time.\n\u001b[36m(_dystack pid=207)\u001b[0m \tNo valid features to train RandomForest_r127_BAG_L1... Skipping this model.\n\u001b[36m(_dystack pid=207)\u001b[0m Fitting model: NeuralNetFastAI_r134_BAG_L1 ... Training model for up to 5393.26s of the 8093.26s of remaining time.\n\u001b[36m(_dystack pid=207)\u001b[0m \tNo valid features to train NeuralNetFastAI_r134_BAG_L1... Skipping this model.\n\u001b[36m(_dystack pid=207)\u001b[0m Fitting model: RandomForest_r34_BAG_L1 ... Training model for up to 5393.22s of the 8093.22s of remaining time.\n\u001b[36m(_dystack pid=207)\u001b[0m \tNo valid features to train RandomForest_r34_BAG_L1... Skipping this model.\n\u001b[36m(_dystack pid=207)\u001b[0m Fitting model: LightGBM_r94_BAG_L1 ... Training model for up to 5393.17s of the 8093.17s of remaining time.\n\u001b[36m(_dystack pid=207)\u001b[0m \tNo valid features to train LightGBM_r94_BAG_L1... Skipping this model.\n\u001b[36m(_dystack pid=207)\u001b[0m Fitting model: NeuralNetTorch_r143_BAG_L1 ... Training model for up to 5393.13s of the 8093.13s of remaining time.\n\u001b[36m(_dystack pid=207)\u001b[0m \tNo valid features to train NeuralNetTorch_r143_BAG_L1... Skipping this model.\n\u001b[36m(_dystack pid=207)\u001b[0m Fitting model: CatBoost_r128_BAG_L1 ... Training model for up to 5393.07s of the 8093.08s of remaining time.\n\u001b[36m(_dystack pid=207)\u001b[0m \tNo valid features to train CatBoost_r128_BAG_L1... Skipping this model.\n\u001b[36m(_dystack pid=207)\u001b[0m Fitting model: NeuralNetFastAI_r111_BAG_L1 ... Training model for up to 5393.03s of the 8093.03s of remaining time.\n\u001b[36m(_dystack pid=207)\u001b[0m \tNo valid features to train NeuralNetFastAI_r111_BAG_L1... Skipping this model.\n\u001b[36m(_dystack pid=207)\u001b[0m Fitting model: NeuralNetTorch_r31_BAG_L1 ... Training model for up to 5392.98s of the 8092.98s of remaining time.\n\u001b[36m(_dystack pid=207)\u001b[0m \tNo valid features to train NeuralNetTorch_r31_BAG_L1... Skipping this model.\n\u001b[36m(_dystack pid=207)\u001b[0m Fitting model: ExtraTrees_r4_BAG_L1 ... Training model for up to 5392.94s of the 8092.94s of remaining time.\n\u001b[36m(_dystack pid=207)\u001b[0m \tNo valid features to train ExtraTrees_r4_BAG_L1... Skipping this model.\n\u001b[36m(_dystack pid=207)\u001b[0m Fitting model: NeuralNetFastAI_r65_BAG_L1 ... Training model for up to 5392.89s of the 8092.89s of remaining time.\n\u001b[36m(_dystack pid=207)\u001b[0m \tNo valid features to train NeuralNetFastAI_r65_BAG_L1... Skipping this model.\n\u001b[36m(_dystack pid=207)\u001b[0m Fitting model: NeuralNetFastAI_r88_BAG_L1 ... Training model for up to 5392.84s of the 8092.85s of remaining time.\n\u001b[36m(_dystack pid=207)\u001b[0m \tNo valid features to train NeuralNetFastAI_r88_BAG_L1... Skipping this model.\n\u001b[36m(_dystack pid=207)\u001b[0m Fitting model: LightGBM_r30_BAG_L1 ... Training model for up to 5392.80s of the 8092.80s of remaining time.\n\u001b[36m(_dystack pid=207)\u001b[0m \tNo valid features to train LightGBM_r30_BAG_L1... Skipping this model.\n\u001b[36m(_dystack pid=207)\u001b[0m Fitting model: XGBoost_r49_BAG_L1 ... Training model for up to 5392.75s of the 8092.75s of remaining time.\n\u001b[36m(_dystack pid=207)\u001b[0m \tNo valid features to train XGBoost_r49_BAG_L1... Skipping this model.\n\u001b[36m(_dystack pid=207)\u001b[0m Fitting model: CatBoost_r5_BAG_L1 ... Training model for up to 5392.71s of the 8092.71s of remaining time.\n\u001b[36m(_dystack pid=207)\u001b[0m \tNo valid features to train CatBoost_r5_BAG_L1... Skipping this model.\n\u001b[36m(_dystack pid=207)\u001b[0m Fitting model: NeuralNetTorch_r87_BAG_L1 ... Training model for up to 5392.66s of the 8092.66s of remaining time.\n\u001b[36m(_dystack pid=207)\u001b[0m \tNo valid features to train NeuralNetTorch_r87_BAG_L1... Skipping this model.\n\u001b[36m(_dystack pid=207)\u001b[0m Fitting model: NeuralNetTorch_r71_BAG_L1 ... Training model for up to 5392.62s of the 8092.62s of remaining time.\n\u001b[36m(_dystack pid=207)\u001b[0m \tNo valid features to train NeuralNetTorch_r71_BAG_L1... Skipping this model.\n\u001b[36m(_dystack pid=207)\u001b[0m Fitting model: CatBoost_r143_BAG_L1 ... Training model for up to 5392.57s of the 8092.57s of remaining time.\n\u001b[36m(_dystack pid=207)\u001b[0m \tNo valid features to train CatBoost_r143_BAG_L1... Skipping this model.\n\u001b[36m(_dystack pid=207)\u001b[0m Fitting model: ExtraTrees_r178_BAG_L1 ... Training model for up to 5392.52s of the 8092.52s of remaining time.\n\u001b[36m(_dystack pid=207)\u001b[0m \tNo valid features to train ExtraTrees_r178_BAG_L1... Skipping this model.\n\u001b[36m(_dystack pid=207)\u001b[0m Fitting model: RandomForest_r166_BAG_L1 ... Training model for up to 5392.47s of the 8092.48s of remaining time.\n\u001b[36m(_dystack pid=207)\u001b[0m \tNo valid features to train RandomForest_r166_BAG_L1... Skipping this model.\n\u001b[36m(_dystack pid=207)\u001b[0m Fitting model: XGBoost_r31_BAG_L1 ... Training model for up to 5392.43s of the 8092.43s of remaining time.\n\u001b[36m(_dystack pid=207)\u001b[0m \tNo valid features to train XGBoost_r31_BAG_L1... Skipping this model.\n\u001b[36m(_dystack pid=207)\u001b[0m Fitting model: NeuralNetTorch_r185_BAG_L1 ... Training model for up to 5392.36s of the 8092.36s of remaining time.\n\u001b[36m(_dystack pid=207)\u001b[0m \tNo valid features to train NeuralNetTorch_r185_BAG_L1... Skipping this model.\n\u001b[36m(_dystack pid=207)\u001b[0m Fitting model: NeuralNetFastAI_r160_BAG_L1 ... Training model for up to 5392.31s of the 8092.31s of remaining time.\n\u001b[36m(_dystack pid=207)\u001b[0m \tNo valid features to train NeuralNetFastAI_r160_BAG_L1... Skipping this model.\n\u001b[36m(_dystack pid=207)\u001b[0m Fitting model: CatBoost_r60_BAG_L1 ... Training model for up to 5392.25s of the 8092.25s of remaining time.\n\u001b[36m(_dystack pid=207)\u001b[0m \tNo valid features to train CatBoost_r60_BAG_L1... Skipping this model.\n\u001b[36m(_dystack pid=207)\u001b[0m Fitting model: RandomForest_r15_BAG_L1 ... Training model for up to 5392.20s of the 8092.20s of remaining time.\n\u001b[36m(_dystack pid=207)\u001b[0m \tNo valid features to train RandomForest_r15_BAG_L1... Skipping this model.\n\u001b[36m(_dystack pid=207)\u001b[0m Fitting model: LightGBM_r135_BAG_L1 ... Training model for up to 5392.15s of the 8092.15s of remaining time.\n\u001b[36m(_dystack pid=207)\u001b[0m \tNo valid features to train LightGBM_r135_BAG_L1... Skipping this model.\n\u001b[36m(_dystack pid=207)\u001b[0m Fitting model: XGBoost_r22_BAG_L1 ... Training model for up to 5392.09s of the 8092.09s of remaining time.\n\u001b[36m(_dystack pid=207)\u001b[0m \tNo valid features to train XGBoost_r22_BAG_L1... Skipping this model.\n\u001b[36m(_dystack pid=207)\u001b[0m Fitting model: NeuralNetFastAI_r69_BAG_L1 ... Training model for up to 5392.04s of the 8092.04s of remaining time.\n\u001b[36m(_dystack pid=207)\u001b[0m \tNo valid features to train NeuralNetFastAI_r69_BAG_L1... Skipping this model.\n\u001b[36m(_dystack pid=207)\u001b[0m Fitting model: CatBoost_r6_BAG_L1 ... Training model for up to 5391.98s of the 8091.99s of remaining time.\n\u001b[36m(_dystack pid=207)\u001b[0m \tNo valid features to train CatBoost_r6_BAG_L1... Skipping this model.\n\u001b[36m(_dystack pid=207)\u001b[0m Fitting model: NeuralNetFastAI_r138_BAG_L1 ... Training model for up to 5391.93s of the 8091.93s of remaining time.\n\u001b[36m(_dystack pid=207)\u001b[0m \tNo valid features to train NeuralNetFastAI_r138_BAG_L1... Skipping this model.\n\u001b[36m(_dystack pid=207)\u001b[0m Fitting model: LightGBM_r121_BAG_L1 ... Training model for up to 5391.88s of the 8091.88s of remaining time.\n\u001b[36m(_dystack pid=207)\u001b[0m \tNo valid features to train LightGBM_r121_BAG_L1... Skipping this model.\n\u001b[36m(_dystack pid=207)\u001b[0m Fitting model: NeuralNetFastAI_r172_BAG_L1 ... Training model for up to 5391.83s of the 8091.84s of remaining time.\n\u001b[36m(_dystack pid=207)\u001b[0m \tNo valid features to train NeuralNetFastAI_r172_BAG_L1... Skipping this model.\n\u001b[36m(_dystack pid=207)\u001b[0m Fitting model: CatBoost_r180_BAG_L1 ... Training model for up to 5391.79s of the 8091.79s of remaining time.\n\u001b[36m(_dystack pid=207)\u001b[0m \tNo valid features to train CatBoost_r180_BAG_L1... Skipping this model.\n\u001b[36m(_dystack pid=207)\u001b[0m Fitting model: NeuralNetTorch_r76_BAG_L1 ... Training model for up to 5391.74s of the 8091.74s of remaining time.\n\u001b[36m(_dystack pid=207)\u001b[0m \tNo valid features to train NeuralNetTorch_r76_BAG_L1... Skipping this model.\n\u001b[36m(_dystack pid=207)\u001b[0m Fitting model: ExtraTrees_r197_BAG_L1 ... Training model for up to 5391.69s of the 8091.69s of remaining time.\n\u001b[36m(_dystack pid=207)\u001b[0m \tNo valid features to train ExtraTrees_r197_BAG_L1... Skipping this model.\n\u001b[36m(_dystack pid=207)\u001b[0m Fitting model: NeuralNetTorch_r121_BAG_L1 ... Training model for up to 5391.63s of the 8091.64s of remaining time.\n\u001b[36m(_dystack pid=207)\u001b[0m \tNo valid features to train NeuralNetTorch_r121_BAG_L1... Skipping this model.\n\u001b[36m(_dystack pid=207)\u001b[0m Fitting model: NeuralNetFastAI_r127_BAG_L1 ... Training model for up to 5391.58s of the 8091.59s of remaining time.\n\u001b[36m(_dystack pid=207)\u001b[0m \tNo valid features to train NeuralNetFastAI_r127_BAG_L1... Skipping this model.\n\u001b[36m(_dystack pid=207)\u001b[0m Fitting model: RandomForest_r16_BAG_L1 ... Training model for up to 5391.53s of the 8091.54s of remaining time.\n\u001b[36m(_dystack pid=207)\u001b[0m \tNo valid features to train RandomForest_r16_BAG_L1... Skipping this model.\n\u001b[36m(_dystack pid=207)\u001b[0m Fitting model: NeuralNetFastAI_r194_BAG_L1 ... Training model for up to 5391.49s of the 8091.49s of remaining time.\n\u001b[36m(_dystack pid=207)\u001b[0m \tNo valid features to train NeuralNetFastAI_r194_BAG_L1... Skipping this model.\n\u001b[36m(_dystack pid=207)\u001b[0m Fitting model: CatBoost_r12_BAG_L1 ... Training model for up to 5391.44s of the 8091.44s of remaining time.\n\u001b[36m(_dystack pid=207)\u001b[0m \tNo valid features to train CatBoost_r12_BAG_L1... Skipping this model.\n\u001b[36m(_dystack pid=207)\u001b[0m Fitting model: NeuralNetTorch_r135_BAG_L1 ... Training model for up to 5391.39s of the 8091.39s of remaining time.\n\u001b[36m(_dystack pid=207)\u001b[0m \tNo valid features to train NeuralNetTorch_r135_BAG_L1... Skipping this model.\n\u001b[36m(_dystack pid=207)\u001b[0m Fitting model: NeuralNetFastAI_r4_BAG_L1 ... Training model for up to 5391.34s of the 8091.34s of remaining time.\n\u001b[36m(_dystack pid=207)\u001b[0m \tNo valid features to train NeuralNetFastAI_r4_BAG_L1... Skipping this model.\n\u001b[36m(_dystack pid=207)\u001b[0m Fitting model: ExtraTrees_r126_BAG_L1 ... Training model for up to 5391.29s of the 8091.29s of remaining time.\n\u001b[36m(_dystack pid=207)\u001b[0m \tNo valid features to train ExtraTrees_r126_BAG_L1... Skipping this model.\n\u001b[36m(_dystack pid=207)\u001b[0m Fitting model: NeuralNetTorch_r36_BAG_L1 ... Training model for up to 5391.24s of the 8091.24s of remaining time.\n\u001b[36m(_dystack pid=207)\u001b[0m \tNo valid features to train NeuralNetTorch_r36_BAG_L1... Skipping this model.\n\u001b[36m(_dystack pid=207)\u001b[0m Fitting model: NeuralNetFastAI_r100_BAG_L1 ... Training model for up to 5391.18s of the 8091.18s of remaining time.\n\u001b[36m(_dystack pid=207)\u001b[0m \tNo valid features to train NeuralNetFastAI_r100_BAG_L1... Skipping this model.\n\u001b[36m(_dystack pid=207)\u001b[0m Fitting model: CatBoost_r163_BAG_L1 ... Training model for up to 5391.10s of the 8091.10s of remaining time.\n\u001b[36m(_dystack pid=207)\u001b[0m \tNo valid features to train CatBoost_r163_BAG_L1... Skipping this model.\n\u001b[36m(_dystack pid=207)\u001b[0m Fitting model: CatBoost_r198_BAG_L1 ... Training model for up to 5391.04s of the 8091.04s of remaining time.\n\u001b[36m(_dystack pid=207)\u001b[0m \tNo valid features to train CatBoost_r198_BAG_L1... Skipping this model.\n\u001b[36m(_dystack pid=207)\u001b[0m Fitting model: NeuralNetFastAI_r187_BAG_L1 ... Training model for up to 5390.97s of the 8090.97s of remaining time.\n\u001b[36m(_dystack pid=207)\u001b[0m \tNo valid features to train NeuralNetFastAI_r187_BAG_L1... Skipping this model.\n\u001b[36m(_dystack pid=207)\u001b[0m Fitting model: NeuralNetTorch_r19_BAG_L1 ... Training model for up to 5390.90s of the 8090.90s of remaining time.\n\u001b[36m(_dystack pid=207)\u001b[0m \tNo valid features to train NeuralNetTorch_r19_BAG_L1... Skipping this model.\n\u001b[36m(_dystack pid=207)\u001b[0m Fitting model: XGBoost_r95_BAG_L1 ... Training model for up to 5390.85s of the 8090.85s of remaining time.\n\u001b[36m(_dystack pid=207)\u001b[0m \tNo valid features to train XGBoost_r95_BAG_L1... Skipping this model.\n\u001b[36m(_dystack pid=207)\u001b[0m Fitting model: XGBoost_r34_BAG_L1 ... Training model for up to 5390.80s of the 8090.80s of remaining time.\n\u001b[36m(_dystack pid=207)\u001b[0m \tNo valid features to train XGBoost_r34_BAG_L1... Skipping this model.\n\u001b[36m(_dystack pid=207)\u001b[0m Fitting model: LightGBM_r42_BAG_L1 ... Training model for up to 5390.75s of the 8090.75s of remaining time.\n\u001b[36m(_dystack pid=207)\u001b[0m \tNo valid features to train LightGBM_r42_BAG_L1... Skipping this model.\n\u001b[36m(_dystack pid=207)\u001b[0m Fitting model: NeuralNetTorch_r1_BAG_L1 ... Training model for up to 5390.68s of the 8090.68s of remaining time.\n\u001b[36m(_dystack pid=207)\u001b[0m \tNo valid features to train NeuralNetTorch_r1_BAG_L1... Skipping this model.\n\u001b[36m(_dystack pid=207)\u001b[0m Fitting model: NeuralNetTorch_r89_BAG_L1 ... Training model for up to 5390.61s of the 8090.61s of remaining time.\n\u001b[36m(_dystack pid=207)\u001b[0m \tNo valid features to train NeuralNetTorch_r89_BAG_L1... Skipping this model.\n\u001b[36m(_dystack pid=207)\u001b[0m No base models to train on, skipping auxiliary stack level 2...\n\u001b[36m(_dystack pid=207)\u001b[0m No base models to train on, skipping stack level 2...\n\u001b[36m(_dystack pid=207)\u001b[0m No base models to train on, skipping auxiliary stack level 3...\n\u001b[36m(_dystack pid=207)\u001b[0m Warning: AutoGluon did not successfully train any models\n\u001b[36m(_dystack pid=207)\u001b[0m AutoGluon training complete, total runtime = 5.48s ... Best model: None\nWarning: Exception encountered during DyStack sub-fit:\n\tNo models were trained successfully during fit(). Inspect the log output or increase verbosity to determine why no models were fit. Alternatively, set `raise_on_no_models_fitted` to False during the fit call.\n\t1\t = Optimal   num_stack_levels (Stacked Overfitting Occurred: False)\n\t11s\t = DyStack   runtime |\t32389s\t = Remaining runtime\nStarting main fit with num_stack_levels=1.\n\tFor future fit calls on this dataset, you can skip DyStack to save time: `predictor.fit(..., dynamic_stacking=False, num_stack_levels=1)`\nValues in column 'fold' used as split folds instead of being automatically set. Bagged models will have 5 splits.\nBeginning AutoGluon training ... Time limit = 32389s\nAutoGluon will save models to \"/kaggle/working/AutogluonModels/ag-20241221_150227\"\nTrain Data Rows:    140700\nTrain Data Columns: 1\nLabel Column:       Depression\nProblem Type:       binary\nPreprocessing data ...\nSelected class <--> label mapping:  class 1 = 1, class 0 = 0\nUsing Feature Generators to preprocess the data ...\nFitting AutoMLPipelineFeatureGenerator...\n\tAvailable Memory:                    30285.44 MB\n\tTrain Data (Original)  Memory Usage: 0.00 MB (0.0% of available memory)\n\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n\tStage 1 Generators:\n\tStage 2 Generators:\n\tStage 3 Generators:\n\tStage 4 Generators:\n\tStage 5 Generators:\n\tWARNING: No useful features were detected in the data! AutoGluon will train using 0 features, and will always predict the same value. Ensure that you are passing the correct data to AutoGluon!\nFitting DummyFeatureGenerator...\n\tTypes of features in original data (raw dtype, special dtypes):\n\tTypes of features in processed data (raw dtype, special dtypes):\n\t\t('int', []) : 1 | ['__dummy__']\n\t0.0s = Fit runtime\n\t0 features in original data used to generate 1 features in processed data.\n\tTrain Data (Processed) Memory Usage: 1.07 MB (0.0% of available memory)\nData preprocessing and feature engineering runtime = 0.07s ...\nAutoGluon will gauge predictive performance using evaluation metric: 'accuracy'\n\tTo change this, specify the eval_metric parameter of Predictor()\nLarge model count detected (112 configs) ... Only displaying the first 3 models of each family. To see all, set `verbosity=3`.\nUser-specified model hyperparameters to be fit:\n{\n\t'NN_TORCH': [{}, {'activation': 'elu', 'dropout_prob': 0.10077639529843717, 'hidden_size': 108, 'learning_rate': 0.002735937344002146, 'num_layers': 4, 'use_batchnorm': True, 'weight_decay': 1.356433327634438e-12, 'ag_args': {'name_suffix': '_r79', 'priority': -2}}, {'activation': 'elu', 'dropout_prob': 0.11897478034205347, 'hidden_size': 213, 'learning_rate': 0.0010474382260641949, 'num_layers': 4, 'use_batchnorm': False, 'weight_decay': 5.594471067786272e-10, 'ag_args': {'name_suffix': '_r22', 'priority': -7}}],\n\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, {'learning_rate': 0.03, 'num_leaves': 128, 'feature_fraction': 0.9, 'min_data_in_leaf': 3, 'ag_args': {'name_suffix': 'Large', 'priority': 0, 'hyperparameter_tune_kwargs': None}}],\n\t'CAT': [{}, {'depth': 6, 'grow_policy': 'SymmetricTree', 'l2_leaf_reg': 2.1542798306067823, 'learning_rate': 0.06864209415792857, 'max_ctr_complexity': 4, 'one_hot_max_size': 10, 'ag_args': {'name_suffix': '_r177', 'priority': -1}}, {'depth': 8, 'grow_policy': 'Depthwise', 'l2_leaf_reg': 2.7997999596449104, 'learning_rate': 0.031375015734637225, 'max_ctr_complexity': 2, 'one_hot_max_size': 3, 'ag_args': {'name_suffix': '_r9', 'priority': -5}}],\n\t'XGB': [{}, {'colsample_bytree': 0.6917311125174739, 'enable_categorical': False, 'learning_rate': 0.018063876087523967, 'max_depth': 10, 'min_child_weight': 0.6028633586934382, 'ag_args': {'name_suffix': '_r33', 'priority': -8}}, {'colsample_bytree': 0.6628423832084077, 'enable_categorical': False, 'learning_rate': 0.08775715546881824, 'max_depth': 5, 'min_child_weight': 0.6294123374222513, 'ag_args': {'name_suffix': '_r89', 'priority': -16}}],\n\t'FASTAI': [{}, {'bs': 256, 'emb_drop': 0.5411770367537934, 'epochs': 43, 'layers': [800, 400], 'lr': 0.01519848858318159, 'ps': 0.23782946566604385, 'ag_args': {'name_suffix': '_r191', 'priority': -4}}, {'bs': 2048, 'emb_drop': 0.05070411322605811, 'epochs': 29, 'layers': [200, 100], 'lr': 0.08974235041576624, 'ps': 0.10393466140748028, 'ag_args': {'name_suffix': '_r102', 'priority': -11}}],\n\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n}\nAutoGluon will fit 2 stack levels (L1 to L2) ...\nFitting 110 L1 models, fit_strategy=\"sequential\" ...\nFitting model: KNeighborsUnif_BAG_L1 ... Training model for up to 21587.14s of the 32388.80s of remaining time.\n\tNo valid features to train KNeighborsUnif_BAG_L1... Skipping this model.\nFitting model: KNeighborsDist_BAG_L1 ... Training model for up to 21587.01s of the 32388.67s of remaining time.\n\tNo valid features to train KNeighborsDist_BAG_L1... Skipping this model.\nFitting model: LightGBMXT_BAG_L1 ... Training model for up to 21586.92s of the 32388.57s of remaining time.\n\tNo valid features to train LightGBMXT_BAG_L1... Skipping this model.\nFitting model: LightGBM_BAG_L1 ... Training model for up to 21586.82s of the 32388.47s of remaining time.\n\tNo valid features to train LightGBM_BAG_L1... Skipping this model.\nFitting model: RandomForestGini_BAG_L1 ... Training model for up to 21586.72s of the 32388.37s of remaining time.\n\tNo valid features to train RandomForestGini_BAG_L1... Skipping this model.\nFitting model: RandomForestEntr_BAG_L1 ... Training model for up to 21586.63s of the 32388.28s of remaining time.\n\tNo valid features to train RandomForestEntr_BAG_L1... Skipping this model.\nFitting model: CatBoost_BAG_L1 ... Training model for up to 21586.54s of the 32388.19s of remaining time.\n\tNo valid features to train CatBoost_BAG_L1... Skipping this model.\nFitting model: ExtraTreesGini_BAG_L1 ... Training model for up to 21586.46s of the 32388.12s of remaining time.\n\tNo valid features to train ExtraTreesGini_BAG_L1... Skipping this model.\nFitting model: ExtraTreesEntr_BAG_L1 ... Training model for up to 21586.38s of the 32388.03s of remaining time.\n\tNo valid features to train ExtraTreesEntr_BAG_L1... Skipping this model.\nFitting model: NeuralNetFastAI_BAG_L1 ... Training model for up to 21586.30s of the 32387.95s of remaining time.\n\tNo valid features to train NeuralNetFastAI_BAG_L1... Skipping this model.\nFitting model: XGBoost_BAG_L1 ... Training model for up to 21586.22s of the 32387.87s of remaining time.\n\tNo valid features to train XGBoost_BAG_L1... Skipping this model.\nFitting model: NeuralNetTorch_BAG_L1 ... Training model for up to 21586.14s of the 32387.79s of remaining time.\n\tNo valid features to train NeuralNetTorch_BAG_L1... Skipping this model.\nFitting model: LightGBMLarge_BAG_L1 ... Training model for up to 21586.05s of the 32387.71s of remaining time.\n\tNo valid features to train LightGBMLarge_BAG_L1... Skipping this model.\nFitting model: CatBoost_r177_BAG_L1 ... Training model for up to 21585.96s of the 32387.62s of remaining time.\n\tNo valid features to train CatBoost_r177_BAG_L1... Skipping this model.\nFitting model: NeuralNetTorch_r79_BAG_L1 ... Training model for up to 21585.88s of the 32387.54s of remaining time.\n\tNo valid features to train NeuralNetTorch_r79_BAG_L1... Skipping this model.\nFitting model: LightGBM_r131_BAG_L1 ... Training model for up to 21585.80s of the 32387.46s of remaining time.\n\tNo valid features to train LightGBM_r131_BAG_L1... Skipping this model.\nFitting model: NeuralNetFastAI_r191_BAG_L1 ... Training model for up to 21585.73s of the 32387.38s of remaining time.\n\tNo valid features to train NeuralNetFastAI_r191_BAG_L1... Skipping this model.\nFitting model: CatBoost_r9_BAG_L1 ... Training model for up to 21585.64s of the 32387.30s of remaining time.\n\tNo valid features to train CatBoost_r9_BAG_L1... Skipping this model.\nFitting model: LightGBM_r96_BAG_L1 ... Training model for up to 21585.57s of the 32387.22s of remaining time.\n\tNo valid features to train LightGBM_r96_BAG_L1... Skipping this model.\nFitting model: NeuralNetTorch_r22_BAG_L1 ... Training model for up to 21585.49s of the 32387.14s of remaining time.\n\tNo valid features to train NeuralNetTorch_r22_BAG_L1... Skipping this model.\nFitting model: XGBoost_r33_BAG_L1 ... Training model for up to 21585.41s of the 32387.06s of remaining time.\n\tNo valid features to train XGBoost_r33_BAG_L1... Skipping this model.\nFitting model: ExtraTrees_r42_BAG_L1 ... Training model for up to 21585.32s of the 32386.98s of remaining time.\n\tNo valid features to train ExtraTrees_r42_BAG_L1... Skipping this model.\nFitting model: CatBoost_r137_BAG_L1 ... Training model for up to 21585.24s of the 32386.90s of remaining time.\n\tNo valid features to train CatBoost_r137_BAG_L1... Skipping this model.\nFitting model: NeuralNetFastAI_r102_BAG_L1 ... Training model for up to 21585.16s of the 32386.82s of remaining time.\n\tNo valid features to train NeuralNetFastAI_r102_BAG_L1... Skipping this model.\nFitting model: CatBoost_r13_BAG_L1 ... Training model for up to 21585.08s of the 32386.74s of remaining time.\n\tNo valid features to train CatBoost_r13_BAG_L1... Skipping this model.\nFitting model: RandomForest_r195_BAG_L1 ... Training model for up to 21585.00s of the 32386.66s of remaining time.\n\tNo valid features to train RandomForest_r195_BAG_L1... Skipping this model.\nFitting model: LightGBM_r188_BAG_L1 ... Training model for up to 21584.92s of the 32386.58s of remaining time.\n\tNo valid features to train LightGBM_r188_BAG_L1... Skipping this model.\nFitting model: NeuralNetFastAI_r145_BAG_L1 ... Training model for up to 21584.84s of the 32386.50s of remaining time.\n\tNo valid features to train NeuralNetFastAI_r145_BAG_L1... Skipping this model.\nFitting model: XGBoost_r89_BAG_L1 ... Training model for up to 21584.76s of the 32386.42s of remaining time.\n\tNo valid features to train XGBoost_r89_BAG_L1... Skipping this model.\nFitting model: NeuralNetTorch_r30_BAG_L1 ... Training model for up to 21584.68s of the 32386.34s of remaining time.\n\tNo valid features to train NeuralNetTorch_r30_BAG_L1... Skipping this model.\nFitting model: LightGBM_r130_BAG_L1 ... Training model for up to 21584.60s of the 32386.26s of remaining time.\n\tNo valid features to train LightGBM_r130_BAG_L1... Skipping this model.\nFitting model: NeuralNetTorch_r86_BAG_L1 ... Training model for up to 21584.52s of the 32386.18s of remaining time.\n\tNo valid features to train NeuralNetTorch_r86_BAG_L1... Skipping this model.\nFitting model: CatBoost_r50_BAG_L1 ... Training model for up to 21584.44s of the 32386.10s of remaining time.\n\tNo valid features to train CatBoost_r50_BAG_L1... Skipping this model.\nFitting model: NeuralNetFastAI_r11_BAG_L1 ... Training model for up to 21584.36s of the 32386.02s of remaining time.\n\tNo valid features to train NeuralNetFastAI_r11_BAG_L1... Skipping this model.\nFitting model: XGBoost_r194_BAG_L1 ... Training model for up to 21584.28s of the 32385.94s of remaining time.\n\tNo valid features to train XGBoost_r194_BAG_L1... Skipping this model.\nFitting model: ExtraTrees_r172_BAG_L1 ... Training model for up to 21584.20s of the 32385.86s of remaining time.\n\tNo valid features to train ExtraTrees_r172_BAG_L1... Skipping this model.\nFitting model: CatBoost_r69_BAG_L1 ... Training model for up to 21584.12s of the 32385.78s of remaining time.\n\tNo valid features to train CatBoost_r69_BAG_L1... Skipping this model.\nFitting model: NeuralNetFastAI_r103_BAG_L1 ... Training model for up to 21584.04s of the 32385.70s of remaining time.\n\tNo valid features to train NeuralNetFastAI_r103_BAG_L1... Skipping this model.\nFitting model: NeuralNetTorch_r14_BAG_L1 ... Training model for up to 21583.96s of the 32385.62s of remaining time.\n\tNo valid features to train NeuralNetTorch_r14_BAG_L1... Skipping this model.\nFitting model: LightGBM_r161_BAG_L1 ... Training model for up to 21583.88s of the 32385.54s of remaining time.\n\tNo valid features to train LightGBM_r161_BAG_L1... Skipping this model.\nFitting model: NeuralNetFastAI_r143_BAG_L1 ... Training model for up to 21583.80s of the 32385.46s of remaining time.\n\tNo valid features to train NeuralNetFastAI_r143_BAG_L1... Skipping this model.\nFitting model: CatBoost_r70_BAG_L1 ... Training model for up to 21583.72s of the 32385.38s of remaining time.\n\tNo valid features to train CatBoost_r70_BAG_L1... Skipping this model.\nFitting model: NeuralNetFastAI_r156_BAG_L1 ... Training model for up to 21583.64s of the 32385.30s of remaining time.\n\tNo valid features to train NeuralNetFastAI_r156_BAG_L1... Skipping this model.\nFitting model: LightGBM_r196_BAG_L1 ... Training model for up to 21583.56s of the 32385.22s of remaining time.\n\tNo valid features to train LightGBM_r196_BAG_L1... Skipping this model.\nFitting model: RandomForest_r39_BAG_L1 ... Training model for up to 21583.48s of the 32385.14s of remaining time.\n\tNo valid features to train RandomForest_r39_BAG_L1... Skipping this model.\nFitting model: CatBoost_r167_BAG_L1 ... Training model for up to 21583.40s of the 32385.06s of remaining time.\n\tNo valid features to train CatBoost_r167_BAG_L1... Skipping this model.\nFitting model: NeuralNetFastAI_r95_BAG_L1 ... Training model for up to 21583.32s of the 32384.97s of remaining time.\n\tNo valid features to train NeuralNetFastAI_r95_BAG_L1... Skipping this model.\nFitting model: NeuralNetTorch_r41_BAG_L1 ... Training model for up to 21583.23s of the 32384.89s of remaining time.\n\tNo valid features to train NeuralNetTorch_r41_BAG_L1... Skipping this model.\nFitting model: XGBoost_r98_BAG_L1 ... Training model for up to 21583.14s of the 32384.80s of remaining time.\n\tNo valid features to train XGBoost_r98_BAG_L1... Skipping this model.\nFitting model: LightGBM_r15_BAG_L1 ... Training model for up to 21583.05s of the 32384.71s of remaining time.\n\tNo valid features to train LightGBM_r15_BAG_L1... Skipping this model.\nFitting model: NeuralNetTorch_r158_BAG_L1 ... Training model for up to 21582.97s of the 32384.62s of remaining time.\n\tNo valid features to train NeuralNetTorch_r158_BAG_L1... Skipping this model.\nFitting model: CatBoost_r86_BAG_L1 ... Training model for up to 21582.88s of the 32384.54s of remaining time.\n\tNo valid features to train CatBoost_r86_BAG_L1... Skipping this model.\nFitting model: NeuralNetFastAI_r37_BAG_L1 ... Training model for up to 21582.79s of the 32384.45s of remaining time.\n\tNo valid features to train NeuralNetFastAI_r37_BAG_L1... Skipping this model.\nFitting model: NeuralNetTorch_r197_BAG_L1 ... Training model for up to 21582.71s of the 32384.37s of remaining time.\n\tNo valid features to train NeuralNetTorch_r197_BAG_L1... Skipping this model.\nFitting model: CatBoost_r49_BAG_L1 ... Training model for up to 21582.60s of the 32384.26s of remaining time.\n\tNo valid features to train CatBoost_r49_BAG_L1... Skipping this model.\nFitting model: ExtraTrees_r49_BAG_L1 ... Training model for up to 21582.49s of the 32384.15s of remaining time.\n\tNo valid features to train ExtraTrees_r49_BAG_L1... Skipping this model.\nFitting model: LightGBM_r143_BAG_L1 ... Training model for up to 21582.38s of the 32384.04s of remaining time.\n\tNo valid features to train LightGBM_r143_BAG_L1... Skipping this model.\nFitting model: RandomForest_r127_BAG_L1 ... Training model for up to 21582.30s of the 32383.96s of remaining time.\n\tNo valid features to train RandomForest_r127_BAG_L1... Skipping this model.\nFitting model: NeuralNetFastAI_r134_BAG_L1 ... Training model for up to 21582.22s of the 32383.88s of remaining time.\n\tNo valid features to train NeuralNetFastAI_r134_BAG_L1... Skipping this model.\nFitting model: RandomForest_r34_BAG_L1 ... Training model for up to 21582.14s of the 32383.80s of remaining time.\n\tNo valid features to train RandomForest_r34_BAG_L1... Skipping this model.\nFitting model: LightGBM_r94_BAG_L1 ... Training model for up to 21582.06s of the 32383.71s of remaining time.\n\tNo valid features to train LightGBM_r94_BAG_L1... Skipping this model.\nFitting model: NeuralNetTorch_r143_BAG_L1 ... Training model for up to 21581.98s of the 32383.63s of remaining time.\n\tNo valid features to train NeuralNetTorch_r143_BAG_L1... Skipping this model.\nFitting model: CatBoost_r128_BAG_L1 ... Training model for up to 21581.90s of the 32383.56s of remaining time.\n\tNo valid features to train CatBoost_r128_BAG_L1... Skipping this model.\nFitting model: NeuralNetFastAI_r111_BAG_L1 ... Training model for up to 21581.82s of the 32383.48s of remaining time.\n\tNo valid features to train NeuralNetFastAI_r111_BAG_L1... Skipping this model.\nFitting model: NeuralNetTorch_r31_BAG_L1 ... Training model for up to 21581.74s of the 32383.39s of remaining time.\n\tNo valid features to train NeuralNetTorch_r31_BAG_L1... Skipping this model.\nFitting model: ExtraTrees_r4_BAG_L1 ... Training model for up to 21581.66s of the 32383.31s of remaining time.\n\tNo valid features to train ExtraTrees_r4_BAG_L1... Skipping this model.\nFitting model: NeuralNetFastAI_r65_BAG_L1 ... Training model for up to 21581.57s of the 32383.22s of remaining time.\n\tNo valid features to train NeuralNetFastAI_r65_BAG_L1... Skipping this model.\nFitting model: NeuralNetFastAI_r88_BAG_L1 ... Training model for up to 21581.48s of the 32383.13s of remaining time.\n\tNo valid features to train NeuralNetFastAI_r88_BAG_L1... Skipping this model.\nFitting model: LightGBM_r30_BAG_L1 ... Training model for up to 21581.39s of the 32383.05s of remaining time.\n\tNo valid features to train LightGBM_r30_BAG_L1... Skipping this model.\nFitting model: XGBoost_r49_BAG_L1 ... Training model for up to 21581.30s of the 32382.96s of remaining time.\n\tNo valid features to train XGBoost_r49_BAG_L1... Skipping this model.\nFitting model: CatBoost_r5_BAG_L1 ... Training model for up to 21581.21s of the 32382.87s of remaining time.\n\tNo valid features to train CatBoost_r5_BAG_L1... Skipping this model.\nFitting model: NeuralNetTorch_r87_BAG_L1 ... Training model for up to 21581.12s of the 32382.78s of remaining time.\n\tNo valid features to train NeuralNetTorch_r87_BAG_L1... Skipping this model.\nFitting model: NeuralNetTorch_r71_BAG_L1 ... Training model for up to 21581.04s of the 32382.69s of remaining time.\n\tNo valid features to train NeuralNetTorch_r71_BAG_L1... Skipping this model.\nFitting model: CatBoost_r143_BAG_L1 ... Training model for up to 21580.95s of the 32382.61s of remaining time.\n\tNo valid features to train CatBoost_r143_BAG_L1... Skipping this model.\nFitting model: ExtraTrees_r178_BAG_L1 ... Training model for up to 21580.87s of the 32382.53s of remaining time.\n\tNo valid features to train ExtraTrees_r178_BAG_L1... Skipping this model.\nFitting model: RandomForest_r166_BAG_L1 ... Training model for up to 21580.79s of the 32382.45s of remaining time.\n\tNo valid features to train RandomForest_r166_BAG_L1... Skipping this model.\nFitting model: XGBoost_r31_BAG_L1 ... Training model for up to 21580.71s of the 32382.36s of remaining time.\n\tNo valid features to train XGBoost_r31_BAG_L1... Skipping this model.\nFitting model: NeuralNetTorch_r185_BAG_L1 ... Training model for up to 21580.62s of the 32382.28s of remaining time.\n\tNo valid features to train NeuralNetTorch_r185_BAG_L1... Skipping this model.\nFitting model: NeuralNetFastAI_r160_BAG_L1 ... Training model for up to 21580.54s of the 32382.20s of remaining time.\n\tNo valid features to train NeuralNetFastAI_r160_BAG_L1... Skipping this model.\nFitting model: CatBoost_r60_BAG_L1 ... Training model for up to 21580.46s of the 32382.12s of remaining time.\n\tNo valid features to train CatBoost_r60_BAG_L1... Skipping this model.\nFitting model: RandomForest_r15_BAG_L1 ... Training model for up to 21580.38s of the 32382.03s of remaining time.\n\tNo valid features to train RandomForest_r15_BAG_L1... Skipping this model.\nFitting model: LightGBM_r135_BAG_L1 ... Training model for up to 21580.30s of the 32381.95s of remaining time.\n\tNo valid features to train LightGBM_r135_BAG_L1... Skipping this model.\nFitting model: XGBoost_r22_BAG_L1 ... Training model for up to 21580.22s of the 32381.87s of remaining time.\n\tNo valid features to train XGBoost_r22_BAG_L1... Skipping this model.\nFitting model: NeuralNetFastAI_r69_BAG_L1 ... Training model for up to 21580.13s of the 32381.79s of remaining time.\n\tNo valid features to train NeuralNetFastAI_r69_BAG_L1... Skipping this model.\nFitting model: CatBoost_r6_BAG_L1 ... Training model for up to 21580.04s of the 32381.70s of remaining time.\n\tNo valid features to train CatBoost_r6_BAG_L1... Skipping this model.\nFitting model: NeuralNetFastAI_r138_BAG_L1 ... Training model for up to 21579.96s of the 32381.62s of remaining time.\n\tNo valid features to train NeuralNetFastAI_r138_BAG_L1... Skipping this model.\nFitting model: LightGBM_r121_BAG_L1 ... Training model for up to 21579.88s of the 32381.54s of remaining time.\n\tNo valid features to train LightGBM_r121_BAG_L1... Skipping this model.\nFitting model: NeuralNetFastAI_r172_BAG_L1 ... Training model for up to 21579.80s of the 32381.46s of remaining time.\n\tNo valid features to train NeuralNetFastAI_r172_BAG_L1... Skipping this model.\nFitting model: CatBoost_r180_BAG_L1 ... Training model for up to 21579.72s of the 32381.38s of remaining time.\n\tNo valid features to train CatBoost_r180_BAG_L1... Skipping this model.\nFitting model: NeuralNetTorch_r76_BAG_L1 ... Training model for up to 21579.64s of the 32381.30s of remaining time.\n\tNo valid features to train NeuralNetTorch_r76_BAG_L1... Skipping this model.\nFitting model: ExtraTrees_r197_BAG_L1 ... Training model for up to 21579.56s of the 32381.22s of remaining time.\n\tNo valid features to train ExtraTrees_r197_BAG_L1... Skipping this model.\nFitting model: NeuralNetTorch_r121_BAG_L1 ... Training model for up to 21579.48s of the 32381.14s of remaining time.\n\tNo valid features to train NeuralNetTorch_r121_BAG_L1... Skipping this model.\nFitting model: NeuralNetFastAI_r127_BAG_L1 ... Training model for up to 21579.40s of the 32381.06s of remaining time.\n\tNo valid features to train NeuralNetFastAI_r127_BAG_L1... Skipping this model.\nFitting model: RandomForest_r16_BAG_L1 ... Training model for up to 21579.32s of the 32380.98s of remaining time.\n\tNo valid features to train RandomForest_r16_BAG_L1... Skipping this model.\nFitting model: NeuralNetFastAI_r194_BAG_L1 ... Training model for up to 21579.24s of the 32380.90s of remaining time.\n\tNo valid features to train NeuralNetFastAI_r194_BAG_L1... Skipping this model.\nFitting model: CatBoost_r12_BAG_L1 ... Training model for up to 21579.16s of the 32380.81s of remaining time.\n\tNo valid features to train CatBoost_r12_BAG_L1... Skipping this model.\nFitting model: NeuralNetTorch_r135_BAG_L1 ... Training model for up to 21579.07s of the 32380.73s of remaining time.\n\tNo valid features to train NeuralNetTorch_r135_BAG_L1... Skipping this model.\nFitting model: NeuralNetFastAI_r4_BAG_L1 ... Training model for up to 21578.99s of the 32380.65s of remaining time.\n\tNo valid features to train NeuralNetFastAI_r4_BAG_L1... Skipping this model.\nFitting model: ExtraTrees_r126_BAG_L1 ... Training model for up to 21578.90s of the 32380.56s of remaining time.\n\tNo valid features to train ExtraTrees_r126_BAG_L1... Skipping this model.\nFitting model: NeuralNetTorch_r36_BAG_L1 ... Training model for up to 21578.82s of the 32380.48s of remaining time.\n\tNo valid features to train NeuralNetTorch_r36_BAG_L1... Skipping this model.\nFitting model: NeuralNetFastAI_r100_BAG_L1 ... Training model for up to 21578.73s of the 32380.39s of remaining time.\n\tNo valid features to train NeuralNetFastAI_r100_BAG_L1... Skipping this model.\nFitting model: CatBoost_r163_BAG_L1 ... Training model for up to 21578.65s of the 32380.31s of remaining time.\n\tNo valid features to train CatBoost_r163_BAG_L1... Skipping this model.\nFitting model: CatBoost_r198_BAG_L1 ... Training model for up to 21578.56s of the 32380.22s of remaining time.\n\tNo valid features to train CatBoost_r198_BAG_L1... Skipping this model.\nFitting model: NeuralNetFastAI_r187_BAG_L1 ... Training model for up to 21578.47s of the 32380.13s of remaining time.\n\tNo valid features to train NeuralNetFastAI_r187_BAG_L1... Skipping this model.\nFitting model: NeuralNetTorch_r19_BAG_L1 ... Training model for up to 21578.39s of the 32380.04s of remaining time.\n\tNo valid features to train NeuralNetTorch_r19_BAG_L1... Skipping this model.\nFitting model: XGBoost_r95_BAG_L1 ... Training model for up to 21578.30s of the 32379.96s of remaining time.\n\tNo valid features to train XGBoost_r95_BAG_L1... Skipping this model.\nFitting model: XGBoost_r34_BAG_L1 ... Training model for up to 21578.22s of the 32379.87s of remaining time.\n\tNo valid features to train XGBoost_r34_BAG_L1... Skipping this model.\nFitting model: LightGBM_r42_BAG_L1 ... Training model for up to 21578.13s of the 32379.79s of remaining time.\n\tNo valid features to train LightGBM_r42_BAG_L1... Skipping this model.\nFitting model: NeuralNetTorch_r1_BAG_L1 ... Training model for up to 21578.03s of the 32379.69s of remaining time.\n\tNo valid features to train NeuralNetTorch_r1_BAG_L1... Skipping this model.\nFitting model: NeuralNetTorch_r89_BAG_L1 ... Training model for up to 21577.93s of the 32379.59s of remaining time.\n\tNo valid features to train NeuralNetTorch_r89_BAG_L1... Skipping this model.\nNo base models to train on, skipping auxiliary stack level 2...\nNo base models to train on, skipping stack level 2...\nNo base models to train on, skipping auxiliary stack level 3...\nWarning: AutoGluon did not successfully train any models\nAutoGluon training complete, total runtime = 9.4s ... Best model: None\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m<ipython-input-1-4d6ef43888ae>\u001b[0m in \u001b[0;36m<cell line: 99>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[0;31m# block 10\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m predictor.fit(\n\u001b[0m\u001b[1;32m    100\u001b[0m     \u001b[0mtrain_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m     \u001b[0mtime_limit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mCFG\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime_limit\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/autogluon/core/utils/decorators.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m             \u001b[0mgargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mother_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mgargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mgkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/autogluon/tabular/predictor/predictor.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, train_data, tuning_data, time_limit, presets, hyperparameters, feature_metadata, infer_limit, infer_limit_batch_size, fit_weighted_ensemble, fit_full_last_level_weighted_ensemble, full_weighted_ensemble_additionally, dynamic_stacking, calibrate_decision_threshold, num_cpus, num_gpus, fit_strategy, memory_limit, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m   1297\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit_strategy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfit_strategy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1298\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1299\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mag_fit_kwargs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mag_fit_kwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mag_post_fit_kwargs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mag_post_fit_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1300\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1301\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/autogluon/tabular/predictor/predictor.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, ag_fit_kwargs, ag_post_fit_kwargs)\u001b[0m\n\u001b[1;32m   1305\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_learner\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mag_fit_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1306\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_post_fit_vars\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1307\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_post_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mag_post_fit_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1308\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1309\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/autogluon/tabular/predictor/predictor.py\u001b[0m in \u001b[0;36m_post_fit\u001b[0;34m(self, keep_only_best, refit_full, set_best_to_refit_full, save_space, calibrate, calibrate_decision_threshold, infer_limit, num_cpus, num_gpus, refit_full_kwargs, fit_strategy, raise_on_no_models_fitted)\u001b[0m\n\u001b[1;32m   1628\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_names\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1629\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mraise_on_no_models_fitted\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1630\u001b[0;31m                 raise RuntimeError(\n\u001b[0m\u001b[1;32m   1631\u001b[0m                     \u001b[0;34m\"No models were trained successfully during fit().\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1632\u001b[0m                     \u001b[0;34m\" Inspect the log output or increase verbosity to determine why no models were fit.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mRuntimeError\u001b[0m: No models were trained successfully during fit(). Inspect the log output or increase verbosity to determine why no models were fit. Alternatively, set `raise_on_no_models_fitted` to False during the fit call."],"ename":"RuntimeError","evalue":"No models were trained successfully during fit(). Inspect the log output or increase verbosity to determine why no models were fit. Alternatively, set `raise_on_no_models_fitted` to False during the fit call.","output_type":"error"}],"execution_count":1},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# alternative code 03","metadata":{}},{"cell_type":"code","source":"!pip install -q autogluon.tabular ray==2.10.0","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-21T15:14:26.072480Z","iopub.execute_input":"2024-12-21T15:14:26.072826Z","iopub.status.idle":"2024-12-21T15:14:52.188785Z","shell.execute_reply.started":"2024-12-21T15:14:26.072798Z","shell.execute_reply":"2024-12-21T15:14:52.187769Z"}},"outputs":[{"name":"stdout","text":"\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.1/65.1 MB\u001b[0m \u001b[31m23.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m352.2/352.2 kB\u001b[0m \u001b[31m18.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m266.2/266.2 kB\u001b[0m \u001b[31m14.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.1/64.1 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m68.2/68.2 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.3/13.3 MB\u001b[0m \u001b[31m74.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25h","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom autogluon.tabular import TabularDataset, TabularPredictor\n\n# Load datasets\ntrain = pd.read_csv('/kaggle/input/mental-health-prediction-hackathon/train.csv')\ntest = pd.read_csv('/kaggle/input/mental-health-prediction-hackathon/test.csv')\n\n# Data preprocessing\nfor feature in train.columns:\n    if train[feature].dtype == 'object':\n        train[feature] = pd.Categorical(train[feature]).codes\ntrain.fillna(0, inplace=True)\n\nfor feature in test.columns:\n    if test[feature].dtype == 'object':\n        test[feature] = pd.Categorical(test[feature]).codes\ntest.fillna(0, inplace=True)\n\n# Specify target and features\ntarget = 'Depression'\nfeatures = [col for col in train.columns if col not in ['id', 'Name', target]]\n\n# Convert to TabularDataset\ntrain_data = TabularDataset(train)\ntest_data = TabularDataset(test)\n\n# Set up AutoGluon\npredictor = TabularPredictor(label=target, verbosity=3)\n\n# Fit the predictor\npredictor.fit(\n    train_data=train_data,\n    presets='best_quality',\n    time_limit=3600,  # 1 hour\n    hyperparameters={\n        'RF': {},  # Random Forest\n        'GBM': {},  # Gradient Boosting Machine\n        'CAT': {},  # CatBoost\n        'XGB': {},  # XGBoost\n        'NN_TORCH': {},  # Neural Network (PyTorch)\n        'FASTAI': {},  # FastAI\n    }\n)\n\n# Evaluate the predictor\nperformance = predictor.evaluate(train_data)\nprint(\"Model Performance on Training Data:\", performance)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-21T15:17:56.254958Z","iopub.execute_input":"2024-12-21T15:17:56.255443Z","iopub.status.idle":"2024-12-21T16:14:16.904923Z","shell.execute_reply.started":"2024-12-21T15:17:56.255407Z","shell.execute_reply":"2024-12-21T16:14:16.903271Z"}},"outputs":[{"name":"stderr","text":"No path specified. Models will be saved in: \"AutogluonModels/ag-20241221_151757\"\nVerbosity: 3 (Detailed Logging)\n=================== System Info ===================\nAutoGluon Version:  1.2\nPython Version:     3.10.12\nOperating System:   Linux\nPlatform Machine:   x86_64\nPlatform Version:   #1 SMP PREEMPT_DYNAMIC Sun Nov 10 10:07:59 UTC 2024\nCPU Count:          4\nGPU Count:          0\nMemory Avail:       29.38 GB / 31.35 GB (93.7%)\nDisk Space Avail:   19.49 GB / 19.52 GB (99.9%)\n===================================================\nPresets specified: ['best_quality']\n============ fit kwarg info ============\nUser Specified kwargs:\n{'auto_stack': True, 'num_bag_sets': 1}\nFull kwargs:\n{'_feature_generator_kwargs': None,\n '_save_bag_folds': None,\n 'ag_args': None,\n 'ag_args_ensemble': None,\n 'ag_args_fit': None,\n 'auto_stack': True,\n 'calibrate': 'auto',\n 'delay_bag_sets': False,\n 'ds_args': {'clean_up_fits': True,\n             'detection_time_frac': 0.25,\n             'enable_callbacks': False,\n             'enable_ray_logging': True,\n             'holdout_data': None,\n             'holdout_frac': 0.1111111111111111,\n             'memory_safe_fits': True,\n             'n_folds': 2,\n             'n_repeats': 1,\n             'validation_procedure': 'holdout'},\n 'excluded_model_types': None,\n 'feature_generator': 'auto',\n 'feature_prune_kwargs': None,\n 'holdout_frac': None,\n 'hyperparameter_tune_kwargs': None,\n 'included_model_types': None,\n 'keep_only_best': False,\n 'learning_curves': False,\n 'name_suffix': None,\n 'num_bag_folds': None,\n 'num_bag_sets': 1,\n 'num_stack_levels': None,\n 'pseudo_data': None,\n 'raise_on_no_models_fitted': True,\n 'refit_full': False,\n 'save_bag_folds': None,\n 'save_space': False,\n 'set_best_to_refit_full': False,\n 'test_data': None,\n 'unlabeled_data': None,\n 'use_bag_holdout': False,\n 'verbosity': 3}\n========================================\nSetting dynamic_stacking from 'auto' to True. Reason: Enable dynamic_stacking when use_bag_holdout is disabled. (use_bag_holdout=False)\nStack configuration (auto_stack=True): num_stack_levels=1, num_bag_folds=8, num_bag_sets=1\nDyStack is enabled (dynamic_stacking=True). AutoGluon will try to determine whether the input data is affected by stacked overfitting and enable or disable stacking as a consequence.\n\tThis is used to identify the optimal `num_stack_levels` value. Copies of AutoGluon will be fit on subsets of the data. Then holdout validation data is used to detect stacked overfitting.\n\tRunning DyStack for up to 900s of the 3600s of remaining time (25%).\n\t\tContext path: \"/kaggle/working/AutogluonModels/ag-20241221_151757/ds_sub_fit/sub_fit_ho\"\nLeaderboard on holdout data (DyStack):\n                     model  score_holdout  score_val eval_metric  pred_time_test  pred_time_val    fit_time  pred_time_test_marginal  pred_time_val_marginal  fit_time_marginal  stack_level  can_infer  fit_order\n0           XGBoost_BAG_L2       0.941794   0.939912    accuracy        7.009043      12.655174  593.182823                 0.307451                0.664999          22.028978            2       True         12\n1      WeightedEnsemble_L3       0.941794   0.939912    accuracy        7.011188      12.667948  596.590594                 0.002146                0.012774           3.407771            3       True         13\n2          LightGBM_BAG_L2       0.941666   0.939544    accuracy        6.849260      12.378726  586.490304                 0.147669                0.388551          15.336458            2       True          8\n3   NeuralNetFastAI_BAG_L2       0.941602   0.939344    accuracy        8.580350      14.652144  733.779592                 1.878758                2.661969         162.625746            2       True         11\n4      RandomForest_BAG_L2       0.941090   0.938488    accuracy        7.474175      16.160609  622.372753                 0.772584                4.170435          51.218907            2       True          9\n5           XGBoost_BAG_L1       0.940834   0.939240    accuracy        0.538092       1.200146   31.482276                 0.538092                1.200146          31.482276            1       True          5\n6          CatBoost_BAG_L1       0.940834   0.939416    accuracy        0.587719       0.090450   50.805662                 0.587719                0.090450          50.805662            1       True          3\n7      WeightedEnsemble_L2       0.940834   0.939416    accuracy        0.589905       0.105842   52.602764                 0.002186                0.015393           1.797102            2       True          7\n8          CatBoost_BAG_L2       0.940642   0.939744    accuracy        6.746607      12.059283  588.035764                 0.045015                0.069108          16.881918            2       True         10\n9   NeuralNetFastAI_BAG_L1       0.940322   0.939000    accuracy        2.338521       3.233453  367.314277                 2.338521                3.233453         367.314277            1       True          4\n10         LightGBM_BAG_L1       0.939875   0.939384    accuracy        1.736289       2.000570   26.172207                 1.736289                2.000570          26.172207            1       True          1\n11     RandomForest_BAG_L1       0.939363   0.936314    accuracy        1.003619       4.302960   24.623158                 1.003619                4.302960          24.623158            1       True          2\n12   NeuralNetTorch_BAG_L1       0.936804   0.934531    accuracy        0.497352       1.162596   70.756265                 0.497352                1.162596          70.756265            1       True          6\n\t1\t = Optimal   num_stack_levels (Stacked Overfitting Occurred: False)\n\t912s\t = DyStack   runtime |\t2688s\t = Remaining runtime\nStarting main fit with num_stack_levels=1.\n\tFor future fit calls on this dataset, you can skip DyStack to save time: `predictor.fit(..., dynamic_stacking=False, num_stack_levels=1)`\nSaving /kaggle/working/AutogluonModels/ag-20241221_151757/learner.pkl\nSaving /kaggle/working/AutogluonModels/ag-20241221_151757/predictor.pkl\nBeginning AutoGluon training ... Time limit = 2688s\nAutoGluon will save models to \"/kaggle/working/AutogluonModels/ag-20241221_151757\"\nTrain Data Rows:    140700\nTrain Data Columns: 19\nLabel Column:       Depression\nProblem Type:       binary\nPreprocessing data ...\nSelected class <--> label mapping:  class 1 = 1, class 0 = 0\nUsing Feature Generators to preprocess the data ...\nFitting AutoMLPipelineFeatureGenerator...\n\tAvailable Memory:                    29351.21 MB\n\tTrain Data (Original)  Memory Usage: 11.14 MB (0.0% of available memory)\n\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n\tStage 1 Generators:\n\t\tFitting AsTypeFeatureGenerator...\n\t\t\tNote: Converting 4 features to boolean dtype as they only contain 2 unique values.\n\t\t\tOriginal Features (exact raw dtype, raw dtype):\n\t\t\t\t('float64', 'float') : 8 | ['Age', 'Academic Pressure', 'Work Pressure', 'CGPA', 'Study Satisfaction', ...]\n\t\t\t\t('int16', 'int')     : 1 | ['Name']\n\t\t\t\t('int64', 'int')     : 1 | ['id']\n\t\t\t\t('int8', 'int')      : 9 | ['Gender', 'City', 'Working Professional or Student', 'Profession', 'Sleep Duration', ...]\n\t\t\tTypes of features in original data (raw dtype, special dtypes):\n\t\t\t\t('float', []) :  8 | ['Age', 'Academic Pressure', 'Work Pressure', 'CGPA', 'Study Satisfaction', ...]\n\t\t\t\t('int', [])   : 11 | ['id', 'Name', 'Gender', 'City', 'Working Professional or Student', ...]\n\t\t\tTypes of features in processed data (raw dtype, special dtypes):\n\t\t\t\t('float', [])     : 8 | ['Age', 'Academic Pressure', 'Work Pressure', 'CGPA', 'Study Satisfaction', ...]\n\t\t\t\t('int', [])       : 7 | ['id', 'Name', 'City', 'Profession', 'Sleep Duration', ...]\n\t\t\t\t('int', ['bool']) : 4 | ['Gender', 'Working Professional or Student', 'Have you ever had suicidal thoughts ?', 'Family History of Mental Illness']\n\t\t\t0.1s = Fit runtime\n\t\t\t19 features in original data used to generate 19 features in processed data.\n\tStage 2 Generators:\n\t\tFitting FillNaFeatureGenerator...\n\t\t\tTypes of features in original data (raw dtype, special dtypes):\n\t\t\t\t('float', [])     : 8 | ['Age', 'Academic Pressure', 'Work Pressure', 'CGPA', 'Study Satisfaction', ...]\n\t\t\t\t('int', [])       : 7 | ['id', 'Name', 'City', 'Profession', 'Sleep Duration', ...]\n\t\t\t\t('int', ['bool']) : 4 | ['Gender', 'Working Professional or Student', 'Have you ever had suicidal thoughts ?', 'Family History of Mental Illness']\n\t\t\tTypes of features in processed data (raw dtype, special dtypes):\n\t\t\t\t('float', [])     : 8 | ['Age', 'Academic Pressure', 'Work Pressure', 'CGPA', 'Study Satisfaction', ...]\n\t\t\t\t('int', [])       : 7 | ['id', 'Name', 'City', 'Profession', 'Sleep Duration', ...]\n\t\t\t\t('int', ['bool']) : 4 | ['Gender', 'Working Professional or Student', 'Have you ever had suicidal thoughts ?', 'Family History of Mental Illness']\n\t\t\t0.0s = Fit runtime\n\t\t\t19 features in original data used to generate 19 features in processed data.\n\tStage 3 Generators:\n\t\tFitting IdentityFeatureGenerator...\n\t\t\tTypes of features in original data (raw dtype, special dtypes):\n\t\t\t\t('float', [])     : 8 | ['Age', 'Academic Pressure', 'Work Pressure', 'CGPA', 'Study Satisfaction', ...]\n\t\t\t\t('int', [])       : 7 | ['id', 'Name', 'City', 'Profession', 'Sleep Duration', ...]\n\t\t\t\t('int', ['bool']) : 4 | ['Gender', 'Working Professional or Student', 'Have you ever had suicidal thoughts ?', 'Family History of Mental Illness']\n\t\t\tTypes of features in processed data (raw dtype, special dtypes):\n\t\t\t\t('float', [])     : 8 | ['Age', 'Academic Pressure', 'Work Pressure', 'CGPA', 'Study Satisfaction', ...]\n\t\t\t\t('int', [])       : 7 | ['id', 'Name', 'City', 'Profession', 'Sleep Duration', ...]\n\t\t\t\t('int', ['bool']) : 4 | ['Gender', 'Working Professional or Student', 'Have you ever had suicidal thoughts ?', 'Family History of Mental Illness']\n\t\t\t0.0s = Fit runtime\n\t\t\t19 features in original data used to generate 19 features in processed data.\n\t\tSkipping CategoryFeatureGenerator: No input feature with required dtypes.\n\t\tSkipping DatetimeFeatureGenerator: No input feature with required dtypes.\n\t\tSkipping TextSpecialFeatureGenerator: No input feature with required dtypes.\n\t\tSkipping TextNgramFeatureGenerator: No input feature with required dtypes.\n\t\tSkipping IdentityFeatureGenerator: No input feature with required dtypes.\n\t\tSkipping IsNanFeatureGenerator: No input feature with required dtypes.\n\tStage 4 Generators:\n\t\tFitting DropUniqueFeatureGenerator...\n\t\t\tTypes of features in original data (raw dtype, special dtypes):\n\t\t\t\t('float', [])     : 8 | ['Age', 'Academic Pressure', 'Work Pressure', 'CGPA', 'Study Satisfaction', ...]\n\t\t\t\t('int', [])       : 7 | ['id', 'Name', 'City', 'Profession', 'Sleep Duration', ...]\n\t\t\t\t('int', ['bool']) : 4 | ['Gender', 'Working Professional or Student', 'Have you ever had suicidal thoughts ?', 'Family History of Mental Illness']\n\t\t\tTypes of features in processed data (raw dtype, special dtypes):\n\t\t\t\t('float', [])     : 8 | ['Age', 'Academic Pressure', 'Work Pressure', 'CGPA', 'Study Satisfaction', ...]\n\t\t\t\t('int', [])       : 7 | ['id', 'Name', 'City', 'Profession', 'Sleep Duration', ...]\n\t\t\t\t('int', ['bool']) : 4 | ['Gender', 'Working Professional or Student', 'Have you ever had suicidal thoughts ?', 'Family History of Mental Illness']\n\t\t\t0.1s = Fit runtime\n\t\t\t19 features in original data used to generate 19 features in processed data.\n\tStage 5 Generators:\n\t\tFitting DropDuplicatesFeatureGenerator...\n\t\t\tTypes of features in original data (raw dtype, special dtypes):\n\t\t\t\t('float', [])     : 8 | ['Age', 'Academic Pressure', 'Work Pressure', 'CGPA', 'Study Satisfaction', ...]\n\t\t\t\t('int', [])       : 7 | ['id', 'Name', 'City', 'Profession', 'Sleep Duration', ...]\n\t\t\t\t('int', ['bool']) : 4 | ['Gender', 'Working Professional or Student', 'Have you ever had suicidal thoughts ?', 'Family History of Mental Illness']\n\t\t\tTypes of features in processed data (raw dtype, special dtypes):\n\t\t\t\t('float', [])     : 8 | ['Age', 'Academic Pressure', 'Work Pressure', 'CGPA', 'Study Satisfaction', ...]\n\t\t\t\t('int', [])       : 7 | ['id', 'Name', 'City', 'Profession', 'Sleep Duration', ...]\n\t\t\t\t('int', ['bool']) : 4 | ['Gender', 'Working Professional or Student', 'Have you ever had suicidal thoughts ?', 'Family History of Mental Illness']\n\t\t\t0.0s = Fit runtime\n\t\t\t19 features in original data used to generate 19 features in processed data.\n\tTypes of features in original data (exact raw dtype, raw dtype):\n\t\t('float64', 'float') : 8 | ['Age', 'Academic Pressure', 'Work Pressure', 'CGPA', 'Study Satisfaction', ...]\n\t\t('int16', 'int')     : 1 | ['Name']\n\t\t('int64', 'int')     : 1 | ['id']\n\t\t('int8', 'int')      : 9 | ['Gender', 'City', 'Working Professional or Student', 'Profession', 'Sleep Duration', ...]\n\tTypes of features in original data (raw dtype, special dtypes):\n\t\t('float', []) :  8 | ['Age', 'Academic Pressure', 'Work Pressure', 'CGPA', 'Study Satisfaction', ...]\n\t\t('int', [])   : 11 | ['id', 'Name', 'Gender', 'City', 'Working Professional or Student', ...]\n\tTypes of features in processed data (exact raw dtype, raw dtype):\n\t\t('float64', 'float') : 8 | ['Age', 'Academic Pressure', 'Work Pressure', 'CGPA', 'Study Satisfaction', ...]\n\t\t('int16', 'int')     : 1 | ['Name']\n\t\t('int64', 'int')     : 1 | ['id']\n\t\t('int8', 'int')      : 9 | ['Gender', 'City', 'Working Professional or Student', 'Profession', 'Sleep Duration', ...]\n\tTypes of features in processed data (raw dtype, special dtypes):\n\t\t('float', [])     : 8 | ['Age', 'Academic Pressure', 'Work Pressure', 'CGPA', 'Study Satisfaction', ...]\n\t\t('int', [])       : 7 | ['id', 'Name', 'City', 'Profession', 'Sleep Duration', ...]\n\t\t('int', ['bool']) : 4 | ['Gender', 'Working Professional or Student', 'Have you ever had suicidal thoughts ?', 'Family History of Mental Illness']\n\t0.4s = Fit runtime\n\t19 features in original data used to generate 19 features in processed data.\n\tTrain Data (Processed) Memory Usage: 11.14 MB (0.0% of available memory)\nData preprocessing and feature engineering runtime = 0.45s ...\nAutoGluon will gauge predictive performance using evaluation metric: 'accuracy'\n\tTo change this, specify the eval_metric parameter of Predictor()\nSaving /kaggle/working/AutogluonModels/ag-20241221_151757/learner.pkl\nUser-specified model hyperparameters to be fit:\n{\n\t'RF': [{}],\n\t'GBM': [{}],\n\t'CAT': [{}],\n\t'XGB': [{}],\n\t'NN_TORCH': [{}],\n\t'FASTAI': [{}],\n}\nSaving /kaggle/working/AutogluonModels/ag-20241221_151757/utils/data/X.pkl\nSaving /kaggle/working/AutogluonModels/ag-20241221_151757/utils/data/y.pkl\nAutoGluon will fit 2 stack levels (L1 to L2) ...\nModel configs that will be trained (in order):\n\tLightGBM_BAG_L1: \t{'ag_args': {'model_type': <class 'autogluon.tabular.models.lgb.lgb_model.LGBModel'>, 'priority': 90}}\n\tRandomForest_BAG_L1: \t{'ag_args': {'model_type': <class 'autogluon.tabular.models.rf.rf_model.RFModel'>, 'priority': 80}, 'ag_args_ensemble': {'use_child_oof': True}}\n\tCatBoost_BAG_L1: \t{'ag_args': {'model_type': <class 'autogluon.tabular.models.catboost.catboost_model.CatBoostModel'>, 'priority': 70}}\n\tNeuralNetFastAI_BAG_L1: \t{'ag_args': {'model_type': <class 'autogluon.tabular.models.fastainn.tabular_nn_fastai.NNFastAiTabularModel'>, 'priority': 50}}\n\tXGBoost_BAG_L1: \t{'ag_args': {'problem_types': ['binary', 'multiclass', 'regression', 'softclass'], 'model_type': <class 'autogluon.tabular.models.xgboost.xgboost_model.XGBoostModel'>, 'priority': 40}}\n\tNeuralNetTorch_BAG_L1: \t{'ag_args': {'model_type': <class 'autogluon.tabular.models.tabular_nn.torch.tabular_nn_torch.TabularNeuralNetTorchModel'>, 'priority': 25}}\nFitting 6 L1 models, fit_strategy=\"sequential\" ...\nFitting model: LightGBM_BAG_L1 ... Training model for up to 1791.03s of the 2687.21s of remaining time.\n\tFitting LightGBM_BAG_L1 with 'num_gpus': 0, 'num_cpus': 4\nSaving /kaggle/working/AutogluonModels/ag-20241221_151757/models/LightGBM_BAG_L1/utils/model_template.pkl\nLoading: /kaggle/working/AutogluonModels/ag-20241221_151757/models/LightGBM_BAG_L1/utils/model_template.pkl\n\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (4 workers, per: cpus=1, gpus=0, memory=0.26%)\nSaving /kaggle/working/AutogluonModels/ag-20241221_151757/models/LightGBM_BAG_L1/utils/oof.pkl\nSaving /kaggle/working/AutogluonModels/ag-20241221_151757/models/LightGBM_BAG_L1/model.pkl\n\t0.9397\t = Validation score   (accuracy)\n\t31.64s\t = Training   runtime\n\t2.54s\t = Validation runtime\n\t6932.1\t = Inference  throughput (rows/s | 17588 batch size)\nSaving /kaggle/working/AutogluonModels/ag-20241221_151757/models/trainer.pkl\nFitting model: RandomForest_BAG_L1 ... Training model for up to 1755.27s of the 2651.45s of remaining time.\n\tFitting RandomForest_BAG_L1 with 'num_gpus': 0, 'num_cpus': 4\nSaving /kaggle/working/AutogluonModels/ag-20241221_151757/models/RandomForest_BAG_L1/utils/model_template.pkl\nLoading: /kaggle/working/AutogluonModels/ag-20241221_151757/models/RandomForest_BAG_L1/utils/model_template.pkl\n\t28.12s\t= Estimated out-of-fold prediction time...\n\t`use_child_oof` was specified for this model. It will function similarly to a bagged model, but will only fit one child model.\nSaving /kaggle/working/AutogluonModels/ag-20241221_151757/models/RandomForest_BAG_L1/utils/oof.pkl\nSaving /kaggle/working/AutogluonModels/ag-20241221_151757/models/RandomForest_BAG_L1/model.pkl\n\t0.9372\t = Validation score   (accuracy)\n\t28.13s\t = Training   runtime\n\t5.59s\t = Validation runtime\n\t25165.3\t = Inference  throughput (rows/s | 140700 batch size)\nSaving /kaggle/working/AutogluonModels/ag-20241221_151757/models/trainer.pkl\nFitting model: CatBoost_BAG_L1 ... Training model for up to 1720.55s of the 2616.73s of remaining time.\n\tFitting CatBoost_BAG_L1 with 'num_gpus': 0, 'num_cpus': 4\nSaving /kaggle/working/AutogluonModels/ag-20241221_151757/models/CatBoost_BAG_L1/utils/model_template.pkl\nLoading: /kaggle/working/AutogluonModels/ag-20241221_151757/models/CatBoost_BAG_L1/utils/model_template.pkl\n\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (4 workers, per: cpus=1, gpus=0, memory=0.27%)\nSaving /kaggle/working/AutogluonModels/ag-20241221_151757/models/CatBoost_BAG_L1/utils/oof.pkl\nSaving /kaggle/working/AutogluonModels/ag-20241221_151757/models/CatBoost_BAG_L1/model.pkl\n\t0.9396\t = Validation score   (accuracy)\n\t87.54s\t = Training   runtime\n\t0.13s\t = Validation runtime\n\t136085.6\t = Inference  throughput (rows/s | 17588 batch size)\nSaving /kaggle/working/AutogluonModels/ag-20241221_151757/models/trainer.pkl\nFitting model: NeuralNetFastAI_BAG_L1 ... Training model for up to 1629.31s of the 2525.48s of remaining time.\n\tFitting NeuralNetFastAI_BAG_L1 with 'num_gpus': 0, 'num_cpus': 4\nSaving /kaggle/working/AutogluonModels/ag-20241221_151757/models/NeuralNetFastAI_BAG_L1/utils/model_template.pkl\nLoading: /kaggle/working/AutogluonModels/ag-20241221_151757/models/NeuralNetFastAI_BAG_L1/utils/model_template.pkl\n\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (4 workers, per: cpus=1, gpus=0, memory=0.41%)\nSaving /kaggle/working/AutogluonModels/ag-20241221_151757/models/NeuralNetFastAI_BAG_L1/utils/oof.pkl\nSaving /kaggle/working/AutogluonModels/ag-20241221_151757/models/NeuralNetFastAI_BAG_L1/model.pkl\n\t0.9394\t = Validation score   (accuracy)\n\t506.84s\t = Training   runtime\n\t3.79s\t = Validation runtime\n\t4642.0\t = Inference  throughput (rows/s | 17588 batch size)\nSaving /kaggle/working/AutogluonModels/ag-20241221_151757/models/trainer.pkl\nFitting model: XGBoost_BAG_L1 ... Training model for up to 1118.70s of the 2014.87s of remaining time.\n\tFitting XGBoost_BAG_L1 with 'num_gpus': 0, 'num_cpus': 4\nSaving /kaggle/working/AutogluonModels/ag-20241221_151757/models/XGBoost_BAG_L1/utils/model_template.pkl\nLoading: /kaggle/working/AutogluonModels/ag-20241221_151757/models/XGBoost_BAG_L1/utils/model_template.pkl\n\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (4 workers, per: cpus=1, gpus=0, memory=0.35%)\nSaving /kaggle/working/AutogluonModels/ag-20241221_151757/models/XGBoost_BAG_L1/utils/oof.pkl\nSaving /kaggle/working/AutogluonModels/ag-20241221_151757/models/XGBoost_BAG_L1/model.pkl\n\t0.9397\t = Validation score   (accuracy)\n\t53.09s\t = Training   runtime\n\t1.78s\t = Validation runtime\n\t9871.1\t = Inference  throughput (rows/s | 17588 batch size)\nSaving /kaggle/working/AutogluonModels/ag-20241221_151757/models/trainer.pkl\nFitting model: NeuralNetTorch_BAG_L1 ... Training model for up to 1061.82s of the 1958.00s of remaining time.\n\tFitting NeuralNetTorch_BAG_L1 with 'num_gpus': 0, 'num_cpus': 4\nSaving /kaggle/working/AutogluonModels/ag-20241221_151757/models/NeuralNetTorch_BAG_L1/utils/model_template.pkl\nLoading: /kaggle/working/AutogluonModels/ag-20241221_151757/models/NeuralNetTorch_BAG_L1/utils/model_template.pkl\n\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (4 workers, per: cpus=1, gpus=0, memory=0.23%)\nSaving /kaggle/working/AutogluonModels/ag-20241221_151757/models/NeuralNetTorch_BAG_L1/utils/oof.pkl\nSaving /kaggle/working/AutogluonModels/ag-20241221_151757/models/NeuralNetTorch_BAG_L1/model.pkl\n\t0.9377\t = Validation score   (accuracy)\n\t646.45s\t = Training   runtime\n\t0.94s\t = Validation runtime\n\t18613.3\t = Inference  throughput (rows/s | 17588 batch size)\nSaving /kaggle/working/AutogluonModels/ag-20241221_151757/models/trainer.pkl\nLoading: /kaggle/working/AutogluonModels/ag-20241221_151757/models/LightGBM_BAG_L1/utils/oof.pkl\nLoading: /kaggle/working/AutogluonModels/ag-20241221_151757/models/RandomForest_BAG_L1/utils/oof.pkl\nLoading: /kaggle/working/AutogluonModels/ag-20241221_151757/models/CatBoost_BAG_L1/utils/oof.pkl\nLoading: /kaggle/working/AutogluonModels/ag-20241221_151757/models/NeuralNetFastAI_BAG_L1/utils/oof.pkl\nLoading: /kaggle/working/AutogluonModels/ag-20241221_151757/models/XGBoost_BAG_L1/utils/oof.pkl\nLoading: /kaggle/working/AutogluonModels/ag-20241221_151757/models/NeuralNetTorch_BAG_L1/utils/oof.pkl\nModel configs that will be trained (in order):\n\tWeightedEnsemble_L2: \t{'ag_args': {'valid_base': False, 'name_bag_suffix': '', 'model_type': <class 'autogluon.core.models.greedy_ensemble.greedy_weighted_ensemble_model.GreedyWeightedEnsembleModel'>, 'priority': 0}, 'ag_args_ensemble': {'save_bag_folds': True}}\nFitting model: WeightedEnsemble_L2 ... Training model for up to 360.00s of the 1308.04s of remaining time.\n\tFitting WeightedEnsemble_L2 with 'num_gpus': 0, 'num_cpus': 4\nSaving /kaggle/working/AutogluonModels/ag-20241221_151757/models/WeightedEnsemble_L2/utils/model_template.pkl\nLoading: /kaggle/working/AutogluonModels/ag-20241221_151757/models/WeightedEnsemble_L2/utils/model_template.pkl\nEnsemble size: 1\nEnsemble weights: \n[1. 0. 0. 0. 0. 0.]\n\t0.21s\t= Estimated out-of-fold prediction time...\nSaving /kaggle/working/AutogluonModels/ag-20241221_151757/models/WeightedEnsemble_L2/utils/oof.pkl\nSaving /kaggle/working/AutogluonModels/ag-20241221_151757/models/WeightedEnsemble_L2/model.pkl\n\tEnsemble Weights: {'LightGBM_BAG_L1': 1.0}\n\t0.9397\t = Validation score   (accuracy)\n\t2.12s\t = Training   runtime\n\t0.01s\t = Validation runtime\n\t6927.4\t = Inference  throughput (rows/s | 17588 batch size)\nSaving /kaggle/working/AutogluonModels/ag-20241221_151757/models/trainer.pkl\nModel configs that will be trained (in order):\n\tLightGBM_BAG_L2: \t{'ag_args': {'model_type': <class 'autogluon.tabular.models.lgb.lgb_model.LGBModel'>, 'priority': 90}}\n\tRandomForest_BAG_L2: \t{'ag_args': {'model_type': <class 'autogluon.tabular.models.rf.rf_model.RFModel'>, 'priority': 80}, 'ag_args_ensemble': {'use_child_oof': True}}\n\tCatBoost_BAG_L2: \t{'ag_args': {'model_type': <class 'autogluon.tabular.models.catboost.catboost_model.CatBoostModel'>, 'priority': 70}}\n\tNeuralNetFastAI_BAG_L2: \t{'ag_args': {'model_type': <class 'autogluon.tabular.models.fastainn.tabular_nn_fastai.NNFastAiTabularModel'>, 'priority': 50}}\n\tXGBoost_BAG_L2: \t{'ag_args': {'problem_types': ['binary', 'multiclass', 'regression', 'softclass'], 'model_type': <class 'autogluon.tabular.models.xgboost.xgboost_model.XGBoostModel'>, 'priority': 40}}\n\tNeuralNetTorch_BAG_L2: \t{'ag_args': {'model_type': <class 'autogluon.tabular.models.tabular_nn.torch.tabular_nn_torch.TabularNeuralNetTorchModel'>, 'priority': 25}}\nFitting 6 L2 models, fit_strategy=\"sequential\" ...\nLoading: /kaggle/working/AutogluonModels/ag-20241221_151757/models/LightGBM_BAG_L1/utils/oof.pkl\nLoading: /kaggle/working/AutogluonModels/ag-20241221_151757/models/RandomForest_BAG_L1/utils/oof.pkl\nLoading: /kaggle/working/AutogluonModels/ag-20241221_151757/models/CatBoost_BAG_L1/utils/oof.pkl\nLoading: /kaggle/working/AutogluonModels/ag-20241221_151757/models/NeuralNetFastAI_BAG_L1/utils/oof.pkl\nLoading: /kaggle/working/AutogluonModels/ag-20241221_151757/models/XGBoost_BAG_L1/utils/oof.pkl\nLoading: /kaggle/working/AutogluonModels/ag-20241221_151757/models/NeuralNetTorch_BAG_L1/utils/oof.pkl\nFitting model: LightGBM_BAG_L2 ... Training model for up to 1305.88s of the 1305.85s of remaining time.\n\tFitting LightGBM_BAG_L2 with 'num_gpus': 0, 'num_cpus': 4\nSaving /kaggle/working/AutogluonModels/ag-20241221_151757/models/LightGBM_BAG_L2/utils/model_template.pkl\nLoading: /kaggle/working/AutogluonModels/ag-20241221_151757/models/LightGBM_BAG_L2/utils/model_template.pkl\n\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (4 workers, per: cpus=1, gpus=0, memory=0.33%)\nSaving /kaggle/working/AutogluonModels/ag-20241221_151757/models/LightGBM_BAG_L2/utils/oof.pkl\nSaving /kaggle/working/AutogluonModels/ag-20241221_151757/models/LightGBM_BAG_L2/model.pkl\n\t0.9402\t = Validation score   (accuracy)\n\t19.01s\t = Training   runtime\n\t0.52s\t = Validation runtime\n\t1691.6\t = Inference  throughput (rows/s | 17588 batch size)\nSaving /kaggle/working/AutogluonModels/ag-20241221_151757/models/trainer.pkl\nFitting model: RandomForest_BAG_L2 ... Training model for up to 1283.32s of the 1283.29s of remaining time.\n\tFitting RandomForest_BAG_L2 with 'num_gpus': 0, 'num_cpus': 4\nSaving /kaggle/working/AutogluonModels/ag-20241221_151757/models/RandomForest_BAG_L2/utils/model_template.pkl\nLoading: /kaggle/working/AutogluonModels/ag-20241221_151757/models/RandomForest_BAG_L2/utils/model_template.pkl\n\t28.1s\t= Estimated out-of-fold prediction time...\n\t`use_child_oof` was specified for this model. It will function similarly to a bagged model, but will only fit one child model.\nSaving /kaggle/working/AutogluonModels/ag-20241221_151757/models/RandomForest_BAG_L2/utils/oof.pkl\nSaving /kaggle/working/AutogluonModels/ag-20241221_151757/models/RandomForest_BAG_L2/model.pkl\n\t0.9388\t = Validation score   (accuracy)\n\t58.73s\t = Training   runtime\n\t5.51s\t = Validation runtime\n\t1664.0\t = Inference  throughput (rows/s | 17588 batch size)\nSaving /kaggle/working/AutogluonModels/ag-20241221_151757/models/trainer.pkl\nFitting model: CatBoost_BAG_L2 ... Training model for up to 1218.34s of the 1218.31s of remaining time.\n\tFitting CatBoost_BAG_L2 with 'num_gpus': 0, 'num_cpus': 4\nSaving /kaggle/working/AutogluonModels/ag-20241221_151757/models/CatBoost_BAG_L2/utils/model_template.pkl\nLoading: /kaggle/working/AutogluonModels/ag-20241221_151757/models/CatBoost_BAG_L2/utils/model_template.pkl\n\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (4 workers, per: cpus=1, gpus=0, memory=0.35%)\nSaving /kaggle/working/AutogluonModels/ag-20241221_151757/models/CatBoost_BAG_L2/utils/oof.pkl\nSaving /kaggle/working/AutogluonModels/ag-20241221_151757/models/CatBoost_BAG_L2/model.pkl\n\t0.94\t = Validation score   (accuracy)\n\t16.96s\t = Training   runtime\n\t0.1s\t = Validation runtime\n\t1761.8\t = Inference  throughput (rows/s | 17588 batch size)\nSaving /kaggle/working/AutogluonModels/ag-20241221_151757/models/trainer.pkl\nFitting model: NeuralNetFastAI_BAG_L2 ... Training model for up to 1198.08s of the 1198.05s of remaining time.\n\tFitting NeuralNetFastAI_BAG_L2 with 'num_gpus': 0, 'num_cpus': 4\nSaving /kaggle/working/AutogluonModels/ag-20241221_151757/models/NeuralNetFastAI_BAG_L2/utils/model_template.pkl\nLoading: /kaggle/working/AutogluonModels/ag-20241221_151757/models/NeuralNetFastAI_BAG_L2/utils/model_template.pkl\n\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (4 workers, per: cpus=1, gpus=0, memory=0.54%)\nSaving /kaggle/working/AutogluonModels/ag-20241221_151757/models/NeuralNetFastAI_BAG_L2/utils/oof.pkl\nSaving /kaggle/working/AutogluonModels/ag-20241221_151757/models/NeuralNetFastAI_BAG_L2/model.pkl\n\t0.9401\t = Validation score   (accuracy)\n\t486.97s\t = Training   runtime\n\t2.9s\t = Validation runtime\n\t1375.6\t = Inference  throughput (rows/s | 17588 batch size)\nSaving /kaggle/working/AutogluonModels/ag-20241221_151757/models/trainer.pkl\nFitting model: XGBoost_BAG_L2 ... Training model for up to 707.16s of the 707.13s of remaining time.\n\tFitting XGBoost_BAG_L2 with 'num_gpus': 0, 'num_cpus': 4\nSaving /kaggle/working/AutogluonModels/ag-20241221_151757/models/XGBoost_BAG_L2/utils/model_template.pkl\nLoading: /kaggle/working/AutogluonModels/ag-20241221_151757/models/XGBoost_BAG_L2/utils/model_template.pkl\n\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (4 workers, per: cpus=1, gpus=0, memory=0.45%)\nSaving /kaggle/working/AutogluonModels/ag-20241221_151757/models/XGBoost_BAG_L2/utils/oof.pkl\nSaving /kaggle/working/AutogluonModels/ag-20241221_151757/models/XGBoost_BAG_L2/model.pkl\n\t0.9402\t = Validation score   (accuracy)\n\t24.32s\t = Training   runtime\n\t0.58s\t = Validation runtime\n\t1680.9\t = Inference  throughput (rows/s | 17588 batch size)\nSaving /kaggle/working/AutogluonModels/ag-20241221_151757/models/trainer.pkl\nFitting model: NeuralNetTorch_BAG_L2 ... Training model for up to 679.25s of the 679.22s of remaining time.\n\tFitting NeuralNetTorch_BAG_L2 with 'num_gpus': 0, 'num_cpus': 4\nSaving /kaggle/working/AutogluonModels/ag-20241221_151757/models/NeuralNetTorch_BAG_L2/utils/model_template.pkl\nLoading: /kaggle/working/AutogluonModels/ag-20241221_151757/models/NeuralNetTorch_BAG_L2/utils/model_template.pkl\n\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (4 workers, per: cpus=1, gpus=0, memory=0.29%)\nSaving /kaggle/working/AutogluonModels/ag-20241221_151757/models/NeuralNetTorch_BAG_L2/utils/oof.pkl\nSaving /kaggle/working/AutogluonModels/ag-20241221_151757/models/NeuralNetTorch_BAG_L2/model.pkl\n\t0.9396\t = Validation score   (accuracy)\n\t408.96s\t = Training   runtime\n\t1.26s\t = Validation runtime\n\t1578.4\t = Inference  throughput (rows/s | 17588 batch size)\nSaving /kaggle/working/AutogluonModels/ag-20241221_151757/models/trainer.pkl\nLoading: /kaggle/working/AutogluonModels/ag-20241221_151757/models/LightGBM_BAG_L1/utils/oof.pkl\nLoading: /kaggle/working/AutogluonModels/ag-20241221_151757/models/RandomForest_BAG_L1/utils/oof.pkl\nLoading: /kaggle/working/AutogluonModels/ag-20241221_151757/models/CatBoost_BAG_L1/utils/oof.pkl\nLoading: /kaggle/working/AutogluonModels/ag-20241221_151757/models/NeuralNetFastAI_BAG_L1/utils/oof.pkl\nLoading: /kaggle/working/AutogluonModels/ag-20241221_151757/models/XGBoost_BAG_L1/utils/oof.pkl\nLoading: /kaggle/working/AutogluonModels/ag-20241221_151757/models/NeuralNetTorch_BAG_L1/utils/oof.pkl\nLoading: /kaggle/working/AutogluonModels/ag-20241221_151757/models/LightGBM_BAG_L2/utils/oof.pkl\nLoading: /kaggle/working/AutogluonModels/ag-20241221_151757/models/RandomForest_BAG_L2/utils/oof.pkl\nLoading: /kaggle/working/AutogluonModels/ag-20241221_151757/models/CatBoost_BAG_L2/utils/oof.pkl\nLoading: /kaggle/working/AutogluonModels/ag-20241221_151757/models/NeuralNetFastAI_BAG_L2/utils/oof.pkl\nLoading: /kaggle/working/AutogluonModels/ag-20241221_151757/models/XGBoost_BAG_L2/utils/oof.pkl\nLoading: /kaggle/working/AutogluonModels/ag-20241221_151757/models/NeuralNetTorch_BAG_L2/utils/oof.pkl\nModel configs that will be trained (in order):\n\tWeightedEnsemble_L3: \t{'ag_args': {'valid_base': False, 'name_bag_suffix': '', 'model_type': <class 'autogluon.core.models.greedy_ensemble.greedy_weighted_ensemble_model.GreedyWeightedEnsembleModel'>, 'priority': 0}, 'ag_args_ensemble': {'save_bag_folds': True}}\nFitting model: WeightedEnsemble_L3 ... Training model for up to 360.00s of the 266.66s of remaining time.\n\tFitting WeightedEnsemble_L3 with 'num_gpus': 0, 'num_cpus': 4\nSaving /kaggle/working/AutogluonModels/ag-20241221_151757/models/WeightedEnsemble_L3/utils/model_template.pkl\nLoading: /kaggle/working/AutogluonModels/ag-20241221_151757/models/WeightedEnsemble_L3/utils/model_template.pkl\nEnsemble size: 1\nEnsemble weights: \n[0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n\t0.23s\t= Estimated out-of-fold prediction time...\nSaving /kaggle/working/AutogluonModels/ag-20241221_151757/models/WeightedEnsemble_L3/utils/oof.pkl\nSaving /kaggle/working/AutogluonModels/ag-20241221_151757/models/WeightedEnsemble_L3/model.pkl\n\tEnsemble Weights: {'LightGBM_BAG_L2': 1.0}\n\t0.9402\t = Validation score   (accuracy)\n\t3.88s\t = Training   runtime\n\t0.01s\t = Validation runtime\n\t1691.3\t = Inference  throughput (rows/s | 17588 batch size)\nSaving /kaggle/working/AutogluonModels/ag-20241221_151757/models/trainer.pkl\nSaving /kaggle/working/AutogluonModels/ag-20241221_151757/models/trainer.pkl\nSaving /kaggle/working/AutogluonModels/ag-20241221_151757/models/trainer.pkl\nAutoGluon training complete, total runtime = 2424.96s ... Best model: WeightedEnsemble_L3 | Estimated inference throughput: 1691.3 rows/s (17588 batch size)\nLoading: /kaggle/working/AutogluonModels/ag-20241221_151757/models/trainer.pkl\nEnabling decision threshold calibration (calibrate_decision_threshold='auto', metric is valid, problem_type is 'binary')\nLoading: /kaggle/working/AutogluonModels/ag-20241221_151757/utils/data/y.pkl\nLoading: /kaggle/working/AutogluonModels/ag-20241221_151757/models/WeightedEnsemble_L3/utils/oof.pkl\nCalibrating decision threshold to optimize metric accuracy | Checking 51 thresholds...\n\tthreshold: 0.500\t| val: 0.9402\t| NEW BEST\n\tthreshold: 0.480\t| val: 0.9397\n\tthreshold: 0.520\t| val: 0.9393\n\tthreshold: 0.460\t| val: 0.9398\n\tthreshold: 0.540\t| val: 0.9386\n\tthreshold: 0.440\t| val: 0.9396\n\tthreshold: 0.560\t| val: 0.9381\n\tthreshold: 0.420\t| val: 0.9393\n\tthreshold: 0.580\t| val: 0.9372\n\tthreshold: 0.400\t| val: 0.9392\n\tthreshold: 0.600\t| val: 0.9360\n\tthreshold: 0.380\t| val: 0.9385\n\tthreshold: 0.620\t| val: 0.9349\n\tthreshold: 0.360\t| val: 0.9377\n\tthreshold: 0.640\t| val: 0.9330\n\tthreshold: 0.340\t| val: 0.9367\n\tthreshold: 0.660\t| val: 0.9315\n\tthreshold: 0.320\t| val: 0.9353\n\tthreshold: 0.680\t| val: 0.9297\n\tthreshold: 0.300\t| val: 0.9336\n\tthreshold: 0.700\t| val: 0.9274\n\tthreshold: 0.280\t| val: 0.9319\n\tthreshold: 0.720\t| val: 0.9246\n\tthreshold: 0.260\t| val: 0.9295\n\tthreshold: 0.740\t| val: 0.9215\n\tthreshold: 0.240\t| val: 0.9270\n\tthreshold: 0.760\t| val: 0.9171\n\tthreshold: 0.220\t| val: 0.9248\n\tthreshold: 0.780\t| val: 0.9118\n\tthreshold: 0.200\t| val: 0.9221\n\tthreshold: 0.800\t| val: 0.9070\n\tthreshold: 0.180\t| val: 0.9185\n\tthreshold: 0.820\t| val: 0.8959\n\tthreshold: 0.160\t| val: 0.9139\n\tthreshold: 0.840\t| val: 0.8914\n\tthreshold: 0.140\t| val: 0.9067\n\tthreshold: 0.860\t| val: 0.8862\n\tthreshold: 0.120\t| val: 0.8963\n\tthreshold: 0.880\t| val: 0.8729\n\tthreshold: 0.100\t| val: 0.8864\n\tthreshold: 0.900\t| val: 0.8637\n\tthreshold: 0.080\t| val: 0.8718\n\tthreshold: 0.920\t| val: 0.8576\n\tthreshold: 0.060\t| val: 0.8446\n\tthreshold: 0.940\t| val: 0.8453\n\tthreshold: 0.040\t| val: 0.7345\n\tthreshold: 0.960\t| val: 0.8344\n\tthreshold: 0.020\t| val: 0.5394\n\tthreshold: 0.980\t| val: 0.8218\n\tthreshold: 0.000\t| val: 0.1817\n\tthreshold: 1.000\t| val: 0.8183\nCalibrating decision threshold via fine-grained search | Checking 38 thresholds...\n\tthreshold: 0.501\t| val: 0.9401\n\tthreshold: 0.502\t| val: 0.9400\n\tthreshold: 0.503\t| val: 0.9399\n\tthreshold: 0.504\t| val: 0.9399\n\tthreshold: 0.505\t| val: 0.9399\n\tthreshold: 0.506\t| val: 0.9398\n\tthreshold: 0.507\t| val: 0.9397\n\tthreshold: 0.508\t| val: 0.9397\n\tthreshold: 0.509\t| val: 0.9396\n\tthreshold: 0.510\t| val: 0.9396\n\tthreshold: 0.511\t| val: 0.9396\n\tthreshold: 0.512\t| val: 0.9395\n\tthreshold: 0.513\t| val: 0.9394\n\tthreshold: 0.514\t| val: 0.9394\n\tthreshold: 0.515\t| val: 0.9393\n\tthreshold: 0.516\t| val: 0.9393\n\tthreshold: 0.517\t| val: 0.9392\n\tthreshold: 0.518\t| val: 0.9392\n\tthreshold: 0.519\t| val: 0.9393\n\tthreshold: 0.499\t| val: 0.9402\n\tthreshold: 0.498\t| val: 0.9401\n\tthreshold: 0.497\t| val: 0.9401\n\tthreshold: 0.496\t| val: 0.9400\n\tthreshold: 0.495\t| val: 0.9400\n\tthreshold: 0.494\t| val: 0.9400\n\tthreshold: 0.493\t| val: 0.9400\n\tthreshold: 0.492\t| val: 0.9400\n\tthreshold: 0.491\t| val: 0.9399\n\tthreshold: 0.490\t| val: 0.9399\n\tthreshold: 0.489\t| val: 0.9399\n\tthreshold: 0.488\t| val: 0.9399\n\tthreshold: 0.487\t| val: 0.9399\n\tthreshold: 0.486\t| val: 0.9398\n\tthreshold: 0.485\t| val: 0.9398\n\tthreshold: 0.484\t| val: 0.9398\n\tthreshold: 0.483\t| val: 0.9398\n\tthreshold: 0.482\t| val: 0.9398\n\tthreshold: 0.481\t| val: 0.9397\n\tBase Threshold: 0.500\t| val: 0.9402\n\tBest Threshold: 0.500\t| val: 0.9402\nSaving /kaggle/working/AutogluonModels/ag-20241221_151757/models/trainer.pkl\nSaving /kaggle/working/AutogluonModels/ag-20241221_151757/learner.pkl\nSaving /kaggle/working/AutogluonModels/ag-20241221_151757/predictor.pkl\nSaving /kaggle/working/AutogluonModels/ag-20241221_151757/version.txt with contents \"1.2\"\nSaving /kaggle/working/AutogluonModels/ag-20241221_151757/metadata.json\nTabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"/kaggle/working/AutogluonModels/ag-20241221_151757\")\nLoading: /kaggle/working/AutogluonModels/ag-20241221_151757/models/CatBoost_BAG_L1/model.pkl\nLoading: /kaggle/working/AutogluonModels/ag-20241221_151757/models/LightGBM_BAG_L1/model.pkl\nLoading: /kaggle/working/AutogluonModels/ag-20241221_151757/models/NeuralNetFastAI_BAG_L1/model.pkl\nLoading: /kaggle/working/AutogluonModels/ag-20241221_151757/models/NeuralNetTorch_BAG_L1/model.pkl\nLoading: /kaggle/working/AutogluonModels/ag-20241221_151757/models/RandomForest_BAG_L1/model.pkl\nLoading: /kaggle/working/AutogluonModels/ag-20241221_151757/models/XGBoost_BAG_L1/model.pkl\nLoading: /kaggle/working/AutogluonModels/ag-20241221_151757/models/LightGBM_BAG_L2/model.pkl\nLoading: /kaggle/working/AutogluonModels/ag-20241221_151757/models/WeightedEnsemble_L3/model.pkl\n","output_type":"stream"},{"name":"stdout","text":"Model Performance on Training Data: {'accuracy': 0.9422885572139303, 'balanced_accuracy': 0.8925628513868771, 'mcc': 0.802260583516681, 'roc_auc': 0.9805106050268774, 'f1': 0.8368363943254431, 'precision': 0.8604901028968139, 'recall': 0.8144483122775453}\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)","\u001b[0;32m<ipython-input-4-48e2ecdae229>\u001b[0m in \u001b[0;36m<cell line: 51>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;31m# Prepare test data and make predictions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m \u001b[0mtest_data_nolabel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Drop the target column for test data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpredictor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_data_nolabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36mdrop\u001b[0;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[1;32m   5342\u001b[0m                 \u001b[0mweight\u001b[0m  \u001b[0;36m1.0\u001b[0m     \u001b[0;36m0.8\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5343\u001b[0m         \"\"\"\n\u001b[0;32m-> 5344\u001b[0;31m         return super().drop(\n\u001b[0m\u001b[1;32m   5345\u001b[0m             \u001b[0mlabels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5346\u001b[0m             \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36mdrop\u001b[0;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[1;32m   4709\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;32min\u001b[0m \u001b[0maxes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4710\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4711\u001b[0;31m                 \u001b[0mobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_drop_axis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlevel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4712\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4713\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0minplace\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m_drop_axis\u001b[0;34m(self, labels, axis, level, errors, only_slice)\u001b[0m\n\u001b[1;32m   4751\u001b[0m                 \u001b[0mnew_axis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlevel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4752\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4753\u001b[0;31m                 \u001b[0mnew_axis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4754\u001b[0m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_axis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4755\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mdrop\u001b[0;34m(self, labels, errors)\u001b[0m\n\u001b[1;32m   6998\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6999\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0merrors\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m\"ignore\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 7000\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{labels[mask].tolist()} not found in axis\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   7001\u001b[0m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m~\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   7002\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdelete\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyError\u001b[0m: \"['Depression'] not found in axis\""],"ename":"KeyError","evalue":"\"['Depression'] not found in axis\"","output_type":"error"}],"execution_count":4},{"cell_type":"code","source":"# Evaluate the predictor\nperformance = predictor.evaluate(train_data)\nprint(\"Model Performance on Training Data:\", performance)\n\n# Make predictions on the test data\npredictions = predictor.predict(test_data)\n\n# Prepare the submission file\nsubmission = pd.DataFrame({\n    'id': test['id'],\n    'Depression': predictions\n})\nsubmission.to_csv('team_technoids-autogluon_ensemble_test_predictions.csv', index=False)\nprint(\"Result output submission file created successfully!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-21T16:23:39.179012Z","iopub.execute_input":"2024-12-21T16:23:39.179478Z","iopub.status.idle":"2024-12-21T16:24:46.188629Z","shell.execute_reply.started":"2024-12-21T16:23:39.179444Z","shell.execute_reply":"2024-12-21T16:24:46.187698Z"}},"outputs":[{"name":"stderr","text":"Loading: /kaggle/working/AutogluonModels/ag-20241221_151757/models/CatBoost_BAG_L1/model.pkl\nLoading: /kaggle/working/AutogluonModels/ag-20241221_151757/models/LightGBM_BAG_L1/model.pkl\nLoading: /kaggle/working/AutogluonModels/ag-20241221_151757/models/NeuralNetFastAI_BAG_L1/model.pkl\nLoading: /kaggle/working/AutogluonModels/ag-20241221_151757/models/NeuralNetTorch_BAG_L1/model.pkl\nLoading: /kaggle/working/AutogluonModels/ag-20241221_151757/models/RandomForest_BAG_L1/model.pkl\nLoading: /kaggle/working/AutogluonModels/ag-20241221_151757/models/XGBoost_BAG_L1/model.pkl\nLoading: /kaggle/working/AutogluonModels/ag-20241221_151757/models/LightGBM_BAG_L2/model.pkl\nLoading: /kaggle/working/AutogluonModels/ag-20241221_151757/models/WeightedEnsemble_L3/model.pkl\nLoading: /kaggle/working/AutogluonModels/ag-20241221_151757/models/CatBoost_BAG_L1/model.pkl\n","output_type":"stream"},{"name":"stdout","text":"Model Performance on Training Data: {'accuracy': 0.9422885572139303, 'balanced_accuracy': 0.8925628513868771, 'mcc': 0.802260583516681, 'roc_auc': 0.9805106050268774, 'f1': 0.8368363943254431, 'precision': 0.8604901028968139, 'recall': 0.8144483122775453}\n","output_type":"stream"},{"name":"stderr","text":"Loading: /kaggle/working/AutogluonModels/ag-20241221_151757/models/LightGBM_BAG_L1/model.pkl\nLoading: /kaggle/working/AutogluonModels/ag-20241221_151757/models/NeuralNetFastAI_BAG_L1/model.pkl\nLoading: /kaggle/working/AutogluonModels/ag-20241221_151757/models/NeuralNetTorch_BAG_L1/model.pkl\nLoading: /kaggle/working/AutogluonModels/ag-20241221_151757/models/RandomForest_BAG_L1/model.pkl\nLoading: /kaggle/working/AutogluonModels/ag-20241221_151757/models/XGBoost_BAG_L1/model.pkl\nLoading: /kaggle/working/AutogluonModels/ag-20241221_151757/models/LightGBM_BAG_L2/model.pkl\nLoading: /kaggle/working/AutogluonModels/ag-20241221_151757/models/WeightedEnsemble_L3/model.pkl\n","output_type":"stream"},{"name":"stdout","text":"Result output submission file created successfully!\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}